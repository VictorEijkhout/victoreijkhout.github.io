<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Single-processor Computing</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


1.1 : <a href="sequential.html#TheVonNeumannarchitecture">The Von Neumann architecture</a><br>
1.2 : <a href="sequential.html#Modernprocessors">Modern processors</a><br>
1.2.1 : <a href="sequential.html#Theprocessingcores">The processing cores</a><br>
1.2.1.1 : <a href="sequential.html#Instructionhandling">Instruction handling</a><br>
1.2.1.2 : <a href="sequential.html#Floatingpointunits">Floating point units</a><br>
1.2.1.3 : <a href="sequential.html#Pipelining">Pipelining</a><br>
1.2.1.3.1 : <a href="sequential.html#Systoliccomputing">Systolic computing</a><br>
1.2.1.4 : <a href="sequential.html#Peakperformance">Peak performance</a><br>
1.2.2 : <a href="sequential.html#8-bit,16-bit,32-bit,64-bit">8-bit, 16-bit, 32-bit, 64-bit</a><br>
1.2.3 : <a href="sequential.html#Caches:on-chipmemory">Caches: on-chip memory</a><br>
1.2.4 : <a href="sequential.html#Graphics,controllers,specialpurposehardware">Graphics, controllers, special purpose hardware</a><br>
1.2.5 : <a href="sequential.html#Superscalarprocessingandinstruction-levelparallelism">Superscalar processing and instruction-level parallelism</a><br>
1.3 : <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a><br>
1.3.1 : <a href="sequential.html#Busses">Busses</a><br>
1.3.2 : <a href="sequential.html#LatencyandBandwidth">Latency and Bandwidth</a><br>
1.3.3 : <a href="sequential.html#Registers">Registers</a><br>
1.3.4 : <a href="sequential.html#Caches">Caches</a><br>
1.3.4.1 : <a href="sequential.html#Amotivatingexample">A motivating example</a><br>
1.3.4.2 : <a href="sequential.html#Cachetags">Cache tags</a><br>
1.3.4.3 : <a href="sequential.html#Cachelevels,speedandsize">Cache levels, speed and size</a><br>
1.3.4.4 : <a href="sequential.html#Typesofcachemisses">Types of cache misses</a><br>
1.3.4.5 : <a href="sequential.html#Reuseisthenameofthegame">Reuse is the name of the game</a><br>
1.3.4.6 : <a href="sequential.html#Replacementpolicies">Replacement policies</a><br>
1.3.4.7 : <a href="sequential.html#Cachelines">Cache lines</a><br>
1.3.4.8 : <a href="sequential.html#Cachemapping">Cache mapping</a><br>
1.3.4.9 : <a href="sequential.html#Directmappedcaches">Direct mapped caches</a><br>
1.3.4.10 : <a href="sequential.html#Associativecaches">Associative caches</a><br>
1.3.4.11 : <a href="sequential.html#Cachememoryversusregularmemory">Cache memory versus regular memory</a><br>
1.3.4.12 : <a href="sequential.html#Loadsversusstores">Loads versus stores</a><br>
1.3.5 : <a href="sequential.html#Prefetchstreams">Prefetch streams</a><br>
1.3.6 : <a href="sequential.html#Concurrencyandmemorytransfer">Concurrency and memory transfer</a><br>
1.3.7 : <a href="sequential.html#Memorybanks">Memory banks</a><br>
1.3.8 : <a href="sequential.html#TLB,pages,andvirtualmemory">TLB, pages, and virtual memory</a><br>
1.3.8.1 : <a href="sequential.html#Largepages">Large pages</a><br>
1.3.8.2 : <a href="sequential.html#TLB">TLB</a><br>
1.4 : <a href="sequential.html#Multicorearchitectures">Multicore architectures</a><br>
1.4.1 : <a href="sequential.html#Cachecoherence">Cache coherence</a><br>
1.4.1.1 : <a href="sequential.html#Solutionstocachecoherence">Solutions to cache coherence</a><br>
1.4.1.2 : <a href="sequential.html#Falsesharing">False sharing</a><br>
1.4.1.3 : <a href="sequential.html#Tagdirectories">Tag directories</a><br>
1.4.2 : <a href="sequential.html#Computationsonmulticorechips">Computations on multicore chips</a><br>
1.4.3 : <a href="sequential.html#TLBshootdown">TLB shootdown</a><br>
1.5 : <a href="sequential.html#Nodearchitectureandsockets">Node architecture and sockets</a><br>
1.6 : <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a><br>
1.6.1 : <a href="sequential.html#Datareuseandarithmeticintensity">Data reuse and arithmetic intensity</a><br>
1.6.1.1 : <a href="sequential.html#Example:vectoroperations">Example: vector operations</a><br>
1.6.1.2 : <a href="sequential.html#Example:matrixoperations">Example: matrix operations</a><br>
1.6.1.3 : <a href="sequential.html#Therooflinemodel">The roofline model</a><br>
1.6.2 : <a href="sequential.html#Locality">Locality</a><br>
1.6.2.1 : <a href="sequential.html#Temporallocality">Temporal locality</a><br>
1.6.2.2 : <a href="sequential.html#Spatiallocality">Spatial locality</a><br>
1.6.2.3 : <a href="sequential.html#Examplesoflocality">Examples of locality</a><br>
1.6.2.4 : <a href="sequential.html#Corelocality">Core locality</a><br>
1.7 : <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a><br>
1.7.1 : <a href="sequential.html#Peakperformance">Peak performance</a><br>
1.7.2 : <a href="sequential.html#Pipelining">Pipelining</a><br>
1.7.3 : <a href="sequential.html#Cachesize">Cache size</a><br>
1.7.4 : <a href="sequential.html#Cachelinesandstriding">Cache lines and striding</a><br>
1.7.5 : <a href="sequential.html#TLB">TLB</a><br>
1.7.6 : <a href="sequential.html#Cacheassociativity">Cache associativity</a><br>
1.7.7 : <a href="sequential.html#Loopnests">Loop nests</a><br>
1.7.8 : <a href="sequential.html#Looptiling">Loop tiling</a><br>
1.7.9 : <a href="sequential.html#Optimizationstrategies">Optimization strategies</a><br>
1.7.10 : <a href="sequential.html#Cacheawareandcacheobliviousprogramming">Cache aware and cache oblivious programming</a><br>
1.7.11 : <a href="sequential.html#Casestudy:Matrix-vectorproduct">Case study: Matrix-vector product</a><br>
1.8 : <a href="sequential.html#Furthertopics">Further topics</a><br>
1.8.1 : <a href="sequential.html#Powerconsumption">Power consumption</a><br>
1.8.2 : <a href="sequential.html#Derivationofscalingproperties">Derivation of scaling properties</a><br>
1.8.3 : <a href="sequential.html#Multicore">Multicore</a><br>
1.8.4 : <a href="sequential.html#Totalcomputerpower">Total computer power</a><br>
1.8.5 : <a href="sequential.html#Operatingsystemeffects">Operating system effects</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>1 Single-processor Computing</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In order to write efficient scientific codes, it is important to
understand computer architecture. The difference in speed between two
codes that compute the same result can range from a few percent to
orders of magnitude, depending only on factors relating to how well
the algorithms are coded for the processor architecture. Clearly, it
is not enough to have an algorithm and `put it on the computer': some
knowledge of computer architecture is advisable, sometimes crucial.
</p>

<p name="switchToTextMode">
Some
problems can be solved on a single 
<span title="acronym" ><i>CPU</i></span>
, others need a parallel
computer that comprises more than one processor. We will go into
detail on parallel computers in the next chapter, but even for
parallel processing, it is necessary to understand the individual 
<span title="acronym" ><i>CPUs</i></span>
.
</p>

<p name="switchToTextMode">
In this chapter, we will focus on what goes on inside a 
<span title="acronym" ><i>CPU</i></span>
 and its
memory system. We start with a brief general discussion of how
instructions are handled, then we will look into the arithmetic
processing in the processor core; last but not least, we will devote
much attention to the movement of data between memory and the
processor, and inside the processor. This latter point is, maybe
unexpectedly, very important, since memory access is typically much
slower than executing the processor's instructions, making it the
determining factor in a program's performance; the days when
`flops (Floating Point Operations per Second) counting' was the key to
predicting a code's performance are long gone. This discrepancy is in
fact a growing trend, so the issue of dealing with memory traffic has
been becoming more important over time, rather than going away.
</p>

<p name="switchToTextMode">
This chapter will give you a basic understanding of the issues
involved in 
<span title="acronym" ><i>CPU</i></span>
 design, how it affects performance, and how you can
code for optimal performance. For much more detail, see
an online book about PC
architecture~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Karbo:book">[Karbo:book]</a>
,
and the standard work about computer architecture, Hennesey and
Patterson~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#HennessyPatterson">[HennessyPatterson]</a>
.
</p>

<h2><a id="TheVonNeumannarchitecture">1.1</a> The Von Neumann architecture</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#TheVonNeumannarchitecture">The Von Neumann architecture</a>
</p>

<p name="switchToTextMode">

While computers, and most relevantly for this chapter, their
processors, can differ in any number of details, they also have many
aspects in common. On a very high level of abstraction, many
architectures can be described as 
<i>von Neumann architectures</i>
.
This describes a design with an undivided memory
that stores both program and data (`stored program'), and a processing
unit that executes the instructions, operating on the data in `fetch,
execute, store cycle'.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
This model with a prescribed sequence
  of instructions is also referred to as 
<i>control flow</i>
.
  This is in contrast to 
<i>data flow</i>
, which we will
  see in section~
6.11
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

This setup distinguishes modern processors for the very earliest, and
some special purpose contemporary, designs where the program was
hard-wired. It also allows programs to modify themselves or generate
other programs, since instructions and data are in the same
storage. This allows us to have editors and compilers: the computer
treats program code as data to operate on.
<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
At one time, the
  stored program concept was included as an essential component the
  ability for a running program to modify its own source. However, it
  was quickly recognized that this leads to unmaintainable code, and
  is rarely done in practice~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#EWD:EWD117">[EWD:EWD117]</a>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">
In this book we will
not explicitly discuss compilers, the programs that translate high
level languages to machine instructions. However, on occasion we will
discuss how a program at high level can be written to ensure
efficiency at the low level.
</p>

<p name="switchToTextMode">
In scientific computing, however, we typically do not pay much
attention to program code, focusing almost exclusively on data and how
it is moved about during program execution.  For most practical
purposes it is as if program and data are stored separately. The
little that is essential about instruction handling can be described
as follows.
</p>

<p name="switchToTextMode">
The machine instructions that a processor executes, as opposed to the
higher level languages users write in, typically specify the name of
an operation, as well as of the locations of the operands and the
result. These locations are not expressed as memory locations, but as
<i>registers</i>
<!-- index -->
: a small number of named memory locations that
are part of the 
<span title="acronym" ><i>CPU</i></span>
.
<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
Direct-to-memory architectures are rare,
  though they have existed. The Cyber 205 supercomputer in the 1980s
  could have three data streams, two from memory to the processor, and one
  back from the processor to memory, going on at the same time. Such
  an architecture is only feasible if memory can keep up with the
  processor speed, which is no longer the case these days.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">
As an
example, here is a simple C~routine
<!-- environment: verbatim start embedded generator -->
</p>
void store(double *a, double *b, double *c) {
 *c = *a + *b;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
and its X86 assembler output, obtained by
 <tt>gcc -O2 -S -o - store.c</tt> :
<!-- environment: verbatim start embedded generator -->
</p>
       .text
       .p2align 4,,15
.globl store
       .type   store, @function
store:
       movsd   (%rdi), %xmm0		# Load *a to %xmm0
       addsd   (%rsi), %xmm0		# Load *b and add to %xmm0
       movsd   %xmm0, (%rdx)		# Store to *c
       ret
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
(This is 64-bit
    output; add the option 
<tt>-m64</tt>
 on 32-bit systems.)
</p>

<p name="switchToTextMode">
The instructions here are:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
A load from memory to register;
<li>
Another load, combined with an addition;
<li>
Writing back the result to memory.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Each instruction is processed as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Instruction fetch: the next instruction according to the
<i>program counter</i>
 is loaded into the processor. We will
  ignore the questions of how and from where this happens.
<li>
Instruction decode: the processor inspects the instruction to
  determine the operation and the operands.
<li>
Memory fetch: if necessary, data is brought from memory
    into a register.
<li>
Execution: the operation is executed, reading data from registers
  and writing it back to a register.
<li>
Write-back: for store operations, the register contents is
  written back to memory.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
The case of array data is a little more complicated: the element
loaded (or stored) is then determined as the base address of the array
plus an offset.
</p>

<p name="switchToTextMode">
In a way, then, the modern 
<span title="acronym" ><i>CPU</i></span>
 looks to the programmer like a von
Neumann machine. There are various ways in which this is not so. For
one, while memory looks randomly addressable\footnote
  {There is in fact
  a theoretical model for computation called the `Random Access
  Machine'; we will briefly see its parallel generalization in
  section&nbsp;
2.2.2
.}, in practice there is a concept of
<i>locality</i>
: once a data item has been loaded, nearby items
are more efficient to load, and reloading the initial item is also faster.
</p>

<p name="switchToTextMode">
Another complication to this story of simple loading of data is that
contemporary 
<span title="acronym" ><i>CPUs</i></span>
 operate on several
instructions simultaneously, which are said to be `in flight', meaning
that they are in various stages of completion.
Of course, together with these simultaneous instructions, their inputs
and outputs are also being moved between memory and processor in an
overlapping manner.
This is the basic idea
of the 
<i>superscalar</i>
<span title="acronym" ><i>CPU</i></span>
 architecture, and is also referred
to as 
<i>ILP</i>
. Thus, while each
instruction can take several clock cycles to complete, a processor can
complete one instruction per cycle in favorable circumstances; in
some cases more than one instruction can be finished per cycle.
</p>

<p name="switchToTextMode">
The main statistic that is quoted about 
<span title="acronym" ><i>CPUs</i></span>
 is their
Gigahertz rating, implying that the speed of the processor is the main
determining factor of a computer's performance. While speed obviously
correlates with performance, the story is more complicated. Some
algorithms are 
<i>cpu-bound</i>
, and the speed of the processor
is indeed the most important factor; other algorithms are
<i>memory-bound</i>
, and aspects such as bus speed and cache
size, to be discussed later,
become important.
</p>

<p name="switchToTextMode">
In scientific computing, this second category is in fact quite
prominent, so in this chapter we will devote plenty of attention to
the process that moves data from memory to the processor, and we will
devote relatively little attention to the actual processor.
</p>

<h2><a id="Modernprocessors">1.2</a> Modern processors</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a>
</p>

<p name="switchToTextMode">

Modern processors are quite complicated, and in this section we will
give a short tour of what their constituent parts.
Figure&nbsp;
1.1
 is a picture of the 
<i>die</i>
of an 
<i>Intel Sandy Bridge</i>
 processor.
This chip is about an inch in size and contains close
to a billion transistors.
</p>

<h3><a id="Theprocessingcores">1.2.1</a> The processing cores</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a>
</p>
<p name="switchToTextMode">

In the Von Neumann model there is a single entity that executes instructions.
This has not been the case in increasing measure since the early 2000s.
The Sandy Bridge pictured in figure&nbsp;
1.1
 has eight 
<i>cores</i>
,
each of which
is an independent unit executing a stream of instructions.
In this chapter we will mostly discuss aspects of a single 
<i>core</i>
;
section&nbsp;
1.4
 will discuss the integration aspects
of the multiple cores.
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/sandybridge-eightcore-ann.jpg" width=800></img>
<p name="caption">
WRAPFIGURE 1.1: The Intel Sandy Bridge processor die
</p>

</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{3.5in}
</p>

<h4><a id="Instructionhandling">1.2.1.1</a> Instruction handling</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a> > <a href="sequential.html#Instructionhandling">Instruction handling</a>
</p>
<p name="switchToTextMode">

<!-- index -->
 The Von
Neumann model is also unrealistic in that it assumes that all
instructions are executed strictly in sequence.  Increasingly, over
the last twenty years, processor have used
<i>out-of-order</i>
<!-- index -->
instruction handling, where instructions can be processed
in a different order than the user program specifies.
Of course the processor is only allowed to re-order instructions
if that leaves the result of the execution intact!
</p>

<p name="switchToTextMode">
In the block diagram (figure&nbsp;
1.2
) you see various
units that are concerned with instruction handling: This cleverness
actually costs considerable energy, as well as sheer amount of
transistors.
For this reason, processors such as the first generation
Intel Xeon Phi, the
<i>Knights Corner</i>
<!-- index -->
,
used
<i>in-order</i>
<!-- index -->
 instruction
handling.
However, in the next generation, the
<i>Knights Landing</i>
<!-- index -->
,
this decision was reversed for reasons of performance.
</p>

<h4><a id="Floatingpointunits">1.2.1.2</a> Floating point units</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a> > <a href="sequential.html#Floatingpointunits">Floating point units</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In scientific computing we are mostly interested in what a processor
does with floating point data. Computing with integers or booleans
is typically of less interest. For this reason, cores have
considerable sophistication for dealing with numerical data.
</p>

<p name="switchToTextMode">
For instance, while past processors had just a single 
<span title="acronym" ><i>FPU</i></span>
,
these days they will have multiple, capable of executing
simultaneously.
</p>

<p name="switchToTextMode">
For instance, often there are separate addition and
multiplication units; if the compiler can find addition and
multiplication operations that are independent, it can schedule them
so as to be executed simultaneously, thereby doubling the performance
of the processor. In some cases, a processor will have multiple
addition or multiplication units.
</p>

<p name="switchToTextMode">
Another way to increase performance is to have a 
<i>FMA</i>
unit, which can execute the instruction $x\leftarrow ax+b$ in the same
amount of time as a separate addition or multiplication. Together with
pipelining (see below), this means that a processor has an asymptotic
speed of several floating point operations per clock cycle.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/sandybridge_pipeline.jpg" width=800></img>
<p name="caption">
FIGURE 1.2: Block diagram of the Intel Sandy Bridge core
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: table start embedded generator -->
</p>
<!-- TranslatingLineGenerator table ['table'] -->
<!-- index -->
<!-- index -->
<!-- index -->
<!-- index -->
<p name="switchToTextMode">
  \centering
<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
Processor</td><td>year</td><td>add/mult/fma units  </td><td>daxpy cycles</td></tr>
<tr><td>
         </td><td>    </td><td>(count$\times$width)</td><td>(arith vs load/store)</td></tr>
<tr><td>
</td></tr>
<tr><td>
MIPS R10000       </td><td>1996 </td><td>$1\times1+1\times1+0$ </td><td>8/24 </td></tr>
<tr><td>
Alpha EV5         </td><td>1996 </td><td>$1\times1+1\times1+0$ </td><td>8/12 </td></tr>
<tr><td>
IBM Power5        </td><td>2004 </td><td>$0+0+2\times1       $ </td><td>4/12 </td></tr>
<tr><td>
AMD Bulldozer     </td><td>2011 </td><td>$2\times2+2\times2+0$ </td><td>2/4  </td></tr>
<tr><td>
Intel Sandy Bridge</td><td>2012 </td><td>$1\times4+1\times4+0$ </td><td>2/4  </td></tr>
<tr><td>
Intel Haswell     </td><td>2014 </td><td>$0+0+2\times 4      $ </td><td>1/2  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
  \caption{Floating point capabilities (per core) of several processor architectures,
  and DAXPY cycle number for 8 operands}

</tbody></table>
</table>
<!-- environment: table end embedded generator -->
<p name="switchToTextMode">

Incidentally, there are few algorithms in which division operations
are a limiting factor. Correspondingly, the division operation is not
nearly as much optimized in a modern 
<span title="acronym" ><i>CPU</i></span>
 as the additions and
multiplications are. Division operations can take 10 or 20 clock
cycles, while a 
<span title="acronym" ><i>CPU</i></span>
 can have multiple addition and/or multiplication
units that (asymptotically) can produce a result per cycle.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="Pipelining">1.2.1.3</a> Pipelining</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a> > <a href="sequential.html#Pipelining">Pipelining</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
The floating point add and multiply units of a processor are
pipelined, which has the effect that a stream of independent
operations can be performed at an asymptotic speed of one result per
clock cycle.
</p>

<p name="switchToTextMode">
The idea behind a pipeline is as follows.
Assume that an operation consists of multiple
simpler operations, then we can potentially speed up the operation
by using dedicated hardware for each suboperation.
If we now have multiple operations to perform,
we get a speedup by having all suboperations active simultaneously:
each hands its result to the next and accepts its input(s) from the previous.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
  This description closely resembles that of
  a 
<i>bucket brigade</i>
,
  or a 
<i>systolic algorithm</i>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

For instance, an addition instruction can have the following components:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Decoding the instruction, including finding the locations of the
  operands.
<li>
Copying the operands into registers (`data fetch').
<li>
Aligning the exponents; the addition
  $.35\times 10^{-1}\,+\, .6\times 10^{-2}$ becomes
  $.35\times 10^{-1}\,+\, .06\times 10^{-1}$.
<li>
Executing the addition of the mantissas, in this case giving
  $.41$.
<li>
Normalizing the result, in this example to $.41\times
  10^{-1}$. (Normalization in this example does not do anything. Check
  for yourself that in
  $.3\times10^0\,+\,.8\times 10^0$ and
  $.35\times10^{-3}\,+\,(-.34)\times 10^{-3}$ there is a non-trivial
  adjustment.)
<li>
Storing the result.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
These parts are often called the `stages' or `segments' of the pipeline.
</p>

<p name="switchToTextMode">
If every component is designed to finish in 1 clock cycle, the whole
instruction takes 6&nbsp;cycles. However, if each has its own hardware, we
can execute two operations in less than 12&nbsp;cycles:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Execute the decode stage for the first operation;
<li>
Do the data fetch for the first operation, and at the same time
  the decode for the second.
<li>
Execute the third stage for the first operation and the second
  stage of the second operation simultaneously.
<li>
Et cetera.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
You see that the first operation still takes 6 clock cycles, but the
second one is finished a mere 1&nbsp;cycle later.
</p>

<p name="switchToTextMode">
Let us make a formal analysis of the speedup you can get from a
pipeline. On a traditional 
<span title="acronym" ><i>FPU</i></span>
, producing $n$ results
takes $t(n)=n\ell\tau$ where $\ell$&nbsp;is the number of stages, and
$\tau$&nbsp;the clock cycle time. The rate at which results are produced is
the reciprocal of $t(n)/n$: $r_{\mathrm{serial}}\equiv(\ell\tau)\inv$.
</p>

<p name="switchToTextMode">
On the other hand, for a pipelined 
<span title="acronym" ><i>FPU</i></span>
 the time is
$t(n)=[s+\ell+n-1]\tau$ where $s$ is a setup cost:
the first operation still has to go through the same stages
as before, but after that one more result will be
produced each cycle. We can also write this formula as
<!-- index -->
\[
 t(n)=[n+n_{1/2}]\tau, 
\]
expressing the linear time, plus an offset.
</p>

<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/pipeline.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.3: Schematic depiction of a pipelined operation
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Let us compare the speed of a classical 
<span title="acronym" ><i>FPU</i></span>
, and a pipelined
  one. Show that the result rate is now dependent on&nbsp;$n$: give a
  formula for $r(n)$, and for
  $r_\infty=\lim_{n\rightarrow\infty}r(n)$. What is the asymptotic
  improvement in $r$ over the non-pipelined case?
</p>

<p name="switchToTextMode">
  Next you can wonder how long it takes to get close to the asymptotic
  behavior. Show that for $n=n_{1/2}$ you get $r(n)=r_\infty/2$.
  This is often used as the definition of&nbsp;$n_{1/2}$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">
Since a vector processor works on a number of instructions
simultaneously, these instructions have to be independent. The
operation $\forall_i\colon a_i\leftarrow b_i+c_i$ has independent
additions; the operation $\forall_i\colon a_{i+1}\leftarrow
a_ib_i+c_i$ feeds the result of one iteration ($a_i$) to the input of
the next ($a_{i+1}=&hellip;$), so
the operations are not independent.
</p>

<p name="switchToTextMode">
A pipelined processor can speed up operations by a factor of $4,5,6$
with respect to earlier CPUs. Such numbers were typical in the 1980s
when the first successful vector computers came on the market. These
days, CPUs can have 20-stage pipelines. Does that mean they are
incredibly fast? This question is a bit complicated. Chip designers
continue to increase the clock rate, and the pipeline segments can no
longer finish their work in one cycle, so they are further split
up. Sometimes there are even segments in which nothing happens: that
time is needed to make sure data can travel to a different part of the
chip in time.
</p>

<p name="switchToTextMode">

The amount of improvement you can get from a pipelined CPU is limited,
so in a quest for ever higher performance several variations on the
pipeline design have been tried. For instance, the Cyber 205 had
separate addition and multiplication pipelines, and it was possible to
feed one pipe into the next without data going back to memory
first. Operations like $\forall_i\colon a_i\leftarrow b_i+c\cdot d_i$
were called `linked triads' (because of the number of paths to memory,
one input operand had to be scalar).
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze the speedup and $n_{1/2}$ of linked triads.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Another way to increase performance is to have multiple identical
pipes. This design was perfected by the NEC SX series. With, for
instance, 4&nbsp;pipes, the operation $\forall_i\colon a_i\leftarrow
b_i+c_i$ would be split module&nbsp;4, so that the first pipe operated on
indices $i=4\cdot j$, the second on $i=4\cdot j+1$, et cetera.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze the speedup and $n_{1/2}$ of a processor with multiple
  pipelines that operate in parallel. That is, suppose that there are
  $p$ independent pipelines, executing the same instruction, that can
  each handle a stream of operands.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

(You may wonder why we are mentioning some fairly old computers here:
true pipeline supercomputers hardly exist anymore. In the US, the
Cray&nbsp;X1 was the last of that line, and in Japan only NEC still makes
them. However, the functional units of a CPU these days are pipelined,
so the notion is still important.)
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The operation
<!-- environment: verbatim start embedded generator -->
</p>
for (i) {
  x[i+1] = a[i]*x[i] + b[i];
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  can not be handled by a pipeline because there is
  a 
<i>dependency</i>
 between input of one iteration of the operation
  and the output of the previous.
  However, you can transform the loop into one that is mathematically
  equivalent, and potentially more efficient to compute. Derive an
  expression that computes 
<tt>x[i+2]</tt>
 from 
<tt>x[i]</tt>
 without
  involving 
<tt>x[i+1]</tt>
. This is known as 
    doubling}. Assume you have plenty of temporary storage. You can now
  perform the calculation by
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Doing some preliminary calculations;
<li>
computing 
<tt>x[i],x[i+2],x[i+4],...</tt>
, and from these,
<li>
compute the missing terms 
<tt>x[i+1],x[i+3],...</tt>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  Analyze the efficiency of this scheme by giving formulas for
  $T_0(n)$ and&nbsp;$T_s(n)$. Can you think of an argument
  why the preliminary calculations may be of lesser importance in some
  circumstances?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h5><a id="Systoliccomputing">1.2.1.3.1</a> Systolic computing</h5>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a> > <a href="sequential.html#Pipelining">Pipelining</a> > <a href="sequential.html#Systoliccomputing">Systolic computing</a>
</p>
</p>

<p name="switchToTextMode">
Pipelining as described above is one case of a
<i>systolic algorithm</i>
. In the 1980s and 1990s there was
research into using pipelined algorithms and building special
hardware, 
<i>systolic array</i>
s, to implement
them&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Ku:systolic">[Ku:systolic]</a>
. This is also connected to computing with
<span title="acronym" ><i>FPGAs</i></span>
, where the systolic array is software-defined.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="Peakperformance">1.2.1.4</a> Peak performance</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Theprocessingcores">The processing cores</a> > <a href="sequential.html#Peakperformance">Peak performance</a>
</p>
</p>

<p name="switchToTextMode">
Thanks to pipelining, for modern 
<span title="acronym" ><i>CPUs</i></span>
 there is a simple relation
between the 
<i>clock speed</i>
 and the 
<i>peak performance</i>
.
Since each 
<span title="acronym" ><i>FPU</i></span>
 can produce one result per cycle
asymptotically, the peak performance is the clock speed times the
number of independent 
<span title="acronym" ><i>FPUs</i></span>
. The measure of floating
point performance is `floating point operations per second',
abbreviated 
<i>flops</i>
. Considering the speed of computers
these days, you will mostly hear floating point performance being
expressed in `gigaflops': multiples of $10^9$ flops.
</p>

<h3><a id="8-bit,16-bit,32-bit,64-bit">1.2.2</a> 8-bit, 16-bit, 32-bit, 64-bit</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#8-bit,16-bit,32-bit,64-bit">8-bit, 16-bit, 32-bit, 64-bit</a>
</p>
<p name="switchToTextMode">

Processors are often characterized in terms of how big a chunk of data
they can process as a unit. This can relate to
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The width of the path between processor and memory: can a 64-bit
  floating point number be loaded in one cycle, or does it arrive in
  pieces at the processor.
<li>
The way memory is addressed: if addresses are limited to 16
  bits, only 64,000 bytes can be identified. Early PCs had a
  complicated scheme with segments to get around this limitation: an
  address was specified with a segment number and an offset inside the segment.
<li>
The number of bits in a register, in particular the size of the
  integer registers which manipulate data address; see the previous
  point. (Floating point register are often larger, for instance 80
  bits in the x86 architecture.) This also corresponds to the size of
  a chunk of data that a processor can operate on simultaneously.
<li>
The size of a floating point number. If the arithmetic unit of a
<span title="acronym" ><i>CPU</i></span>
 is designed to multiply 8-byte numbers efficiently (`double
  precision'; see section&nbsp;
3.3.2
) then numbers half
  that size (`single precision') can sometimes be processed at higher
  efficiency, and for larger numbers (`quadruple precision') some
  complicated scheme is needed. For instance, a quad precision number
  could be emulated by two double precision numbers with a fixed
  difference between the exponents.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
These measurements are not necessarily identical. For instance, the
original Pentium processor had 64-bit data busses, but a 32-bit
processor. On the other hand, the Motorola 68000 processor (of the
original Apple Macintosh) had a 32-bit 
<span title="acronym" ><i>CPU</i></span>
, but 16-bit data busses.
</p>

<p name="switchToTextMode">
The first Intel
<!-- index -->
 microprocessor, the 4004, was a 4-bit
processor in the sense that it processed 4 bit chunks. These days,
64 bit processors are becoming the norm.
</p>

<h3><a id="Caches:on-chipmemory">1.2.3</a> Caches: on-chip memory</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Caches:on-chipmemory">Caches: on-chip memory</a>
</p>
<p name="switchToTextMode">

The bulk of computer memory is in chips that are separate from the processor.
However, there is usually a small amount (typically a few megabytes)
of on-chip memory, called the 
<i>cache</i>
. This will be
explained in detail in section&nbsp;
1.3.4
.
</p>

<h3><a id="Graphics,controllers,specialpurposehardware">1.2.4</a> Graphics, controllers, special purpose hardware</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Graphics,controllers,specialpurposehardware">Graphics, controllers, special purpose hardware</a>
</p>
<p name="switchToTextMode">

One difference between `consumer' and `server' type processors
is that the consumer chips devote considerable real-estate
on the processor chip to graphics. Processors for cell phones and tablets
can even have dedicated circuitry for security or mp3 playback.
Other parts of the processor
are dedicated to communicating with memory or the 
<i>I/O subsystem</i>
.
We will not discuss those aspects in this book.
</p>

<h3><a id="Superscalarprocessingandinstruction-levelparallelism">1.2.5</a> Superscalar processing and instruction-level parallelism</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Modernprocessors">Modern processors</a> > <a href="sequential.html#Superscalarprocessingandinstruction-levelparallelism">Superscalar processing and instruction-level parallelism</a>
</p>

<p name="switchToTextMode">

In the von Neumann model processors operate through 
<i>control flow</i>
:
instructions follow each other linearly or with branches without regard for what
data they involve. As processors became more powerful and capable of executing
more than one instruction at a time, it became necessary to switch to the
<i>data flow</i>
 model. Such 
<i>superscalar</i>
 processors
analyze several instructions to find data dependencies, and execute
instructions in parallel that do not depend on each other.
</p>

<p name="switchToTextMode">
This concept is also known as 
<i>ILP</i>
, and it is facilitated by
various mechanisms:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
multiple-issue: instructions that are independent can be started
  at the same time;
<li>
pipelining: already mentioned, arithmetic units can deal with
  multiple operations in various stages of completion;
<li>
branch prediction and speculative execution: a compiler can
  `guess' whether a conditional instruction will evaluate to true, and
  execute those instructions accordingly;
<li>
out-of-order execution: instructions can be rearranged if they
  are not dependent on each other, and if the resulting execution will
  be more efficient;
<li>
<i>prefetching</i>
: data can be speculatively requested before any
  instruction needing it is actually encountered (this is discussed
  further in section&nbsp;
1.3.5
).
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Above, you saw pipelining in the context of floating point
operations. Nowadays, the whole 
<span title="acronym" ><i>CPU</i></span>
 is pipelined. Not only floating point
operations, but any sort of instruction will be put in the
<i>instruction pipeline</i>
 as soon
as possible. Note that this pipeline is no longer limited to identical
instructions: the notion of pipeline is now generalized to any stream
of partially executed instructions that are simultaneously ``in
flight''.
</p>

<p name="switchToTextMode">
As clock frequency has gone up, the processor pipeline has grown in
length to make the segments executable in less time. You have already
seen that longer pipelines have a larger $n_{1/2}$, so more
independent instructions are needed to make the pipeline run at full
efficiency. As the limits to instruction-level parallelism are
reached, making pipelines longer
<!-- index -->
 (sometimes
called `deeper'
<!-- index -->
) no longer pays off. This is
generally seen as the reason that chip designers have moved to
<i>multicore</i>
 architectures as a way of more efficiently
using the transistors on a chip; section&nbsp;
1.4
.
</p>

<p name="switchToTextMode">
There is a second problem with these longer pipelines: if the code
comes to a branch point (a conditional or the test in a loop), it is
not clear what the next instruction to execute is. At that point the
pipeline can 
<i>stall</i>

<!-- index -->
. 
<span title="acronym" ><i>CPUs</i></span>
 have taken to
<i>speculative execution</i>
 for instance, by always assuming
that the test will turn out true. If the code then takes the other
branch (this is called a 
<i>branch misprediction</i>
), the
pipeline has to be 
<i>flushed</i>

<!-- index -->
 and
restarted. The resulting delay in the execution stream is called the
<i>branch penalty</i>
.
</p>

<h2><a id="MemoryHierarchies">1.3</a> Memory Hierarchies</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a>
</p>

<p name="switchToTextMode">

We will now refine the picture of the Von Neumann architecture, in
which data is loaded immediately from memory to the processors, where
it is operated on. This picture is unrealistic because of the
so-called 
<i>memory wall</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Wulf:memory-wall">[Wulf:memory-wall]</a>
: the
memory is too slow to load data into the process at the rate the
processor can absorb it. Specifically, a single load can take 1000
cycles, while a processor can perform several operations per
cycle. (After this long wait for a load, the next load can come
faster, but still too slow for the processor. This matter of wait time
versus throughput will be addressed below in
section&nbsp;
1.3.2
.)
</p>

<p name="switchToTextMode">
In reality, there will be various memory levels in between the
<span title="acronym" ><i>FPU</i></span>
 and the main memory: the registers
<!-- index -->
and the caches
<!-- index -->
, together called the
<i>memory hierarchy</i>
. These try to alleviate the memory
wall problem by making recently used data available quicker than it
would be from main memory. Of course, this presupposes that the
algorithm and its implementation allow for data to be used multiple
times.  Such questions of 
<i>data reuse</i>
 will be discussed in
more detail in section&nbsp;
1.6.1
.
</p>

<p name="switchToTextMode">
Both registers and caches are faster
than main memory to various degrees; unfortunately, the faster the memory on a certain
level, the smaller it will be.
These differences in size and access speed lead to interesting programming
problems, which we will discuss later in this chapter, and
particularly section&nbsp;
1.7
.
</p>

<p name="switchToTextMode">
We will now discuss the various components of the memory hierarchy and
the theoretical concepts needed to analyze their behavior.
</p>

<h3><a id="Busses">1.3.1</a> Busses</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Busses">Busses</a>
</p>
<p name="switchToTextMode">

The wires that move data around in a computer, from memory to cpu or
to a disc controller or screen, are called 
<i>busses</i>

<!-- index -->
. The
most important one for us is the 
<i>FSB</i>
 which connects the processor to memory. In one popular
architecture, this is called the `north bridge', as opposed to the
`south bridge' which connects to external devices, with the exception
of the graphics controller.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bridges.jpg" width=800></img>
<p name="caption">
FIGURE 1.4: Bus structure of a processor
</p>
</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The bus is typically slower than the processor, operating with clock
frequencies slightly in excess of&nbsp;1GHz, which is a fraction of the 
<span title="acronym" ><i>CPU</i></span>
clock frequency.  This is one reason that caches are needed; the fact
that a processors can consume many data items per clock tick
contributes to this. Apart from the frequency, the bandwidth of a bus is
also determined by the number of bits that can be moved per clock
cycle. This is typically 64 or 128 in current architectures. We will
now discuss this in some more detail.
</p>

<h3><a id="LatencyandBandwidth">1.3.2</a> Latency and Bandwidth</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#LatencyandBandwidth">Latency and Bandwidth</a>
</p>

<p name="switchToTextMode">

Above, we mentioned in very general terms that accessing data in
registers is almost instantaneous, whereas loading data from memory
into the registers, a necessary step before any operation, incurs a
substantial delay. We will now make this story slightly more precise.
</p>

<p name="switchToTextMode">
There are two important concepts to describe the movement of data:
<i>latency</i>
 and 
<i>bandwidth</i>
. The assumption here is
that requesting an item of data incurs an initial delay; if this item
was the first in a stream of data, usually a consecutive range of
memory addresses, the remainder of the stream will arrive with no
further delay
at a regular amount per time period.
<!-- environment: description start embedded generator -->
</p>
<!-- TranslatingLineGenerator description ['description'] -->
<li>
[Latency] is the delay between the processor issuing a request
  for a memory item, and the item actually arriving. We can
  distinguish between various latencies, such as the transfer from
  memory to cache, cache to register, or summarize them all into the
  latency between memory and processor. Latency is measured in (nano)
  seconds, or clock periods.
</p>

<p name="switchToTextMode">
  If a processor executes instructions in the order they are found in
  the assembly code, then execution will often 
<i>stall</i>

<!-- index -->
 while
  data is being fetched from memory; this is also called
<i>memory stall</i>
. For this reason, a low latency is very
  important. In practice, many processors have `out-of-order
  execution' of instructions, allowing them to perform other
  operations while waiting for the requested data. Programmers can
  take this into account, and code in a way that achieves
<i>latency hiding</i>
; see also section&nbsp;
1.6.1
.
<span title="acronym" ><i>GPUs</i></span>
 (see section&nbsp;
2.9.3
)
  can switch very quickly between threads in order to achieve latency hiding.
<li>
[Bandwidth] is the rate at which data arrives at its destination,
  after the initial latency is overcome. Bandwidth is measured in
  bytes (kilobytes, megabytes, gigabytes) per second or per clock cycle.
  The bandwidth between two memory levels is usually the product of
  the cycle speed of the channel (the 
<i>bus speed</i>
) and
  the 
<i>bus width</i>
: the number of bits that can be sent
  simultaneously in every cycle of the bus clock.
</ul>
</description>
<!-- environment: description end embedded generator -->
<p name="switchToTextMode">

The concepts of latency and bandwidth are often combined in a formula
for the time that a message takes from start to finish:
\[
 T(n) = \alpha+\beta n 
\]
where $\alpha$ is the latency and $\beta$ is the inverse of the
bandwidth: the time per byte.
</p>

<p name="switchToTextMode">
Typically, the further away from the processor one gets, the longer
the latency is, and the lower the bandwidth.
These two factors make it important to program in such a
way that, if at all possible, the processor uses data from cache or register,
rather than from main memory. To illustrate that this is a serious
matter, consider a vector addition
<!-- environment: verbatim start embedded generator -->
</p>
for (i)
  a[i] = b[i]+c[i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Each iteration performs one floating point operation, which modern
<span title="acronym" ><i>CPUs</i></span>
 can do in one clock cycle by using pipelines. However, each
iteration needs two numbers loaded and one written, for a total of 24
bytes of memory traffic.
(Actually, 
<tt>a[i]</tt>
 is loaded before it can be written,
so there are 4 memory access, with a total of 32 bytes, per
iteration.)
Typical memory bandwidth figures (see
for instance figure&nbsp;
1.5
) are nowhere near 24 (or&nbsp;32) bytes per
cycle. This means that, without caches, algorithm performance can be
bounded by memory performance. Of course, caches will not speed up
every operations, and in fact will have no effect on the above
example. Strategies for programming that lead to significant cache use
are discussed in section&nbsp;
1.7
.
</p>

<p name="switchToTextMode">
The concepts of latency and bandwidth will also appear in parallel
computers, when we talk about sending data from one processor to the
next.
</p>

<h3><a id="Registers">1.3.3</a> Registers</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Registers">Registers</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Every processor has a small amount of memory that is internal to the
processor: the 
<i>registers</i>
, or together the
<i>register file</i>
. The registers are what the processor
actually operates on: an operation such as
<!-- environment: verbatim start embedded generator -->
</p>
a := b + c
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
is actually implemented as
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
load the value of 
<tt>b</tt>
 from memory into a register,
<li>
load the value of 
<tt>c</tt>
 from memory into another register,
<li>
compute the sum and write that into yet another register, and
<li>
write the sum value back to the memory location of&nbsp;
<tt>a</tt>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Looking at assembly code (for instance the output of a compiler), you
see the explicit load, compute, and store instructions.
</p>

<p name="switchToTextMode">
Compute instructions such as add or multiply only operate on
registers. For instance, in 
<i>assembly language</i>
you will see instructions such as
<!-- environment: verbatim start embedded generator -->
</p>
addl	%eax, %edx
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
which adds the content of one register to
another. As you see in this sample instruction, registers are not
numbered, as opposed to memory addresses,
but have distinct names that are referred to in
the assembly instruction. Typically, a processor has 16 or 32
floating point registers; the 
<i>Intel Itanium</i>
 was
exceptional with 128 floating point registers.
</p>

<p name="switchToTextMode">
Registers have a high bandwidth and low latency because they
are part of the processor. You can consider data movement to and from
registers as essentially instantaneous.
</p>

<p name="switchToTextMode">
In this chapter you will see stressed that moving data from memory is
relatively expensive. Therefore, it would be a simple optimization to
leave data in register when possible. For instance, if the above
computation is followed by a statement
<!-- environment: verbatim start embedded generator -->
</p>
a := b + c
d := a + e
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the computed value of 
<tt>a</tt>
 could be left in register. This
optimization is typically performed as a
<i>compiler optimization</i>
: the compiler will simply not
generate the instructions for storing and reloading&nbsp;
<tt>a</tt>
. We say that

<tt>a</tt>
 stays 
<i>resident in register</i>
.
</p>

<p name="switchToTextMode">
Keeping values in register is often done to avoid recomputing a
quantity. For instance, in
<!-- environment: verbatim start embedded generator -->
</p>
t1 = sin(alpha) * x + cos(alpha) * y;
t2 = -cos(alpha) * x + sin(alpha) * y;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the sine and cosine quantity will probably be kept in register. You
can help the compiler by explicitly introducing temporary quantities:
<!-- environment: verbatim start embedded generator -->
</p>
s = sin(alpha); c = cos(alpha);
t1 = s * x + c * y;
t2 = -c * x + s * y
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Of
course, there is a limit to how many quantities can be kept in
register; trying to keep too many quantities in register is called
<i>register spill</i>
 and lowers the performance of a code.
</p>

<p name="switchToTextMode">
Keeping a variable in register is especially important if that
variable appears in an inner loop. In the computation
<!-- environment: verbatim start embedded generator -->
</p>
for i=1,length
  a[i] = b[i] * c
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the quantity 
<tt>c</tt>
 will probably be kept in register by the compiler,
but in
<!-- environment: verbatim start embedded generator -->
</p>
for k=1,nvectors
  for i=1,length
    a[i,k] = b[i,k] * c[k]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
it is a good idea to introduce explicitly a temporary variable to
hold&nbsp;
<tt>c[k]</tt>
. In&nbsp;C, you can give a hint to the compiler
to keep a variable in register by declaring it as a 
<i>register variable</i>
:
<!-- environment: verbatim start embedded generator -->
</p>
register double t;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h3><a id="Caches">1.3.4</a> Caches</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In between the registers, which contain the immediate input and output
data for instructions, and the main memory where lots of data can reside for a
long time, are various levels of 
<i>cache</i>
 memory, that have
lower latency and higher bandwidth than main memory and where data are
kept for an intermediate amount of time.
</p>

<p name="switchToTextMode">
Data from
memory travels through the caches to wind up in registers. The
advantage to having cache memory is that if a data item is reused
shortly after it was first needed, it will still be in cache, and
therefore it can be accessed much faster than if it would have to be
brought in from memory.
</p>

<p name="switchToTextMode">
On a historical note, the notion of levels of memory hierarchy was
already discussed in 1946&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Burks:discussion">[Burks:discussion]</a>
, motivated
by the slowness of the memory technology at the time.
</p>

<h4><a id="Amotivatingexample">1.3.4.1</a> A motivating example</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Amotivatingexample">A motivating example</a>
</p>
<p name="switchToTextMode">

As an example, let's suppose a variable 
<tt>x</tt>
 is used twice, and its
uses are too far apart that it would stay 
  in}{register}:
<!-- environment: verbatim start embedded generator -->
</p>
... = ... x ..... // instruction using x
.........         // several instructions not involving x
... = ... x ..... // instruction using x
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
The assembly code would then be
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
load 
<tt>x</tt>
 from memory into register; operate on it;
<li>
do the intervening instructions;
<li>
load 
<tt>x</tt>
 from memory into register; operate on it;
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
With a cache, the assembly code stays the same, but the actual
behavior of the memory system now becomes:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
load 
<tt>x</tt>
 from memory into cache, and from cache into register;
  operate on it;
<li>
do the intervening instructions;
<li>
request 
<tt>x</tt>
 from memory, but since it is still in the cache,
  load it from the cache into register; operate on it.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Since loading from cache is faster than loading from main memory, the
computation will now be faster. Caches are fairly small, so values
can not be kept there indefinitely. We will see the implications of
this in the following discussion.
</p>

<p name="switchToTextMode">
There is an important difference between cache memory and registers:
while data is moved into register by explicit assembly instructions,
the move from main memory to cache is entirely done by hardware.  Thus
cache use and reuse is outside of direct programmer control. Later,
especially in sections 
1.6.2
 and
&nbsp;
1.7
, you will see how it is possible to
influence cache use indirectly.
</p>

<h4><a id="Cachetags">1.3.4.2</a> Cache tags</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Cachetags">Cache tags</a>
</p>

<p name="switchToTextMode">

In the above example, the mechanism was left unspecified by which it is found whether an
item is present in cache. For this, there is a
<i>tag</i>
<!-- index -->
<!-- index -->
for each cache location: sufficient information to reconstruct the
memory location that the cache item came from.
</p>

<h4><a id="Cachelevels,speedandsize">1.3.4.3</a> Cache levels, speed and size</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Cachelevels,speedandsize">Cache levels, speed and size</a>
</p>

<p name="switchToTextMode">

The caches are called `level&nbsp;1' and `level&nbsp;2' (or, for short, L1 and
L2) cache; some processors can have an L3 cache.  The L1 and L2 caches
are part of the 
<i>die</i>
, the processor chip, although for the
L2 cache that is a relatively recent development; the L3 cache is
off-chip.  The L1 cache is small, typically around 16Kbyte. Level&nbsp;2
(and, when present, level&nbsp;3) cache is more plentiful, up to several
megabytes, but it is also slower.  Unlike main memory, which is
expandable, caches are fixed in size. If a version of a processor chip
exists with a larger cache, it is usually considerably more expensive.
</p>

<p name="switchToTextMode">
Data needed in some operation gets copied into the various
caches on its way to the processor. If, some instructions later, a
data item is needed again, it is first searched for in the L1 cache; if it is
not found there, it is searched for in the L2 cache; if it is not found there,
it is loaded from main memory. Finding data in cache is called a
<i>cache hit</i>
, and not finding it a 
<i>cache miss</i>
.
</p>

<p name="switchToTextMode">
Figure 
1.5
 illustrates the basic facts of the
<i>cache hierarchy</i>
, in
this case for the 
<i>Intel Sandy Bridge</i>
 chip: the
closer caches are to the 
<span title="acronym" ><i>FPUs</i></span>
, the faster, but also the
smaller they are.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/hierarchysb.jpeg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.5: Memory hierarchy of an Intel Sandy Bridge, characterized by speed and size.
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Some points about this figure.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Loading data from registers is so fast that it does not
  constitute a limitation on algorithm execution speed. On the other
  hand, there are few registers. Each core has 16 general purpose
  registers, and 16 SIMD registers.
<li>
The L1 cache is small, but sustains a bandwidth of 32 bytes,
  that is 4 double precision number, per cycle. This is enough to load
  two operands each for two operations, but note that the core can
  actually perform 4 operations per cycle. Thus, to achieve peak
  speed, certain operands need to stay in register: typically, L1
  bandwidth is enough for about half of peak performance.
<li>
The bandwidth of the L2 and L3 cache is nominally the same as
  of L1. However, this bandwidth is partly wasted on coherence issues.
<li>
Main memory access has a latency of more than 100 cycles, and a
  bandwidth of 4.5 bytes per cycle, which is about $1/7$th of the L1
  bandwidth. However, this bandwidth is shared by the multiple cores
  of a processor chip, so effectively the bandwidth is a fraction of
  this number. Most clusters will also have more than one
<i>socket</i>
 (processor chip) per node, typically 2&nbsp;or&nbsp;4, so
  some bandwidth is spent on maintaining
<i>cache coherence</i>
 (see section&nbsp;
1.4
),
  again reducing the bandwidth available for each chip.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

On level&nbsp;1, there are separate caches for instructions and data; the
L2 and L3 cache contain both data and instructions.
</p>

<p name="switchToTextMode">
You see that the larger caches are increasingly unable to supply data
to the processors fast enough. For this reason it is necessary to code
in such a way that data is kept as much as possible in the highest
cache level possible. We will discuss this issue in detail in the rest
of this chapter.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The L1 cache is smaller than the L2 cache, and if there is an L3,
  the L2 is smaller than the L3. Give a practical and a theoretical
  reason why this is so.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Typesofcachemisses">1.3.4.4</a> Types of cache misses</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Typesofcachemisses">Types of cache misses</a>
</p>

</p>

<p name="switchToTextMode">
There are three types of cache misses.
</p>

<p name="switchToTextMode">
As you saw in the example above, the first time you reference data you
will always incur a cache miss. This is known as a 
<i>compulsory cache miss</i>

<!-- index -->
 since these are unavoidable.
Does that mean that you will always be waiting for a data item, the first
time you need it? Not necessarily: section&nbsp;
1.3.5
 explains
how the hardware tries to help you by predicting what data is needed next.
</p>

<p name="switchToTextMode">
The next type of cache misses is due to the size of your working set:
a 
<i>capacity cache miss</i>

<!-- index -->
 is caused by
data having been overwritten because the cache can simply not contain
all your problem data. (Section&nbsp;
1.3.4.6
 discusses how the processor
decides what data to overwrite.) If you want to avoid this type of misses, you
need to partition your problem in chunks that are small enough that
data can stay in cache for an appreciable time. Of course, this
presumes that data items are operated on multiple times, so that there
is actually a point in keeping it in cache; this is discussed in
section&nbsp;
1.6.1
.
</p>

<p name="switchToTextMode">
Finally, there are 
<i>conflict misses</i>

<!-- index -->
caused by one data item being mapped to the same cache location
as another, while both are still needed for the computation, and
there would have been better candidates to evict. This is discussed
in section&nbsp;
1.3.4.10
.
</p>

<p name="switchToTextMode">
In a 
<i>multicore</i>
 context there is a further type of cache miss:
the 
<i>invalidation miss</i>

<!-- index -->
. This happens
if an item in cache has become invalid because another core
changed the value of the corresponding memory address. The core will then
have to reload this address.
</p>

<h4><a id="Reuseisthenameofthegame">1.3.4.5</a> Reuse is the name of the game</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Reuseisthenameofthegame">Reuse is the name of the game</a>
</p>
<p name="switchToTextMode">

The presence of one or more caches is not immediately a guarantee for
high performance: this largely depends on the 
  pattern} of the code, and how well this exploits the caches.
The first time that an item is
referenced, it is copied from memory into cache, and through to the
processor registers. The latency and bandwidth for this are not mitigated in any
way by the presence of a cache. When the same item is referenced a
second time, it may be found in cache, at a considerably reduced cost
in terms of latency and bandwidth: caches have shorter latency and
higher bandwidth than main memory.
</p>

<p name="switchToTextMode">
We conclude that, first, an algorithm has to have an opportunity for
data reuse. If every data item is used only once (as in addition of
two vectors), there can be no reuse, and the presence of caches is
largely irrelevant. A&nbsp;code will only benefit from the increased
bandwidth and reduced latency of a cache if items in cache are
referenced more than once; see section&nbsp;
1.6.1
 for a detailed
discussion.. An example would be the matrix-vector multiplication
$y=Ax$ where each element of&nbsp;$x$ is used in $n$ operations, where $n$
is the matrix dimension.
</p>

<p name="switchToTextMode">
Secondly, an algorithm may theoretically
have an opportunity for reuse, but it needs to be coded in such a way
that the reuse is actually exposed. We will address these points in
section&nbsp;
1.6.2
. This second point especially is not
trivial.
</p>

<p name="switchToTextMode">
Some problems are small enough that they fit completely in cache, at
least in the L3 cache. This is something to watch out for when
<i>benchmarking</i>
, since it gives a too rosy picture of
processor performance.
</p>

<h4><a id="Replacementpolicies">1.3.4.6</a> Replacement policies</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Replacementpolicies">Replacement policies</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Data in cache and registers is placed there by the system, outside of
programmer control. Likewise, the system decides when to overwrite
data in the cache or in registers if it is not referenced in a while,
and as other data needs to be placed there.  Below, we will go into
detail on how caches do this, but as a general principle, a 
<span title="acronym" ><i>LRU</i></span>
cache replacement policy is used: if a cache is full and new data
needs to be placed into it, the data that was least recently used is
<i>flushed from cache</i>
,
meaning that it is overwritten with the new item,
and therefore no longer accessible. LRU is by far the most common
replacement policy; other possibilities are FIFO (first in first out)
or random replacement.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  How does the LRU replacement policy related to direct-mapped versus
  associative caches?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Sketch a simple scenario, and give some (pseudo) code, to argue that
  LRU is preferable over FIFO as a replacement strategy.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h4><a id="Cachelines">1.3.4.7</a> Cache lines</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Cachelines">Cache lines</a>
</p>


<!-- index -->
<p name="switchToTextMode">

Data movement between memory and cache, or between caches, is not done
in single bytes, or even words. Instead, the smallest unit of data
moved is called a 
<i>cache line</i>
, sometimes called a
<i>cache block</i>
.  A&nbsp;typical cache line is 64 or 128 bytes
long, which in the context of scientific computing implies 8 or 16
double precision floating point numbers. The cache line size for data
moved into L2 cache can be larger than for data moved into L1 cache.
</p>

<p name="switchToTextMode">
A first motivation for cache lines is a practical one: it simplifies
the circuitry involved. Secondly, cachelines make sense since many
codes show 
<i>spatial locality</i>
;
section&nbsp;
1.6.2
.
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/stride-1.jpg" width=800></img>
<p name="caption">
WRAPFIGURE 1.6: Accessing 4 elements at stride 1
</p>

</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{3in}
Conversely, there is now a strong incentive to code in such a way to
exploit this locality,
since any memory access costs the transfer of several words (see
section&nbsp;
1.7.4
 for some examples). An
efficient program then tries to use the other items on the cache line,
since access to them is effectively free. This phenomenon is visible in
code that accesses arrays by 
<i>stride</i>
: elements are read or
written at regular intervals.
</p>

<p name="switchToTextMode">
Stride&nbsp;1 corresponds to sequential access of an array:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N; i++)
  ... = ... x[i] ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Let us use as illustration a case with 4 words per cacheline. Requesting
the first elements loads the whole cacheline that contains it into
cache. A&nbsp;request for the 2nd, 3rd, and 4th element can then be
satisfied from cache, meaning with high bandwidth and low latency.
\bigskip
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/stride-3.jpg" width=800></img>
<p name="caption">
WRAPFIGURE 1.7: Accessing 4 elements at stride 3
</p>

<p name="switchToTextMode">
  \vskip-.5in
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{3in}
A larger stride
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N; i+=stride)
  ... = ... x[i] ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
implies that in every cache line only certain elements
are used. We illustrate that with stride&nbsp;3: requesting the first
elements loads a cacheline, and this cacheline also contains the
second element. However, the third element is on the next cacheline,
so loading this incurs the latency and bandwidth of main memory. The
same holds for the fourth element. Loading four elements now needed
loading three cache lines instead of one, meaning that two-thirds of
the available bandwidth has been wasted. (This second case would also incur
three times the latency of the first, if it weren't for a hardware
mechanism that notices the regular access patterns, and pre-emptively
loads further cachelines; see section&nbsp;
1.3.5
.)
</p>

<p name="switchToTextMode">
Some applications naturally lead to strides greater than&nbsp;1, for
instance, accessing only the real parts of an array of complex
numbers (for some remarks on the practical realization of complex
numbers see section&nbsp;
3.7.6
). Also, methods that use
<i>recursive doubling</i>
 often have a code structure that
exhibits non-unit strides
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N/2; i++)
  x[i] = y[2*i];
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

In this discussion of cachelines, we have implicitly assumed the
beginning of a cacheline is also the beginning of a word, be that an
integer or a floating point number. This need not be true: an 8-byte
floating point number can be placed straddling the boundary between
two cachelines. You can image that this is not good for
performance. Section&nbsp;
sec:memalign
 discusses ways to address
<i>cacheline boundary alignment</i>
in practice.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="Cachemapping">1.3.4.8</a> Cache mapping</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Cachemapping">Cache mapping</a>
</p>
</p>

<p name="switchToTextMode">
Caches get faster, but also smaller, the closer to the 
<span title="acronym" ><i>FPUs</i></span>
 they
get, yet even the largest cache is considerably smaller than the main
memory size. In section&nbsp;
1.3.4.6
 we have already discussed how
the decision is made which elements to keep and which to replace.
</p>

<p name="switchToTextMode">
We will now address the issue
of 
<i>cache mapping</i>
, which is the question of `if
an item is placed in cache, where does it get placed'. This problem is
generally addressed by mapping the (main memory) address of the item
to an address in cache, leading to the question `what if two items get
mapped to the same address'.
</p>

<h4><a id="Directmappedcaches">1.3.4.9</a> Direct mapped caches</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Directmappedcaches">Direct mapped caches</a>
</p>

<p name="switchToTextMode">

The simplest cache mapping strategy is 
  mapping}. Suppose that memory addresses are 32 bits long, so that
they can address 4G bytes\footnote
  {We implicitly use the convention
  that K,M,G suffixes refer to powers of 2 rather than&nbsp;10: 1K=1024,
  1M=1,048,576, 1G=1,073,741,824.}; suppose further that the
cache has 8K words, that is, 64K&nbsp;bytes, needing 16 bits to address.
Direct mapping then takes from each memory address the last (`least
significant') 16&nbsp;bits,
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/directmap.jpeg" width=800></img>
<p name="caption">
FIGURE 1.8: Direct mapping of 32-bit addresses into a 64K cache
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
and uses these as the address of the data item in cache; see figure&nbsp;
1.8
.
</p>

<p name="switchToTextMode">
Direct mapping is very efficient because its address calculations
can be performed very quickly, leading to low latency, but it
has a problem in practical applications. If two items are addressed
that are separated by 8K words, they will be mapped to the same cache
location, which will make certain calculations inefficient. Example:
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/directmapconflict.jpeg" width=800></img>
<p name="caption">
FIGURE 1.9: Mapping conflicts in direct mapped cache
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<!-- environment: verbatim start embedded generator -->
double A[3][8192];
for (i=0; i&lt;512; i++)
  a[2][i] = ( a[0][i]+a[1][i] )/2.;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
or in Fortran:
<!-- environment: verbatim start embedded generator -->
</p>
real*8 A(8192,3);
do i=1,512
  a(i,3) = ( a(i,1)+a(i,2) )/2
end do
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Here, the locations of 
<tt>a[0][i]</tt>
, 
<tt>a[1][i]</tt>
, and

<tt>a[2][i]</tt>
 (or 
<tt>a(i,1),a(i,2),a(i,3)</tt>
)
are 8K from each other for every&nbsp;
<tt>i</tt>
, so the last
16 bits of their addresses will be the same, and hence they will
be mapped to the same location in cache; see figure&nbsp;
1.9
.
</p>

<p name="switchToTextMode">
The execution of the loop will now
go as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The data at 
<tt>a[0][0]</tt>
 is brought into cache and
  register. This engenders a certain amount of latency. Together with
  this element, a whole cache line is transferred.
<li>
The data at 
<tt>a[1][0]</tt>
 is brought into cache (and
  register, as we will not remark anymore from now on), together
  with its whole cache line, at cost of some latency. Since this cache
  line is mapped to the same location as the first, the first cache
  line is overwritten.
<li>
In order to write the output, the cache line containing
  
<tt>a[2][0]</tt>
 is brought into memory. This is again mapped to the
  same location, causing flushing of the cache line just loaded
  for&nbsp;
<tt>a[1][0]</tt>
.
<li>
In the next iteration, 
<tt>a[0][1]</tt>
 is needed, which is
  on the same cache line as 
<tt>a[0][0]</tt>
. However, this cache line
  has been flushed, so it needs to be brought in anew from main memory
  or a deeper cache level. In doing so, it overwrites the cache line
  that holds 
<tt>a[2][0]</tt>
.
<li>
A similar story hold for 
<tt>a[1][1]</tt>
: it is on the cache
  line of \text{a[1][0]}, which unfortunately has been overwritten in
  the previous step.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
If a cache line holds four words, we see that each four iterations of
the loop involve eight transfers of elements of&nbsp;
<tt>a</tt>
, where two
would have sufficed, if it were not for the cache conflicts.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
1.3.4.9
)
  In the example of direct mapped caches, mapping from memory to cache
  was done by using the final 16 bits of a 32 bit memory address as
  cache address. Show that the problems in this example go away if the
  mapping is done by using the first (`most significant') 16 bits as
  the cache address. Why is this not a good solution in general?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
  So far, we have pretended that caching is based on virtual memory
  addresses. In reality, caching is based on 
    addresses} of the data in memory, which depend on the algorithm
  mapping virtual addresses to 
<i>memory pages</i>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Associativecaches">1.3.4.10</a> Associative caches</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Associativecaches">Associative caches</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
The problem of cache conflicts, outlined in the previous section, would
be solved if any data item could go to any cache location. In that
case there would be no conflicts, other than the cache filling up, in
which case a cache replacement policy (section&nbsp;
1.3.4.6
) would
flush data to make room for the incoming item. Such a cache is called
<i>fully associative</i>
, and while it seems optimal, it is also
very costly to build, and much slower in use than a direct mapped cache.
</p>

<p name="switchToTextMode">
For this reason, the most common solution is to have a
$k$-way 
<i>associative cache</i>
, where $k$&nbsp;is at least two. In
this case, a data item can go to any of $k$ cache locations. Code
would have to have a $k+1$-way conflict before data would be flushed
prematurely as in the above example. In that example, a value of $k=2$
would suffice, but in practice higher values are often encountered.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/assoc-mapping.jpeg" width=800></img>
<p name="caption">
FIGURE 1.10: Two caches of 12 elements: direct mapped (left) and 3-way associative (right)
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
1.10
 illustrates the mapping of memory
addresses to cache locations for a direct mapped and a 3-way associative
cache. Both caches have 12 elements, but these are used differently.
The direct mapped cache (left)
will have a conflict between memory address 0&nbsp;and&nbsp;12, but
in the 3-way associative cache these two addresses can be mapped
to any of three elements.
</p>

<p name="switchToTextMode">
As a practical example, the
<i>Intel Woodcrest</i>
 processor has
an L1 cache of 32K bytes that is 8-way set associative with a 64
  byte cache line size, and
an L2 cache of 4M bytes that is 8-way set associative with a 64
  byte cache line size.
On the other hand, the 
<i>AMD Barcelona</i>
 chip
has 2-way associativity for the L1 cache, and 8-way for
the&nbsp;L2. A&nbsp;higher associativity (`way-ness') is obviously desirable,
but makes a processor slower, since determining whether an address is
already in cache becomes more complicated. For this reason, the
associativity of the L1&nbsp;cache, where speed is of the greatest
importance, is typically lower than of the L2.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Write a small cache simulator in your favorite language. Assume a
  $k$-way associative cache of 32 entries and an architecture with 16
  bit addresses. Run the following
  experiment for $k=1,2,4,&hellip;$:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Let $k$ be the associativity of the simulated cache.
<li>
Write the translation from 16 bit memory addresses to $32/k$
    cache addresses.
<li>

 Generate 32 random machine addresses, and
    simulate storing them in cache.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
  Since the cache has 32 entries, optimally the 32 addresses can all
  be stored in cache. The chance of this actually happening is small,
  and often the data of one address will be 
<i>evicted</i>
 from the cache
  (meaning that it is overwritten) when another address conflicts with
  it. Record how many addresses, out of&nbsp;32, are actually stored in the
  cache at the end of the simulation. Do step&nbsp;
1.10
 100
  times, and plot the results; give median and average value, and the
  standard deviation. Observe that increasing the associativity
  improves the number of addresses stored. What is the limit
  behavior? (For bonus points, do a formal statistical analysis.)
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h4><a id="Cachememoryversusregularmemory">1.3.4.11</a> Cache memory versus regular memory</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Cachememoryversusregularmemory">Cache memory versus regular memory</a>
</p>
<p name="switchToTextMode">

So what's so special about cache memory; why don't we use its
technology for all of memory?
</p>

<p name="switchToTextMode">
Caches typically
consist of 
<i>SRAM</i>
, which is faster than 
<i>DRAM</i>
used for the main memory, but is also more expensive, taking 5--6
transistors per bit rather than one, and it draws more power.
</p>

<h4><a id="Loadsversusstores">1.3.4.12</a> Loads versus stores</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Caches">Caches</a> > <a href="sequential.html#Loadsversusstores">Loads versus stores</a>
</p>
<p name="switchToTextMode">

In the above description, all data accessed in the program needs to be
moved into the cache before the instructions using it can
execute. This holds both for data that is read and data that is
written. However, data that is written, and that will not be needed
again (within some reasonable amount of time) has no reason for
staying in the cache, potentially creating conflicts or evicting data
that can still be reused. For this reason, compilers often have
support for 
<i>streaming stores</i>
: a&nbsp;contiguous stream of
data that is purely output will be written straight to memory, without
being cached.
</p>

<h3><a id="Prefetchstreams">1.3.5</a> Prefetch streams</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Prefetchstreams">Prefetch streams</a>
</p>
<!-- index -->

<p name="switchToTextMode">

In the traditional von Neumann model (section&nbsp;
1.1
),
each instruction contains the location of its operands, so a 
<span title="acronym" ><i>CPU</i></span>
implementing this model would make a separate request for each new
operand. In practice, often subsequent data items are adjacent or
regularly spaced in memory. The memory system can try to detect such
data patterns by looking at cache miss points,
and request a 
<i>prefetch data stream</i>
;
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/prefetch.jpeg" width=800></img>
<p name="caption">
FIGURE 1.11: Prefetch stream generated by equally spaced requests
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
figure&nbsp;
1.11
.
</p>

<p name="switchToTextMode">
In its simplest form, the 
<span title="acronym" ><i>CPU</i></span>
 will detect that consecutive loads come
from two consecutive cache lines, and automatically issue a request
for the next following cache line. This process can be repeated or
extended if the code makes an actual request for that third cache
line. Since these cache lines are now brought from memory well before
they are needed, prefetch has the possibility of eliminating the
latency for all but the first couple of data items.
</p>

<p name="switchToTextMode">
The concept of 
<i>cache miss</i>
 now needs to be revisited a
little. From a performance point of view we are only interested in
<i>stall</i>
s on cache misses, that is, the case where the
computation has to wait for the data to be brought in. Data that is
not in cache, but can be brought in while other instructions are still
being processed, is not a problem. If an `L1 miss' is understood to be
only a `stall on miss', then the term `L1 cache refill' is used to
describe all cacheline loads, whether the processor is stalling on
them or not.
</p>

<p name="switchToTextMode">
Since prefetch is controlled by the hardware, it is also described as
<i>hardware prefetch</i>
.
Prefetch streams can sometimes be
controlled from software, for instance through 
<i>intrinsics</i>
.
</p>

<p name="switchToTextMode">
Introducing prefetch by the programmer is a careful balance of a
number of factors&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Guttman:prefetchKNC">[Guttman:prefetchKNC]</a>
. Prime among these is the
<i>prefetch distance</i>
: the number of cycles between the
start of the prefetch and when the data is needed. In practice, this
is often the number of iterations of a loop: the prefetch instruction
requests data for a future iteration.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Concurrencyandmemorytransfer">1.3.6</a> Concurrency and memory transfer</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Concurrencyandmemorytransfer">Concurrency and memory transfer</a>
</p>
</p>

<p name="switchToTextMode">
In the discussion about the memory hierarchy we made the point that
memory is slower than the processor. As if that is not bad enough, it
is not even trivial to exploit all the bandwidth that memory
offers. In other words, if you don't program carefully you will get
even less performance than you would expect based on the available
bandwidth. Let's analyze this.
</p>

<p name="switchToTextMode">
The memory system typically has a bandwidth of more than one floating
point number per cycle, so you need to issue that many requests per
cycle to utilize the available bandwidth. This would be true even with
zero latency; since there is latency, it takes a while for data to
make it from memory and be processed. Consequently, any data requested
based on computations on the first data has to be requested with a
delay at least equal to the memory latency.
</p>

<p name="switchToTextMode">
For full utilization of the bandwidth,
at all times a volume of data equal to the bandwidth times
the latency has to be in flight. Since these data have to be
independent, we get a statement of 
  law}&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Little:law">[Little:law]</a>
:
\[
 \mathrm{Concurrency}=\mathrm{Bandwidth}\times \mathrm{Latency}. 
\]
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/little.jpg" width=800></img>
<p name="switchToTextMode">
  \caption{Illustration of Little's Law that states how much
    independent data needs to be in flight}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
This is illustrated in figure&nbsp;
1.12
. The problem with
maintaining this concurrency is not that a program does not have it;
rather, the program is to get the compiler and runtime system
recognize it. For instance, if a loop traverses a long array, the
compiler will not issue a large number of memory requests. The
prefetch mechanism (section&nbsp;
1.3.5
) will issue some memory
requests ahead of time, but typically not enough. Thus, in order to
use the available bandwidth, multiple streams of data need to be under
way simultaneously. Therefore, we can also phrase Little's law as
\[
 \mathrm{Effective\ throughput}=\mathrm{Expressed\ concurrency} / \mathrm{Latency}. 
\]
</p>

<h3><a id="Memorybanks">1.3.7</a> Memory banks</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#Memorybanks">Memory banks</a>
</p>

<p name="switchToTextMode">

Above, we discussed issues relating to bandwidth. You saw that memory,
and to a lesser extent caches, have a bandwidth that is less than what
a processor can maximally absorb. The situation is actually even worse
than the above discussion made it seem. For this reason, memory is
often divided into 
<i>memory banks</i>
 that are interleaved: with
four memory banks, words $0,4,8,&hellip;$ are in bank&nbsp;0, words
$1,5,9,&hellip;$ are in bank&nbsp;1, et cetera.
</p>

<p name="switchToTextMode">
Suppose we now access memory sequentially, then such 4-way interleaved
memory can sustain four times the bandwidth of a single memory
bank. Unfortunately, accessing by stride&nbsp;2 will halve the bandwidth,
and larger strides are even worse.
If two consecutive operations
access the same memory bank, we speak of a
<i>bank conflict</i>
<!-- index -->
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Bailey:conflict">[Bailey:conflict]</a>
.
In practice the number of memory
banks will be higher, so that strided memory access with small strides
will still have the full advertised bandwidth. For instance, the
<i>Cray-1</i>
<!-- index -->
 had 16 banks, and the
<i>Cray-2</i>
<!-- index -->
&nbsp;1024.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that with a prime number of banks, any stride up to that number
  will be conflict free. Why do you think this solution is not adopted
  in actual memory architectures?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In modern processors, 
<i>DRAM</i>
 still has banks, but the effects
of this are felt less because of the presence of caches. However,
<i>GPUs</i>
<!-- index -->
<!-- index -->
have memory banks and no caches,
so they suffer from some of the same problems as the old
supercomputers.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The 
<i>recursive doubling</i>
 algorithm for summing the
  elements of an array is:
<!-- environment: verbatim start embedded generator -->
</p>
for (s=2; s&lt;2*n; s*=2)
  for (i=0; i&lt;n-s/2; i+=s)
    x[i] += x[i+s/2]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Analyze bank conflicts for this algorithm. Assume $n=2^p$ and banks
  have $2^k$ elements where&nbsp;$k&lt;p$. Also consider this as a parallel
  algorithm where all iterations of the inner loop are independent,
  and therefore can be performed
  simultaneously.
</p>

<p name="switchToTextMode">
  Alternatively, we can use 
<i>recursive halving</i>
:
<!-- environment: verbatim start embedded generator -->
</p>
for (s=(n+1)/2; s&gt;1; s/=2)
  for (i=0; i&lt;n; i+=1)
    x[i] += x[i+s]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Again analyze bank conficts. Is this algorithm better? In the
  parallel case?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<i>Cache memory</i>
<!-- index -->
 can also use banks. For instance, the
cache lines in the L1 cache of the 
<i>AMD Barcelona</i>
 chip
are 16 words long, divided into two interleaved banks of 8 words. This
means that sequential access to the elements of a cache line is
efficient, but strided access suffers from a deteriorated performance.
</p>

<h3><a id="TLB,pages,andvirtualmemory">1.3.8</a> TLB, pages, and virtual memory</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#TLB,pages,andvirtualmemory">TLB, pages, and virtual memory</a>
</p>
<p name="switchToTextMode">

All of a program's data may not be in memory simultaneously. This can
happen for a number of reasons:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The computer serves multiple users, so the memory is not
  dedicated to any one user;
<li>
The computer is running multiple programs, which together need
  more than the physically available memory;
<li>
One single program can use more data than the available memory.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For this reason, computers use 
<i>virtual memory</i>
: if more
memory is needed than is available, certain blocks of memory are
written to disc. In effect, the disc acts as an extension of the real
memory. This means that a block of data can be anywhere in memory, and
in fact, if it is 
<i>swapped</i>
 in and out, it can be in
different locations at different times. Swapping does not act on
individual memory locations, but rather on 
<!-- index -->
contiguous blocks of memory, from a few kilobytes to megabytes in size.
(In an earlier generation of operating systems, moving memory to disc
was a programmer's responsibility. Pages that would replace each other
were called 
<i>overlays</i>
.)
</p>

<p name="switchToTextMode">
For this reason, we need a translation mechanism from the memory
addresses that the program uses to the actual addresses in memory, and
this translation has to be dynamic. A&nbsp;program has a `logical data
space' (typically starting from address zero) of the addresses used in
the compiled code, and this needs to be translated during program
execution to actual memory addresses. For this reason, there is a
<i>page table</i>
 that specifies which memory pages contain which
logical pages.
</p>

<h4><a id="Largepages">1.3.8.1</a> Large pages</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#TLB,pages,andvirtualmemory">TLB, pages, and virtual memory</a> > <a href="sequential.html#Largepages">Large pages</a>
</p>
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">
In very irregular applications, for instance databases, the page table
can get very large as more-or-less random data is brought into
memory. However, sometimes these pages show some amount of clustering,
meaning that if the page size had been larger, the number of needed
pages would be greatly reduced. For this reason, operating systems can
have support for 
<i>large pages</i>

<!-- index -->
,
typically of size around&nbsp;2Mb. (Sometimes `huge pages' are used; for
instance the
<i>Intel Knights Landing</i>
<!-- index -->
has Gigabyte pages.)
</p>

<p name="switchToTextMode">
The benefits of large pages are application-dependent: if the small
pages have insufficient clustering, use of large pages may fill up
memory prematurely with the unused parts of the large pages.
</p>

<h4><a id="TLB">1.3.8.2</a> TLB</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#MemoryHierarchies">Memory Hierarchies</a> > <a href="sequential.html#TLB,pages,andvirtualmemory">TLB, pages, and virtual memory</a> > <a href="sequential.html#TLB">TLB</a>
</p>

<p name="switchToTextMode">

However, address translation by lookup in this table is slow, so 
<span title="acronym" ><i>CPUs</i></span>
have a 
<i>TLB</i>
.
The 
<span title="acronym" ><i>TLB</i></span>
 is a cache of frequently used
Page Table Entries: it provides fast address translation for a number
of pages. If a program needs a memory
location, the 
<span title="acronym" ><i>TLB</i></span>
 is consulted to see whether this location is in fact
on a page that is remembered in the 
<span title="acronym" ><i>TLB</i></span>
.
If this is the case, the logical address is
translated to a physical one; this is a very fast process. The case
where the page is not remembered in the 
<span title="acronym" ><i>TLB</i></span>
 is called a
<i>TLB miss</i>
, and the page lookup table is then consulted,
if necessary bringing the needed page into memory.
The 
<span title="acronym" ><i>TLB</i></span>
 is (sometimes fully) associative
(section&nbsp;
1.3.4.10
), using an LRU policy
(section&nbsp;
1.3.4.6
).
</p>

<p name="switchToTextMode">
A typical 
<span title="acronym" ><i>TLB</i></span>
 has between 64 and 512 entries. If a program accesses
data sequentially, it will typically alternate between just a few
pages, and there will be no 
<span title="acronym" ><i>TLB</i></span>
 misses. On the other hand, a program
that access many random memory locations can experience a slowdown
because of such misses. The set of pages that is in current use is
called the `working set'.
</p>

<p name="switchToTextMode">
Section&nbsp;
1.7.5
 and appendix&nbsp;
sec:tlb-code
 discuss
some simple code illustrating the
behavior of the 
<span title="acronym" ><i>TLB</i></span>
.
</p>

<p name="switchToTextMode">
[There are some complications to this story. For instance, there is
  usually more than one 
<span title="acronym" ><i>TLB</i></span>
. The first one is associated with the
  L2&nbsp;cache, the second one with the&nbsp;L1. In the
<i>AMD Opteron</i>
, the L1&nbsp;
<span title="acronym" ><i>TLB</i></span>
 has 48 entries, and
  is is fully (48-way) associative, while the L2&nbsp;
<span title="acronym" ><i>TLB</i></span>
 has 512
  entries, but is only 4-way associative. This means that there can
  actually be 
<span title="acronym" ><i>TLB</i></span>
 conflicts. In the discussion above, we have
  only talked about the L2 
<span title="acronym" ><i>TLB</i></span>
. The reason that this can be
  associated with the L2&nbsp;cache, rather than with main memory, is that
  the translation from memory to L2&nbsp;cache is deterministic.]
</p>

<p name="switchToTextMode">
Use of 
<i>large pages</i>

<!-- index -->
 also reduces the
number of potential TLB misses, since the working set of pages can be
reduced.
</p>

<h2><a id="Multicorearchitectures">1.4</a> Multicore architectures</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In recent years, the limits of performance have been reached for the
traditional processor chip design.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Clock frequency can not be increased further, since it increases
  energy consumption, heating the chips too
  much; see section&nbsp;
1.8.1
.
<li>
It is not possible to extract more 
<span title="acronym" ><i>ILP</i></span>
  from codes, either because of compiler limitations, because of the
  limited amount of intrinsically available parallelism, or because
  branch prediction makes it impossible (see
  section&nbsp;
1.2.5
).
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

One of the ways of getting a higher utilization out of a single
processor chip is then to move from a strategy of further
sophistication of the single processor, to a division of the chip into
multiple processing `cores'.
The separate cores can work on unrelated
tasks, or, by introducing what is in effect data parallelism
(section&nbsp;
2.3.1
), collaborate on a common task at a higher
overall efficiency
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Olukotun:1996:single-chip">[Olukotun:1996:single-chip]</a>
.
<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
Another solution is Intel
<!-- index -->
's
<i>hyperthreading</i>
, which lets a processor mix the instructions of
  several instruction streams. The benefits of this are strongly
  dependent on the individual case. However, this same mechanism is
  exploited with great success in GPUs; see
  section&nbsp;
2.9.3
. For a discussion see
  section&nbsp;
2.6.1.9
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

This solves the above two problems:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Two cores at a lower frequency can have the same throughput as a
  single processor at a higher frequency; hence, multiple cores are
  more energy-efficient.
<li>
Discovered 
<span title="acronym" ><i>ILP</i></span>
 is now replaced by explicit task
  parallelism, managed by the programmer.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

While the first multicore 
<span title="acronym" ><i>CPUs</i></span>
were simply two processors on the same die, later generations
incorporated L3 or L2 caches that were shared between the two
processor cores; see figure&nbsp;
1.13
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/cache-hierarchy.jpg" width=800></img>
<p name="caption">
FIGURE 1.13: Cache hierarchy in a single-core and dual-core chip
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
This design makes it efficient for the cores to work
jointly on the same problem.
The cores would still have their own L1 cache, and these separate
caches lead to a 
<i>cache coherence</i>
 problem; see
section&nbsp;
1.4.1
 below.
</p>

<p name="switchToTextMode">
We note that the term `processor' is now ambiguous: it can refer to either the
chip, or the processor core on the chip. For this reason, we mostly
talk about a 
<i>socket</i>
 for the whole chip and
<i>core</i>
<!-- index -->
for the part containing one arithmetic and logic unit and
having its own registers. Currently, 
<span title="acronym" ><i>CPUs</i></span>
 with 4 or 6 cores are
common,
even in laptops, and Intel and AMD are marketing 12-core chips.
The core count is
likely to go up in the future: Intel
<!-- index -->
has already shown an 80-core prototype that is developed into the 48
core `Single-chip Cloud Computer', illustrated in
fig&nbsp;
1.14
. This chip has a structure with 24 dual-core
`tiles' that are connected through a 2D mesh network. Only certain
tiles are connected to a memory controller, others can not reach
memory other than through the on-chip network.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/ScCC-1.png" width=800></img>
<img src="graphics/ScCC-2.png" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.14: Structure of the Intel Single-chip Cloud Computer chip
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

With
this mix of shared and private caches, the programming model for
multicore processors is becoming a hybrid between shared and
distributed memory:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[
<i> Core</i>
] The cores have their own private L1 cache, which is a sort of
  distributed memory. The above mentioned Intel 80-core prototype has the
  cores communicating in a distributed memory fashion.
<li>
[
<i> Socket</i>
] On one socket, there is often a shared L2 cache, which is shared
  memory for the cores.
<li>
[
<i> Node</i>
] There can be multiple sockets on a single `node' or
  motherboard, accessing the same shared memory.
<li>
[
<i> Network</i>
] Distributed memory programming (see the next
  chapter) is needed to let nodes
  communicate.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Historically, multicore architectures have a precedent in
multiprocessor shared memory designs (section&nbsp;
2.4.1
) such as
the 
<i>Sequent Symmetry</i>
 and the 
  FX/8}. Conceptually the program model is the same, but the
technology now allows to shrink a multiprocessor board to a multicore
chip.
</p>

<h3><a id="Cachecoherence">1.4.1</a> Cache coherence</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#Cachecoherence">Cache coherence</a>
</p>

<!-- index -->
<p name="switchToTextMode">

With parallel processing, there is the potential for a conflict if more
than one processor has a copy of the same data item. The problem of
ensuring that all cached data are an accurate copy of main memory is
referred to as 
<i>cache coherence</i>
: if one processor alters
its copy, the other copy needs to be updated.
</p>

<p name="switchToTextMode">
In distributed memory architectures, a dataset is usually partitioned
disjointly over the processors, so conflicting copies of data can only
arise with knowledge of the user, and it is up to the user to
deal with the problem. The case of shared memory is more subtle: since
processes access the same main memory, it would seem that conflicts
are in fact impossible. However, processors typically have some private
cache that contains copies of data from memory, so conflicting
copies can occur.  This situation arises in particular in multicore
designs.
</p>

<p name="switchToTextMode">
Suppose that two cores have a copy of the same data item in their
(private) L1 cache, and one modifies its copy. Now the other has
cached data that is no longer an accurate copy of its counterpart: the
processor will 
<i>invalidate</i>

<!-- index -->
 that
copy of the item, and in fact its whole cacheline. When the process
needs access to the item again, it needs to reload that cacheline.
The alternative is for any core that alters data to send that
cacheline to the other cores. This strategy probably has a higher overhead,
since other cores are not likely to have a copy of a cacheline.
</p>

<p name="switchToTextMode">
This process of updating or invalidating cachelines
is known as 
<i>maintaining cache coherence</i>
, and it is done on
a very low level of the processor, with no programmer involvement
needed. (This makes updating memory locations an 
  operation}; more about this in section&nbsp;
2.6.1.5
.)
However, it will slow down the computation, and it wastes bandwidth to
the core that could otherwise be used for loading or storing operands.
</p>

<p name="switchToTextMode">
The state of a cache line with respect to a data item in main memory
is usually described as one of the following:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[Scratch:] the cache line does not contain a copy of the item;
<li>
[Valid:] the cache line is a correct copy of data in main memory;
<li>
[Reserved:] the cache line is the 
<i>only</i>
 copy of that piece
  of data;
<li>
[Dirty:] the cache line has been modified, but not yet written
  back to main memory;
<li>
[Invalid:] the data on the cache line is also present on other
  processors (it is not 
<i>reserved</i>
), and another process has
  modified its copy of the data.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

A simpler variant of this is the 
<span title="acronym" ><i>MSI</i></span>
 coherence protocol, where a
cache line can be in the following states on a given core:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[Modified:] the cacheline has been modified, and needs to be
  written to the backing store. This writing can be done when the line
  is 
<i>evicted</i>
, or it is done immediately, depending on the
  write-back policy.
<li>
[Shared:] the line is present in at least one cache and is unmodified.
<li>
[Invalid:] the line is not present in the current cache, or it
  is present but a copy in another cache has been modified.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

These states control the movement of cachelines between memory and the
caches. For instance, suppose a core does a read to a cacheline that
is invalid on that core. It can then load it from memory or get it
from another cache, which may be faster. (Finding whether a line exists
(in state M or&nbsp;S) on another cache is called 
<i>snooping</i>
; an alternative
is to maintain cache directories; see below.) If
the line is Shared, it can now simply be copied; if it is in state&nbsp;M in
the other cache, that core first needs to write it back to memory.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider two processors, a data item $x$ in memory, and cachelines
  $x_1$,$x_2$ in the private caches of the two processors to which $x$
  is mapped. Describe the transitions between the states of $x_1$ and
  $x_2$ under reads and writes of&nbsp;$x$ on the two processors. Also
  indicate which actions cause memory bandwidth to be used. (This list
  of transitions is a 
<i>FSA</i>
; see section&nbsp;
app:fsa
.)
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Variants of the 
<span title="acronym" ><i>MSI</i></span>
 protocol add an `Exclusive' or `Owned' state
for increased efficiency.
</p>

<h4><a id="Solutionstocachecoherence">1.4.1.1</a> Solutions to cache coherence</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#Cachecoherence">Cache coherence</a> > <a href="sequential.html#Solutionstocachecoherence">Solutions to cache coherence</a>
</p>
<p name="switchToTextMode">

There are two basic  mechanisms for realizing cache coherence:
snooping and directory-based schemes.
</p>

<p name="switchToTextMode">
In the 
<i>snooping</i>
 mechanism, any request for data is sent
to all caches, and the data is returned if it is present anywhere;
otherwise it is retrieved from memory. In a variation on this scheme,
a core
`listens in' on all bus traffic, so that it can invalidate or update its own cacheline
copies when another core modifies its copy. Invalidating is cheaper than updating
since it is a bit operation, while updating involves copying the whole cacheline.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  When would updating pay off? Write a simple cache simulator to evaluate this question.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Since snooping often involves broadcast information to all cores, it
does not scale beyond a small number of cores. A&nbsp;solution that scales
better is using a 
<i>tag directory</i>
: a central directory
that contains the information on what data is present in some cache,
and what cache it is in specifically. For processors with large
numbers of cores (such as the 
<i>Intel Xeon Phi</i>
) the
directory can be distributed over the cores.
</p>

<h4><a id="Falsesharing">1.4.1.2</a> False sharing</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#Cachecoherence">Cache coherence</a> > <a href="sequential.html#Falsesharing">False sharing</a>
</p>

<p name="switchToTextMode">

The cache coherence problem can even appear if the cores access
different items. For instance, a declaration
<!-- environment: verbatim start embedded generator -->
</p>
  double x,y;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
will likely allocate 
<tt>x</tt>
 and&nbsp;
<tt>y</tt>
 next to each other in memory, so
there is a high chance they fall on the same cacheline. Now if one
core updates&nbsp;
<tt>x</tt>
 and the other&nbsp;
<tt>y</tt>
, this cacheline will
continuously be moved between the cores. This is called
<i>false sharing</i>
.
</p>

<p name="switchToTextMode">
The most common case of false sharing happens when
threads update consecutive locations of an array. For instance, in the
following OpenMP fragment all threads update their own location in an
array of partial results:
<!-- environment: verbatim start embedded generator -->
</p>
  local_results = new double[num_threads];
#pragma omp parallel
{
  int thread_num = omp_get_thread_num();
  for (int i=my_lo; i&lt;my_hi; i++)
    local_results[thread_num] = ... f(i) ...
}
global_result = g(local_results)
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
While there is no actual 
<i>race condition</i>
 (as there would be
if the threads all updated the 
<tt>global_result</tt>
 variable),
this code will have low performance, since the cacheline(s) with the

<tt>local_result</tt>
 array will continuously be invalidated.
</p>

<h4><a id="Tagdirectories">1.4.1.3</a> Tag directories</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#Cachecoherence">Cache coherence</a> > <a href="sequential.html#Tagdirectories">Tag directories</a>
</p>
<p name="switchToTextMode">

In multicore processors with distributed, but coherent, caches
(such as the 
<i>Intel Xeon Phi</i>
)
the
<i>tag directories</i>
<!-- index -->
can themselves be distributed. This increases the latency of cache lookup.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Computationsonmulticorechips">1.4.2</a> Computations on multicore chips</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#Computationsonmulticorechips">Computations on multicore chips</a>
</p>
</p>

<p name="switchToTextMode">
There are various ways that a multicore processor can lead to
increased performance. First of all, in a desktop situation, multiple
cores can actually run multiple programs. More importantly, we can use
the parallelism to speed up the execution of a single code. This can
be done in two different ways.
</p>

<p name="switchToTextMode">
The MPI library (section&nbsp;
2.6.3.3
) is typically used to
communicate between processors that are connected through a
network. However, it can also be used in a single multicore processor:
the MPI calls then are realized through shared memory copies.
</p>

<p name="switchToTextMode">
Alternatively, we can use the shared memory and shared caches and
program using threaded systems such as OpenMP
(section&nbsp;
2.6.2
). The advantage of this mode is that
parallelism can be much more dynamic, since the runtime system can set
and change
the correspondence between threads and cores during the program run.
</p>

<p name="switchToTextMode">
We will discuss in some detail the
scheduling of linear algebra operations on multicore chips;
section&nbsp;
6.11
.
</p>

<h3><a id="TLBshootdown">1.4.3</a> TLB shootdown</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Multicorearchitectures">Multicore architectures</a> > <a href="sequential.html#TLBshootdown">TLB shootdown</a>
</p>
<p name="switchToTextMode">

Section&nbsp;
1.3.8.2
 explained how the 
<span title="acronym" ><i>TLB</i></span>
 is used to cache
the translation from logical address, and therefore logical page, to
physical page. The TLB is part of the memory unit of the 
<i>socket</i>
, so in
a multi-socket design, it is possible for a process on one socket to
change the page mapping, which makes the mapping on the other
incorrect.
</p>

<p name="switchToTextMode">
One solution to this problem is called 
<i>TLB shoot-down</i>
:
the process changing the mapping generates an
<i>Inter-Processor Interrupt</i>
, which causes the other
processors to rebuild their TLB.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Nodearchitectureandsockets">1.5</a> Node architecture and sockets</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Nodearchitectureandsockets">Node architecture and sockets</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
In the previous sections we have made our way down through the memory
hierarchy, visiting registers and various cache levels, and the extent
to which they can be private or shared. At the bottom level of the
memory hierarchy is the memory that all cores share. This can range
from a few Gigabyte on a lowly laptop to a few Terabyte in some
supercomputer centers.
</p>

<p name="switchToTextMode">
While this memory is shared between all cores, there is some structure
to it. This derives from the fact that cluster 
<i>node</i>
 can
have more than one 
<i>socket</i>
, that is, processor chip.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
  \hbox{
<img src="graphics/ranger-node-small.jpg" width=800></img>
<img src="graphics/stampede-node.jpg" width=800></img>
    }
<p name="caption">
FIGURE 1.15: Left: a four-socket design. Right: a&nbsp;two-socket design with co-processor.
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
The shared memory on the node is typically spread over banks that are
directly attached to one particular socket.
This is for instance illustrated in figure&nbsp;
1.15
,
which shows the four-socket node of the 
<i>TACC Ranger cluster</i>
supercomputer (no longer in production) and the two-socket node of the
<i>TACC Stampede cluster</i>
supercomputer which contains an 
<i>Intel Xeon Phi</i>
co-processor.
In both designs you clearly see the memory chips that are directly
connected to the sockets.
</p>

<p name="switchToTextMode">
This is an example of 
<i>NUMA</i>
 design: for a process running on
some core, the memory attached to its socket is slightly faster to
access than the memory attached to another socket.
</p>

<p name="switchToTextMode">
One result of this is the 
<i>first-touch</i>
 phenomenon.
Dynamically allocated memory is not actually allocated until it's
first written to. Consider now the following OpenMP
(section&nbsp;
2.6.2
) code:
<!-- environment: verbatim start embedded generator -->
</p>
double *array = (double*)malloc(N*sizeof(double));
for (int i=0; i&lt;N; i++)
   array[i] = 1;
#pragma omp parallel for
for (int i=0; i&lt;N; i++)
   .... lots of work on array[i] ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Because of first-touch, the array is allocated completely in the
memory of the socket of the main thread. In the subsequently parallel
loop the cores of the other socket will then have slower access to the
memory they operate on.
</p>

<p name="switchToTextMode">
The solution here is to also make the initialization loop parallel,
even if the amount of work in it may be negligible.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Localityanddatareuse">1.6</a> Locality and data reuse</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a>
</p>
</p>

<p name="switchToTextMode">
By now it should be clear that there is more to the execution of an
algorithm than counting the operations: the data transfer involved is
important, and can in fact dominate the cost. Since we have caches and
registers, the amount of data transfer can be minimized by programming
in such a way that data stays as close to the processor as
possible. Partly this is a matter of programming cleverly, but we can
also look at the theoretical question: does the algorithm allow for it
to begin with.
</p>

<p name="switchToTextMode">
It turns out that in scientific computing data often
interacts mostly with data that is close by in some sense, which will
lead to data locality; section&nbsp;
1.6.2
. Often such locality
derives from the nature of the application, as in the case of the 
<span title="acronym" ><i>PDEs</i></span>
 you
will see in chapter&nbsp;
Numerical treatment of differential equations
. In other cases such as molecular
dynamics (chapter&nbsp;
app:md
) there is no such intrinsic locality
because all particles interact with all others,
and considerable programming cleverness is needed to get high performance.
</p>

<h3><a id="Datareuseandarithmeticintensity">1.6.1</a> Data reuse and arithmetic intensity</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Datareuseandarithmeticintensity">Data reuse and arithmetic intensity</a>
</p>



<p name="switchToTextMode">

In the previous sections you learned that processor design is somewhat unbalanced:
loading data is slower than executing the actual operations.
This imbalance is large for main memory and less for the various cache levels.
Thus we are motivated to keep data in cache and
keep the amount of 
<i>data reuse</i>
  as high as possible.
</p>

<p name="switchToTextMode">
Of course, we need to determine first if the computation allows for data to be
reused.
For this we define the 
<i>arithmetic intensity</i>
of an algorithm as follows:
<!-- environment: quote start embedded generator -->
</p>
<!-- TranslatingLineGenerator quote ['quote'] -->
  If $n$ is the number of data items that an algorithm operates on, and
  $f(n)$ the number of operations it takes, then the arithmetic intensity is
  $f(n)/n$.
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
(We can measure data items in either floating point numbers or bytes.
The latter possibility makes it easier to relate arithmetic intensity
to hardware specifications of a processor.)
</p>

<p name="switchToTextMode">
Arithmetic intensity is also related to 
<i>latency hiding</i>
:
the concept that you can mitigate the negative performance impact
of data loading
behind computational activity going on.
For this to work, you need more computations than data loads to make
this hiding effective. And that is the very definition of
computational intensity: a high ratio of operations per
byte/word/number loaded.
</p>

<h4><a id="Example:vectoroperations">1.6.1.1</a> Example: vector operations</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Datareuseandarithmeticintensity">Data reuse and arithmetic intensity</a> > <a href="sequential.html#Example:vectoroperations">Example: vector operations</a>
</p>
<p name="switchToTextMode">

Consider for example the vector addition
\[
 \forall_i\colon x_i\leftarrow x_i+y_i.
\]
This involves three memory accesses (two loads and one store)
and one operation per iteration,
giving a arithmetic intensity of&nbsp;$1/3$. The 
<i>axpy</i>
 (for
`
<i>a</i>
&nbsp;times 
<i>x</i>
 plus&nbsp;
<i>y</i>
) operation
\[
 \forall_i\colon x_i\leftarrow a\,x_i+ y_i
\]
has
two operations, but the same number of memory access since the
one-time load of&nbsp;$a$ is amortized. It is therefore more efficient
than the simple addition, with a reuse of&nbsp;$2/3$.
</p>

<p name="switchToTextMode">
The inner product calculation
\[
 \forall_i\colon s\leftarrow s+x_i\cdot y_i
\]
is similar in structure to the axpy operation, involving one
multiplication and addition per iteration, on two vectors and one
scalar. However, now there are only two load operations, since $s$&nbsp;can
be kept in register and only written back to memory at the end of the
loop. The reuse here is&nbsp;$1$.
</p>

<h4><a id="Example:matrixoperations">1.6.1.2</a> Example: matrix operations</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Datareuseandarithmeticintensity">Data reuse and arithmetic intensity</a> > <a href="sequential.html#Example:matrixoperations">Example: matrix operations</a>
</p>

<p name="switchToTextMode">

Next, consider the 
<i>matrix-matrix product</i>

<!-- index -->
:
\[
 \forall_{i,j}\colon c_{ij} = \sum_k a_{ik}b_{kj}. 
\]
 This involves
$3n^2$ data items and $2n^3$ operations, which is of a
higher order. The arithmetic intensity is&nbsp;$O(n)$, meaning that every data item
will be used $O(n)$ times.  This has the implication that, with
suitable programming, this operation has the potential of overcoming
the bandwidth/clock speed gap by keeping data in fast cache memory.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The matrix-matrix product, considered 
<i>as operation</i>
, clearly
  has data reuse by the above definition. Argue that this reuse is not
  trivially attained by a simple implementation. What determines
  whether the naive implementation has  reuse of data that is in cache?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

[In this discussion we were only concerned with the number of
  operations of a given 
<i>implementation</i>
, not the mathematical
  
<i>operation</i>
. For instance, there are ways of performing the
  matrix-matrix multiplication and Gaussian elimination algorithms in
  fewer than $O(n^3)$
  operations&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#St:gaussnotoptimal,Pa:combinations">[St:gaussnotoptimal,Pa:combinations]</a>
. However, this
  requires a different implementation, which has its own analysis in
  terms of memory access and reuse.]
</p>

<p name="switchToTextMode">
The matrix-matrix product is the heart of the 
{benchmark}&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Dongarra1987LinpackBenchmark">[Dongarra1987LinpackBenchmark]</a>
; see
section&nbsp;
2.11.6
. Using this as the sole measure of
<i>benchmarking</i>
 a computer may give an optimistic view of its
performance: the matrix-matrix product is an operation
that has considerable data reuse, so it is relatively insensitive to
memory bandwidth and, for parallel computers, properties of the
network. Typically, computers will attain 60--90\% of their
<i>peak performance</i>
 on the Linpack benchmark. Other benchmark
may give considerably lower figures.
</p>

<h4><a id="Therooflinemodel">1.6.1.3</a> The roofline model</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Datareuseandarithmeticintensity">Data reuse and arithmetic intensity</a> > <a href="sequential.html#Therooflinemodel">The roofline model</a>
</p>

<!-- index -->
<p name="switchToTextMode">

There is an elegant way of talking about how arithmetic intensity,
which is a statement about the ideal algorithm, not its implementation,
interacts with hardware parameters and the actual implementation
to determine performance.
This is known as the 
<i>roofline model</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Williams:2009:roofline">[Williams:2009:roofline]</a>
,
and it expresses the basic fact that performance is bounded by two factors,
illustrated in the first graph of figure&nbsp;
1.16
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">

<img src="graphics/roofline1.jpeg" width=800></img>
<img src="graphics/roofline2.jpeg" width=800></img>
<img src="graphics/roofline3.jpeg" width=800></img>
</p>

<p name="caption">
FIGURE 1.16: Illustration of factors determining performance in the roofline model
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<!-- environment: enumerate start embedded generator -->
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
The 
<i>peak performance</i>
, indicated by the horizontal
  line at the top of the graph, is an absolute bound on the
  performance\footnote
    {An old joke states that the peak performance
    is that number that the manufacturer guarantees you will never
    exceed}, achieved only if every aspect of a 
<span title="acronym" ><i>CPU</i></span>
 (pipelines,
  multiple floating point units) are perfectly used. The calculation
  of this number is purely based on 
<span title="acronym" ><i>CPU</i></span>
 properties and clock cycle; it
  is assumed that memory bandwidth is not a limiting factor.
<li>
The number of operations per second is also limited by the
  product of the bandwidth, an absolute number, and the arithmetic
  intensity:
\[
 \frac{\hbox
<i> operations</i>
}{\hbox
<i> second</i>
}=
  \frac{\hbox
<i> operations</i>
}{\hbox
<i> data item</i>
}\cdot
  \frac{\hbox
<i> data items</i>
}{\hbox
<i> second</i>
}
\]
  This is depicted by the linearly increasing line in the graph.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
The roofline model is an elegant way of expressing that various
factors lower the ceiling.  For instance, if an algorithm fails to use
the full 
<i>SIMD width</i>
, this imbalance lowers the
attainable peak.  The second graph in figure&nbsp;
1.16
indicates various factors that lower the ceiling.
There are also various factors that lower the available bandwidth,
such as imperfect data hiding. This is indicated by a lowering of the
sloping roofline in the third graph.
</p>

<p name="switchToTextMode">
For a given arithmetic intensity, the performance is determined by
where its vertical line intersects the roof line. If this is at the
horizontal part, the computation is called 
<i>compute-bound</i>
:
performance is determined by characteristics of the processor, and
bandwidth is not an issue. On the other hand, if that vertical line
intersects the sloping part of the roof, the computation is called
<i>bandwidth-bound</i>
: performance is determined by the memory
subsystem, and the full capacity of the processor is not used.
</p>

<!-- index -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  How would you determine whether a given program kernel is bandwidth
  or compute bound?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Locality">1.6.2</a> Locality</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Locality">Locality</a>
</p>

</p>

<p name="switchToTextMode">
Since using data in cache is cheaper than getting data from main
memory, a programmer obviously wants to code in such a way that data
in cache is reused. While placing data in cache is not under explicit
programmer control, even from assembly language
(low level memory access can be controlled by the
    programmer in the Cell processor and in some GPUs.),
in most 
<span title="acronym" ><i>CPUs</i></span>
, it is still
possible, knowing the behavior of the caches, to know
what data is in cache, and to some extent to control it.
</p>

<p name="switchToTextMode">
The two crucial concepts here are
<i>temporal locality</i>
  locality|see{locality, temporal}} and
<i>spatial locality</i>
    spatial}}. Temporal locality is the easiest to explain: this
describes the use of a data element within a short time of its last
use.  Since most caches have an 
<span title="acronym" ><i>LRU</i></span>
 replacement policy
(section&nbsp;
1.3.4.6
), if in between the two references less data
has been referenced than the cache size, the element will still be in
cache and therefore be quickly accessible. With other replacement
policies, such as random replacement, this guarantee can not be made.
</p>

<h4><a id="Temporallocality">1.6.2.1</a> Temporal locality</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Locality">Locality</a> > <a href="sequential.html#Temporallocality">Temporal locality</a>
</p>
<p name="switchToTextMode">

As an example of temporal locality, consider the repeated use of a
long vector:
<!-- environment: verbatim start embedded generator -->
</p>
for (loop=0; loop&lt;10; loop++) {
  for (i=0; i&lt;N; i++) {
    ... = ... x[i] ...
  }
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Each element of 
<tt>x</tt>
 will be used 10 times, but if the vector
(plus other data accessed) exceeds
the cache size, each element will be flushed before its
next use. Therefore, the use of 
<tt>x[i]</tt>
 does not exhibit temporal
locality: subsequent uses are spaced too far apart in time for it to
remain in cache.
</p>

<p name="switchToTextMode">
If the structure of the computation allows us to exchange
the loops:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N; i++) {
  for (loop=0; loop&lt;10; loop++) {
    ... = ... x[i] ...
  }
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the elements of 
<tt>x</tt>
 are now repeatedly reused, and are
therefore more likely to remain in the cache. This rearranged code
displays better
temporal locality in its use of&nbsp;
<tt>x[i]</tt>
.
</p>

<h4><a id="Spatiallocality">1.6.2.2</a> Spatial locality</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Locality">Locality</a> > <a href="sequential.html#Spatiallocality">Spatial locality</a>
</p>
<p name="switchToTextMode">

The concept of 
<i>spatial locality</i>
 is slightly more
involved. A program is said to exhibit spatial locality if it
references memory that is `close' to memory it already referenced. In
the classical von Neumann architecture with only a processor and
memory, spatial locality should be irrelevant, since one address in
memory can be as quickly  retrieved as any other. However, in a modern 
<span title="acronym" ><i>CPU</i></span>
with caches, the story is different. Above, you have seen two examples
of spatial locality:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Since data is moved in 
<i>cache line</i>
s rather than
  individual words or bytes,
  there is a great benefit to coding in such a manner that all
  elements of the cacheline are used. In the loop
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N*s; i+=s) {
    ... x[i] ...
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  spatial locality is a decreasing function of the
<i>stride</i>
&nbsp;
<tt>s</tt>
.
</p>

<p name="switchToTextMode">
  Let 
<tt>S</tt>
&nbsp;be the cacheline size, then as 
<tt>s</tt>
 ranges from $1&hellip;\mathtt{S}$,
  the number of elements used of each cacheline goes
  down from&nbsp;
<tt>S</tt>
 to&nbsp;1. Relatively speaking, this increases the cost
  of memory traffic in the loop: if $\mathtt{s}=1$, we load $1/\mathtt{S}$ cachelines
  per element; if $\mathtt{s}=\mathtt{S}$, we load one cacheline for each element. This
  effect is demonstrated in section&nbsp;
1.7.4
.
<li>
A second example of spatial locality worth observing involves
  the 
<span title="acronym" ><i>TLB</i></span>
 (section&nbsp;
1.3.8.2
). If a program references
  elements that are close together, they are likely on the same memory
  page, and address translation through the TLB will be fast. On the
  other hand, if a program references many widely disparate elements,
  it will also be referencing many different pages. The resulting TLB
  misses are very costly; see also section&nbsp;
1.7.5
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the following pseudocode of an algorithm for summing $n$
  numbers $x[i]$ where $n$ is a power of&nbsp;2:
<!-- environment: verbatim start embedded generator -->
</p>
for s=2,4,8,...,n/2,n:
  for i=0 to n-1 with steps s:
    x[i] = x[i] + x[i+s/2]
sum = x[0]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Analyze the spatial and temporal locality of this algorithm, and
  contrast it with the standard algorithm
<!-- environment: verbatim start embedded generator -->
</p>
sum = 0
for i=0,1,2,...,n-1
  sum = sum + x[i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
Consider the following code, and assume that 
<tt>nvectors</tt>
 is small
compared to the cache size, and 
<tt>length</tt>
 large.
<!-- environment: verbatim start embedded generator -->
</p>
for (k=0; k&lt;nvectors; k++)
  for (i=0; i&lt;length; i++)
    a[k,i] = b[i] * c[k]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
How do the following concepts relate to the performance of this code:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Reuse
<li>
Cache size
<!-- environment: answer start embedded generator -->
</p>

</answer>
<!-- environment: answer end embedded generator -->
<li>
Associativity
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Would the following code where the loops are exchanged
perform better or worse, and why?
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;length; i++)
  for (k=0; k&lt;nvectors; k++)
    a[k,i] = b[i] * c[k]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Examplesoflocality">1.6.2.3</a> Examples of locality</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Locality">Locality</a> > <a href="sequential.html#Examplesoflocality">Examples of locality</a>
</p>
</p>

<p name="switchToTextMode">
Let us examine locality issues for a realistic example. The
matrix-matrix multiplication $C\leftarrow A\cdot B$ can be computed
in several ways. We compare two implementations, assuming that all
matrices are stored by rows, and that the cache size is insufficient
to store a whole row or column.
</p>

<p name="switchToTextMode">
\hbox{%
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
\setbox0=\hbox{%
<!-- environment: minipage start embedded generator -->
</p>
<!-- TranslatingLineGenerator minipage ['minipage'] -->
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  for j=1..n
    for k=1..n
      c[i,j] += a[i,k]*b[k,j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</minipage>
<!-- environment: minipage end embedded generator -->
<p name="switchToTextMode">
{.4\textwidth}
\ht0=1.2\ht0 \box0
\,
<!-- environment: minipage start embedded generator -->
</p>
<!-- TranslatingLineGenerator minipage ['minipage'] -->
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  for k=1..n
    for j=1..n
      c[i,j] += a[i,k]*b[k,j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</minipage>
<!-- environment: minipage end embedded generator -->
<p name="switchToTextMode">
{.4\textwidth}
}%
</p name="quotation">
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">
These implementations are illustrated in figure&nbsp;
1.17
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/ijk-mult.jpeg" width=800></img>
<p name="switchToTextMode">
  \caption{Two loop orderings for the $C\leftarrow A\cdot B$
    matrix-matrix product}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
The first implementation constructs the $(i,j)$ element of&nbsp;$C$ by the
inner product of a row of&nbsp;$A$ and a column of&nbsp;$B$, in the second a row
of&nbsp;$C$ is updated by scaling rows of&nbsp;$B$ by elements of&nbsp;$A$.
</p>

<p name="switchToTextMode">
Our first observation is that both implementations
indeed compute $C\leftarrow
C+A\cdot B$, and that they both take roughly $2n^3$
operations. However, their memory behavior, including spatial and
temporal locality is very different.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[
<tt>c[i,j]</tt>
] In the first implementation, 
<tt>c[i,j]</tt>
 is
  invariant in the inner iteration, which constitutes temporal
  locality,
  so it can be kept in register. As
  a result, each element of&nbsp;$C$ will be loaded and stored only once.
</p>

<p name="switchToTextMode">
  In the second implementation, 
<tt>c[i,j]</tt>
 will be loaded and stored
  in each inner iteration. In particular, this implies that there are
  now $n^3$ store operations, a&nbsp;factor of&nbsp;$n$ more than the first
  implementation.
<li>
[
<tt>a[i,k]</tt>
] In both implementations, 
<tt>a[i,k]</tt>
 elements are
  accessed by rows, so there is good spatial locality, as each loaded
  cacheline will be used entirely. In the second implementation,
  
<tt>a[i,k]</tt>
&nbsp;is invariant in the inner loop, which constitutes
  temporal locality; it can be kept in register. As a result, in the
  second case $A$&nbsp;will be loaded only once, as opposed to $n$&nbsp;times in
  the first case.
<li>
[
<tt>b[k,j]</tt>
] The two implementations differ greatly in how they
  access the matrix&nbsp;$B$. First of all, 
<tt>b[k,j]</tt>
&nbsp;is never invariant
  so it will not be kept in register, and $B$&nbsp;engenders $n^3$&nbsp;memory
  loads in both cases. However, the access patterns differ.
</p>

<p name="switchToTextMode">
  In second case, 
<tt>b[k,j]</tt>
&nbsp;is access by rows
  so there is good spatial locality: cachelines will be fully utilized
  after they are loaded.
</p>

<p name="switchToTextMode">
  In the first implementation, 
<tt>b[k,j]</tt>
&nbsp;is accessed by
  columns. Because of the row storage of the matrices, a cacheline contains
  a part of a row, so for each cacheline loaded, only one element is
  used in the columnwise traversal. This means that the first
  implementation has more loads for&nbsp;$B$ by a factor of the cacheline
  length. There may also be TLB effects.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Note that we are not making any absolute predictions on code
performance for these implementations, or even relative comparison of
their runtimes. Such predictions are very hard to make. However, the
above discussion identifies issues that are relevant for a wide range
of classical 
<span title="acronym" ><i>CPUs</i></span>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  There are more algorithms for computing the product $C\leftarrow
  A\cdot B$. Consider the following:
<!-- environment: verbatim start embedded generator -->
</p>
for k=1..n:
  for i=1..n:
    for j=1..n:
      c[i,j] += a[i,k]*b[k,j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Analyze the memory traffic for the matrix&nbsp;$C$, and show that it is
worse than the two algorithms given above.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: comment start embedded generator -->
</p>

</comment>
<!-- environment: comment end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Corelocality">1.6.2.4</a> Core locality</h4>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Localityanddatareuse">Locality and data reuse</a> > <a href="sequential.html#Locality">Locality</a> > <a href="sequential.html#Corelocality">Core locality</a>
</p>
</p>

<p name="switchToTextMode">
The above concepts of spatial and temporal locality were mostly
properties of programs, although hardware properties such as cacheline
length and cache size play a role in analyzing the amount of
locality. There is a third type of locality that is more intimately
tied to hardware: 
<i>core locality</i>
.
</p>

<p name="switchToTextMode">
A code's execution is said to exhibit core locality if write accesses
that are spatially or temporally close are performed on the same core
or processing unit. The issue here is that of
<i>cache coherence</i>
 (section&nbsp;
1.4.1
) where two
cores both have a copy of a certain cacheline in their local stores.
If they both read from it there is no problem. However, if one of them
writes to it, the coherence protocol will copy the cacheline to the
other core's local store. This takes up precious memory bandwidth, so
it is to be avoided.
</p>

<p name="switchToTextMode">
Core locality is not just a property of a program, but also to a large
extent of how
the program is executed in parallel.
</p>

<h2><a id="Programmingstrategiesforhighperformance">1.7</a> Programming strategies for high performance</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a>
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In this section we will look at how different ways of programming can
influence the performance of a code. This will only be an introduction
to the topic; for further discussion see the book by Goedeker and
Hoisie~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Goedeker:performance-book">[Goedeker:performance-book]</a>
.
</p>

<p name="switchToTextMode">
The full listings of the codes and explanations of the data graphed
here can be found in chapter~
app:codes
. All performance results
were obtained on the 
<i>AMD Opteron</i>
 processors of the
<i>TACC Ranger cluster</i>
~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#tacc:ranger">[tacc:ranger]</a>
.
</p>

<h3><a id="Peakperformance">1.7.1</a> Peak performance</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Peakperformance">Peak performance</a>
</p>
<p name="switchToTextMode">

For marketing purposes, it may be desirable to define a `top speed' for a
CPU. Since a pipelined floating point unit can yield one result per
cycle asymptotically, you would calculate the theoretical
<i>peak performance</i>
 as the product of the clock speed (in
ticks per second), number of floating point units, and the number of
cores; see section~
1.4
.  This top speed is
unobtainable in practice, and very few codes come even close to it.
The 
<i>Linpack benchmark</i>
 is one of the measures how close
you can get to it; the parallel version of this benchmark is reported
in the `top 500'; see section~
2.11.6
.
</p>

<h3><a id="Pipelining">1.7.2</a> Pipelining</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Pipelining">Pipelining</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In section~
1.2.1.3
 you learned that the floating point
units in a modern CPU are pipelined, and that pipelines require a
number of independent operations to function efficiently. The typical
pipelineable operation is a vector addition; an example of an
operation that can not be pipelined is the inner product accumulation
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N; i++)
  s += a[i]*b[i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
The fact that 
<tt>s</tt>
 gets both read and written halts the addition
pipeline. One way to fill the 
<i>floating point pipeline</i>
is to apply 
<!-- environment: verbatim start embedded generator -->
</p>
for (i = 0; i &lt; N/2-1; i ++) {
  sum1 += a[2*i] * b[2*i];
  sum2 += a[2*i+1] * b[2*i+1];
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Now there are two independent multiplies in between the accumulations.
With a little indexing optimization this becomes:
<!-- environment: verbatim start embedded generator -->
</p>
for (i = 0; i &lt; N/2-1; i ++) {
  sum1 += *(a + 0) * *(b + 0);
  sum2 += *(a + 1) * *(b + 1);


  a += 2; b += 2;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
A first observation about this code is that we are implicitly using
associativity and commutativity of addition: while the same quantities
are added, they are now in effect added in a different order. As you
will see in chapter&nbsp;
Computer Arithmetic
, in computer arithmetic
this is not guaranteed to
give the exact same result.
</p>

<p name="switchToTextMode">
In a further optimization, we disentangle the addition and
multiplication part of each instruction. The hope is that while the
accumulation is waiting for the result of the multiplication, the
intervening instructions will keep the processor busy, in effect
increasing the number of operations per second.
<!-- environment: verbatim start embedded generator -->
</p>
for (i = 0; i &lt; N/2-1; i ++) {
  temp1 = *(a + 0) * *(b + 0);
  temp2 = *(a + 1) * *(b + 1);


  sum1 += temp1; sum2 += temp2;


  a += 2; b += 2;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Finally, we realize that the furthest we can move the addition away
from the multiplication, is to put it right in front of the
multiplication 
<i>of the next iteration</i>
:
<!-- environment: verbatim start embedded generator -->
</p>
for (i = 0; i &lt; N/2-1; i ++) {
  sum1 += temp1;
  temp1 = *(a + 0) * *(b + 0);


  sum2 += temp2;
  temp2 = *(a + 1) * *(b + 1);


  a += 2; b += 2;
}
s = temp1 + temp2;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Of course, we can unroll the operation by more than a factor of
two. While we expect an increased performance because of the longer
sequence of pipelined operations, large unroll factors
need large numbers of registers. Asking for more registers than a CPU
has is called 
<i>register spill</i>
, and it will decrease
performance.
</p>

<p name="switchToTextMode">
Another thing to keep in mind is that the total number of operations
is unlikely to be divisible by the unroll factor. This requires
<i>cleanup code</i>
 after the loop to account for the final
iterations. Thus, unrolled code is harder to write than straight code,
and people have written tools to perform such
<i>source-to-source transformations</i>
 automatically.
</p>

<p name="switchToTextMode">
Cycle times for unrolling the inner product operation up to six times
are given in table&nbsp;
1.7.2
. Note that the timings do
not show a monotone behavior at the unrolling by four. This sort of
variation is due to various memory-related factors.
</p>

<!-- environment: table start embedded generator -->
<!-- TranslatingLineGenerator table ['table'] -->
<p name="switchToTextMode">
\leavevmode\kern\unitindent
<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
    </td></tr>
<tr><td>
    1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6\ </td></tr>
<tr><td> 6794</td><td>507</td><td>340</td><td>359</td><td>334</td><td>528\ </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
  \caption{Cycle times for the inner product operation, unrolled up to
    six times}

</tbody></table>
</table>
<!-- environment: table end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h3><a id="Cachesize">1.7.3</a> Cache size</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Cachesize">Cache size</a>
</p>

<p name="switchToTextMode">

Above, you learned that data from L1 can be moved with lower latency
and higher bandwidth than from&nbsp;L2, and L2 is again faster than L3 or
memory. This is easy to demonstrate with code that repeatedly access
the same data:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;NRUNS; i++)
  for (j=0; j&lt;size; j++)
    array[j] = 2.3*array[j]+1.2;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
If the size parameter allows the array to fit in cache, the operation
will be relatively fast. As the size of the dataset grows, parts of it
will evict other parts from the L1 cache, so the speed of the
operation will be determined by the latency and bandwidth of the L2
cache. This can be seen in figure&nbsp;
1.18
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/cacheoverflow.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{Average cycle count per operation as function of the
    dataset size}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
The full code is given in section&nbsp;
sec:cachesize-code
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Argue that with a large enough problem and an 
<span title="acronym" ><i>LRU</i></span>
 replacement policy
  (section&nbsp;
1.3.4.6
) essentially all data in the L1 will be
  replaced in every iteration of the outer loop. Can you write an
  example code that will let some of the L1 data stay resident?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Often, it is possible to arrange the operations to keep data in L1
cache. For instance, in our example, we could write
<!-- environment: verbatim start embedded generator -->
</p>
for (b=0; b&lt;size/l1size; b++) {
  blockstart = 0;
  for (i=0; i&lt;NRUNS; i++) {
    for (j=0; j&lt;l1size; j++)
      array[blockstart+j] = 2.3*array[blockstart+j]+1.2;
  }
  blockstart += l1size;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
assuming that the L1 size divides evenly in the dataset size.
This strategy is called 
<i>cache blocking</i>
 or
<i>blocking for cache reuse</i>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Measure the number of memory accesses per cycle in the following loop,
  for various values of the cache size.
  If you observe that the time is independent of the cachesize,
  let your compiler generate an
  
<i>optimization report</i>

<!-- index -->
.
  For the 
<i>Intel compiler</i>
 use 
<tt>-qopt-report</tt>
.
<!-- environment: lstlisting start embedded generator -->
</p>
for (int irepeat=0; irepeat&lt;how_many_repeats; irepeat++) {
  for (int iword=0; iword&lt;cachesize_in_words; iword++)
    memory[iword] += 1.1;
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Argue what happened. Can you find a way to prevent the loop exchange?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  To arrive at the blocked code, the loop over 
<tt>j</tt>
 was split into a
  loop over blocks and an inner loop over the elements of the block;
  the outer loop over 
<tt>i</tt>
 was then exchanged with the loop over the
  blocks. In this particular example you could also simply exchange
  the 
<tt>i</tt>
 and 
<tt>j</tt>
 loops. Why may this not be optimal for performance?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
  Blocked code may change the order of evaluation of
  expressions. Since floating point arithmetic is not
  
<i>associative</i>
%
<!-- index -->
,
  blocking is not a transformation that compilers are allowed to make.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Cachelinesandstriding">1.7.4</a> Cache lines and striding</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Cachelinesandstriding">Cache lines and striding</a>
</p>

</p>

<p name="switchToTextMode">
Since data is moved from memory to cache in consecutive chunks named
cachelines (see section&nbsp;
1.3.4.7
), code that does not
utilize all data in a cacheline pays a bandwidth penalty. This is born
out by a simple code
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0,n=0; i&lt;L1WORDS; i++,n+=stride)
  array[n] = 2.3*array[n]+1.2;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Here, a fixed number of operations is performed, but on elements that
are at distance 
<tt>stride</tt>
. As this 
<i>stride</i>
 increases, we expect an
increasing runtime, which is born out by
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/cacheline8.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.19: Run time in kcycles and L1 reuse as a function of stride
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
the graph in figure&nbsp;
1.19
.
</p>

<p name="switchToTextMode">
The graph also shows a decreasing reuse of cachelines, defined as the
number of vector elements divided by the number of L1 misses (on
stall; see section&nbsp;
1.3.5
).
</p>

<p name="switchToTextMode">
The full code is given in section&nbsp;
sec:cacheline-code
.
</p>

<p name="switchToTextMode">
The effects of striding can be mitigated by the bandwidth
and cache behavior of a processor.
Consider some run on the 
<i>Intel Cascadelake</i>
 processor
of the 
<i>Frontera</i>
 cluster at TACC,
(28-cores per socket, dual socket, for a total of 56 cores per node).
We measure the time-per-operation on a simple streaming kernel,
using increasing strides.
Table&nbsp;
1.19
 reports in the second column
indeed a per-operation time that goes up linearly with the stride.
</p>

<!-- environment: table start embedded generator -->
<!-- TranslatingLineGenerator table ['table'] -->
<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
    </td></tr>
<tr><td>
    stride</td><td>nsec/word</td></tr>
<tr><td>
    </td><td>56 cores, 3M</td><td> 56 cores, .3M</td><td>28 cores, 3M</td></tr>
<tr><td>
    </td></tr>
<tr><td>
  1 </td><td>  7.268 </td><td> 1.368 </td><td> 1.841</td></tr>
<tr><td>
  2 </td><td> 13.716 </td><td> 1.313 </td><td> 2.051</td></tr>
<tr><td>
  3 </td><td> 20.597 </td><td> 1.319 </td><td> 2.852</td></tr>
<tr><td>
  4 </td><td> 27.524 </td><td> 1.316 </td><td> 3.259</td></tr>
<tr><td>
  5 </td><td> 34.004 </td><td> 1.329 </td><td> 3.895</td></tr>
<tr><td>
  6 </td><td> 40.582 </td><td> 1.333 </td><td> 4.479</td></tr>
<tr><td>
  7 </td><td> 47.366 </td><td> 1.331 </td><td> 5.233</td></tr>
<tr><td>
  8 </td><td> 53.863 </td><td> 1.346 </td><td> 5.773</td></tr>
<tr><td>
    </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="caption">
TABLE: Time per operation in nanoseconds as a function of striding on 56 cores of Frontera, per-core datasize 3.2M
</p>

</tbody></table>
</table>
<!-- environment: table end embedded generator -->
<p name="switchToTextMode">

However, this is for a dataset that overflows the L2 cache.
If we make this run contained in the L2 cache,
as reported in the 3rd column,
this increase goes away as there is enough bandwdidth
available to stream strided data at full speed from L2 cache.
</p>

<h3><a id="TLB">1.7.5</a> TLB</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#TLB">TLB</a>
</p>

<p name="switchToTextMode">

As explained in section&nbsp;
1.3.8.2
, the 
<span title="acronym" ><i>TLB</i></span>
 maintains a small
list of frequently used memory pages and their locations; addressing
data that are location on one of these pages is much faster than data
that are not. Consequently, one wants to code in such a way that the
number of pages accessed is kept low.
</p>

<p name="switchToTextMode">
Consider code for traversing the elements of a two-dimensional array
in two different ways.
<!-- environment: verbatim start embedded generator -->
</p>
#define INDEX(i,j,m,n) i+j*m
array = (double*) malloc(m*n*sizeof(double));


/* traversal #1 */
for (j=0; j&lt;n; j++)
  for (i=0; i&lt;m; i++)
    array[INDEX(i,j,m,n)] = array[INDEX(i,j,m,n)]+1;


/* traversal #2 */
for (i=0; i&lt;m; i++)
  for (j=0; j&lt;n; j++)
    array[INDEX(i,j,m,n)] = array[INDEX(i,j,m,n)]+1;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

The results (see Appendix&nbsp;
sec:tlb-code
 for the source code) are
plotted in figures 
1.21
 and&nbsp;
1.20
.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/tlb_col.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{Number of TLB misses per column as function of the number
    of columns; columnwise traversal of the array.}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/tlb_row.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{Number of TLB misses per column as function of the number
    of columns; rowwise traversal of the array.}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Using $m=1000$ means that, on the 
<i>AMD Opteron</i>
 which
has pages of $512$ doubles, we need roughly two pages for each
column. We run this example, plotting the number `TLB misses', that
is, the number of times a page is referenced that is not recorded in
the TLB.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
In the first traversal this is indeed what happens. After we
  touch an element, and the TLB records the page it is on, all other
  elements on that page are used subsequently, so no further TLB
  misses occur. Figure&nbsp;
1.20
 shows that, with increasing&nbsp;$n$,
  the number of TLB misses per column is roughly two.
<li>
In the second traversal, we touch a new page for every element
  of the first row. Elements of the second row will be on these pages,
  so, as long as the number of columns is less than the number of TLB
  entries, these pages will still be recorded in the TLB. As the
  number of columns grows, the number of TLB increases, and ultimately
  there will be one TLB miss for each element
  access. Figure&nbsp;
1.21
 shows that, with a large enough number
  of columns, the number of TLB misses per column is equal to the
  number of elements per column.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Cacheassociativity">1.7.6</a> Cache associativity</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Cacheassociativity">Cache associativity</a>
</p>

</p>

<p name="switchToTextMode">
There are many algorithms that work by recursive division of a
problem, for instance the 
<i>FFT</i>
 algorithm. As a result, code
for such algorithms will often operate on vectors whose length is a power of
two. Unfortunately, this can cause conflicts with certain
architectural features of a CPU, many of which involve powers of two.
</p>

<p name="switchToTextMode">
In section&nbsp;
1.3.4.9
 you saw how
the operation of adding a small number of vectors
\[
 \forall_j\colon y_j= y_j+\sum_{i=1}^mx_{i,j} 
\]
is a problem for direct mapped caches or set-associative caches with
associativity.
</p>

<p name="switchToTextMode">
As an example we take the
<i>AMD Opteron</i>
, which has an L1 cache of 64K bytes, and
which is two-way set associative. Because of the set associativity,
the cache can handle two addresses being mapped to the same cache
location, but not three or more. Thus, we let the vectors be of
size&nbsp;$n=4096$ doubles, and we measure the effect in cache misses and
cycles of letting $m=1,2,&hellip;$.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/l1_assoc.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{The number of L1 cache misses and the number of cycles for
    each $j$ column accumulation, vector length&nbsp;$4096$}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/l1_assocshift.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{The number of L1 cache misses and the number of cycles for
    each $j$ column accumulation, vector length&nbsp;$4096+8$}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

First of all, we note that we use the vectors sequentially, so, with a
cacheline of eight doubles, we should ideally see a cache miss rate of
$1/8$ times the number of vectors&nbsp;$m$. Instead, in
figure&nbsp;
1.22
 we see a rate approximately proportional
to&nbsp;$m$, meaning that indeed cache lines are evicted immediately. The
exception here is the case $m=1$, where the two-way associativity
allows the cachelines of two vectors to stay in cache.
</p>

<p name="switchToTextMode">
Compare this to figure&nbsp;
1.23
, where we used a
slightly longer vector length, so that locations with the same $j$ are
no longer mapped to the same cache location. As a result, we see a
cache miss rate around $1/8$, and a smaller number of cycles,
corresponding to a complete reuse of the cache lines.
</p>

<p name="switchToTextMode">
Two remarks: the cache miss numbers are in fact lower than the theory
predicts, since the processor will use prefetch streams. Secondly, in
figure&nbsp;
1.23
 we see a decreasing time with
increasing&nbsp;$m$; this is probably due to a progressively more
favorable balance between load and store operations. Store operations
are more expensive than loads, for various reasons.
</p>

<h3><a id="Loopnests">1.7.7</a> Loop nests</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Loopnests">Loop nests</a>
</p>
<!-- index -->
<p name="switchToTextMode">

If your code has 
<i>nested loops</i>
, and the iterations of the outer
loop are independent, you have a choice which loop to make outer and
which to make inner.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Give an example of a doubly-nested loop where the loops can be
  exchanged; give an example where this can not be done. If at all
  possible, use practical examples from this book.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

If you have such choice, there are many factors that can influence
your decision.
</p>

<p name="switchToTextMode">

<b>Programming language: C&nbsp;versus&nbsp;Fortran</b><br>

If your loop describes the $(i,j)$ indices of a two-dimensional array,
it is often best to let the $i$-index be in the inner loop for
Fortran, and the $j$-index inner for&nbsp;C.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you come up with at least two reasons why this is possibly better for performance?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

However, this is not a hard-and-fast rule. It can depend on the size
of the loops, and other factors. For instance, in the matrix-vector
product, changing the loop ordering changes how the input and output
vectors are used.
</p>

<p name="switchToTextMode">

<b>Parallelism model</b><br>

If you want to parallelize your loops with 
<i>OpenMP</i>
, you
generally want the outer loop to be larger than the inner. Having a
very short outer loop is definitely bad. A&nbsp;short inner loop can also
often be 
<i>vectorized by the   compiler</i>

<!-- index -->
.
</p>

<p name="switchToTextMode">
On the other hand, if you are targeting a 
<i>GPU</i>
, you want the
large loop to be the inner one. The unit of parallel work should not have branches
or loops.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Looptiling">1.7.8</a> Loop tiling</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Looptiling">Loop tiling</a>
</p>

</p>

<p name="switchToTextMode">
In some cases performance can be increased by breaking up a loop into
two nested loops, an outer one for the blocks in the iteration space,
and an inner one that goes through the block. This is known as
consecutive instances of which form the iteration space.
</p>

<p name="switchToTextMode">
For instance
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;n; i++)
  ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
becomes
<!-- environment: verbatim start embedded generator -->
</p>
bs = ...       /* the blocksize */
nblocks = n/bs /* assume that n is a multiple of bs */
for (b=0; b&lt;nblocks; b++)
  for (i=b*bs,j=0; j&lt;bs; i++,j++)
    ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
For a single loop this may not make any difference, but given the
right context it may. For instance, if an array is repeatedly used,
but it is too large to fit into cache:
<!-- environment: verbatim start embedded generator -->
</p>
for (n=0; n&lt;10; n++)
  for (i=0; i&lt;100000; i++)
    ... = ...x[i] ...


</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
then loop tiling may lead to a situation where the array is divided
into blocks that will fit in cache:
<!-- environment: verbatim start embedded generator -->
</p>
bs = ... /* the blocksize */
for (b=0; b&lt;100000/bs; b++)
  for (n=0; n&lt;10; n++)
    for (i=b*bs; i&lt;(b+1)*bs; i++)
      ... = ...x[i] ...
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
For this reason, loop tiling is also known as
<i>cache blocking</i>
. The block size depends on how much
data is accessed in the loop body; ideally you would try to make data
reused in L1 cache, but it is also possible to block for L2 reuse. Of
course, L2 reuse will not give as high a performance as L1 reuse.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze this example. When is 
<tt>x</tt>
 brought into cache, when is it
  reused, and when is it flushed? What is the required cache size in
  this example? Rewrite this example, using a constant
<!-- environment: verbatim start embedded generator -->
</p>
#define L1SIZE 65536
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

For a less trivial example, let's look at
<i>matrix transposition</i>
 $A\leftarrow B^t$. Ordinarily you would traverse
the input and output matrices:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#regulartranspose" aria-expanded="false" aria-controls="regulartranspose">
        Text: regulartranspose
      </button>
    </h5>
  </div>
  <div id="regulartranspose" class="collapse">
  <pre>
// regular.c
for (int i=0; i&lt;N; i++)
  for (int j=0; j&lt;N; j++)
    A[i][j] += B[j][i];
</pre>
</div>
</div>
Using blocking this becomes:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#blockedtranspose" aria-expanded="false" aria-controls="blockedtranspose">
        Text: blockedtranspose
      </button>
    </h5>
  </div>
  <div id="blockedtranspose" class="collapse">
  <pre>
// blocked.c
for (int ii=0; ii&lt;N; ii+=blocksize)
  for (int jj=0; jj&lt;N; jj+=blocksize)
    for (int i=ii*blocksize; i&lt;MIN(N,(ii+1)*blocksize); i++)
      for (int j=jj*blocksize; j&lt;MIN(N,(jj+1)*blocksize); j++)
        A[i][j] += B[j][i];
</pre>
</div>
</div>
Unlike in the example above, each element of the input and output is
touched only once, so there is no direct reuse. However, there is
reuse of cachelines.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/blockedtranspose.jpeg" width=800></img>
<p name="caption">
FIGURE 1.24: Regular and blocked traversal of a matrix
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
1.24
 shows how one of the matrices is
traversed in a different order from its storage order, for instance
columnwise while it is stored by rows. This has the effect that each
element load transfers a cacheline, of which only one element is
immediately used. In the regular traversal, this streams of cachelines
quickly overflows the cache, and there is no reuse. In the blocked
traversal, however, only a small number of cachelines is traversed
before the next element of these lines is needed. Thus there is reuse
of cachelines, or 
<i>spatial locality</i>
.
</p>

<p name="switchToTextMode">
The most important example of attaining performance through blocking
is the 
<i>matrix!matrix product!tiling</i>
.
In section&nbsp;
1.6.2
 we looked at the matrix-matrix
multiplication, and concluded that little data could be kept in
cache. With loop tiling we can improve this situation. For instance,
the standard way of writing this product
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  for j=1..n
    for k=1..n
      c[i,j] += a[i,k]*b[k,j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
can only be optimized to keep 
<tt>c[i,j]</tt>
 in register:
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  for j=1..n
    s = 0
    for k=1..n
      s += a[i,k]*b[k,j]
    c[i,j] += s
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Using loop tiling we can keep parts of&nbsp;
<tt>a[i,:]</tt>
 in cache,
assuming that 
<tt>a</tt>
 is stored by rows:
<!-- environment: verbatim start embedded generator -->
</p>
for kk=1..n/bs
  for i=1..n
    for j=1..n
      s = 0
      for k=(kk-1)*bs+1..kk*bs
        s += a[i,k]*b[k,j]
      c[i,j] += s
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

</p>

<h3><a id="Optimizationstrategies">1.7.9</a> Optimization strategies</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Optimizationstrategies">Optimization strategies</a>
</p>

<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/dft.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.25: Performance of naive and optimized implementations of the Discrete Fourier Transform
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/gemm.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 1.26: Performance of naive and optimized implementations of the matrix-matrix product
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Figures 
1.25
 and 
1.26
 show that there can
be wide discrepancy between the performance of naive implementations
of an operation (sometimes called the `reference implementation'), and
optimized implementations. Unfortunately, optimized implementations
are not simple to find. For one, since they rely on blocking, their
loop nests are double the normal depth: the matrix-matrix
multiplication becomes a six-deep loop. Then, the optimal block size
is dependent on factors like the target architecture.
</p>

<p name="switchToTextMode">
We make the following observations:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Compilers
<!-- index -->
 are not able to extract anywhere close
  to optimal performance
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
    {Presenting a compiler with the
    reference implementation may still lead to high performance, since
    some compilers are trained to recognize this operation. They will
    then forego translation and simply replace it by an optimized
    variant.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
  .
<li>
There are 
<i>autotuning</i>
 projects for automatic
  generation of implementations that are tuned to the
  architecture. This approach can be moderately to very
  successful. Some of the best known of these projects are
  Atlas&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#atlas-parcomp">[atlas-parcomp]</a>
 for Blas kernels, and
  Spiral&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#spiral">[spiral]</a>
 for transforms.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Cacheawareandcacheobliviousprogramming">1.7.10</a> Cache aware and cache oblivious programming</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Cacheawareandcacheobliviousprogramming">Cache aware and cache oblivious programming</a>
</p>
</p>

<p name="switchToTextMode">
Unlike registers and main memory, both of
which can be addressed in (assembly) code, use of caches is
implicit. There is no way a programmer can load data explicitly to a
certain cache, even in assembly language.
</p>

<p name="switchToTextMode">
However, it is possible to code in a `cache aware' manner. Suppose a
piece of code repeatedly operates on an amount of data that is less
than the cache size. We can assume that the first time the data is
accessed, it is brought into cache; the next time it is accessed it
will already be in cache. On the other hand, if the amount of data is
more than the cache size
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {We are conveniently ignoring matters
  of set-associativity here, and basically assuming a fully
  associative cache.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
, it will partly or fully be flushed out of cache
in the process of accessing it.
</p>

<p name="switchToTextMode">
We can experimentally demonstrate this phenomenon. With a very
accurate counter, the code fragment
<!-- environment: verbatim start embedded generator -->
</p>
for (x=0; x&lt;NX; x++)
  for (i=0; i&lt;N; i++)
    a[i] = sqrt(a[i]);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
will take time linear in 
<tt>N</tt>
 up to the point where 
<tt>a</tt>

fills the cache. An easier way to picture this is to compute a
normalized time, essentially a time per execution of the inner loop:
<!-- environment: verbatim start embedded generator -->
</p>
t = time();
for (x=0; x&lt;NX; x++)
  for (i=0; i&lt;N; i++)
    a[i] = sqrt(a[i]);
t = time()-t;
t_normalized = t/(N*NX);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
The normalized time will be constant until the array 
<tt>a</tt>
 fills
the cache, then increase and eventually level off again. (See
section&nbsp;
1.7.3
 for an elaborate discussion.)
</p>

<p name="switchToTextMode">
The explanation is that,
as long as 
<tt>a[0]...a[N-1]</tt>
 fit in L1 cache, the inner loop will
use data from the L1 cache. Speed of access is then determined by the
latency and bandwidth of the L1 cache.
As the amount of data grows beyond the L1 cache size, some or all of
the data will be flushed from the L1, and performance will be determined by
the characteristics of the L2 cache. Letting the amount of data grow
even further, performance will again drop to a linear behavior
determined by the bandwidth from main memory.
</p>

<!-- index -->
<p name="switchToTextMode">

If you know the cache size, it is possible in cases such as above to
arrange the algorithm to use the cache optimally. However, the cache
size is different per processor, so this makes your code not portable,
or at least its high performance is not portable. Also, blocking for
multiple levels of cache is complicated. For these reasons, some
people advocate 
<i>cache oblivious   programming</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Frigo:oblivious">[Frigo:oblivious]</a>
.
</p>

<p name="switchToTextMode">
Cache oblivious programming can be described as a way of programming
that automatically uses all levels of the
<i>cache hierarchy</i>
. This is typically done by using a
<i>divide-and-conquer</i>
 strategy, that is, recursive
subdivision of a problem.
</p>

<p name="switchToTextMode">
As a simple example of cache oblivious programming is the 
{transposition} operation $B\leftarrow A^t$. First we observe that each
element of either matrix is accessed once, so the only reuse is in the
utilization of cache lines. If both matrices are stored by
rows and we traverse $B$ by rows, then $A$&nbsp;is traversed by columns,
and for each element accessed one cacheline is loaded. If the number
of rows times the number of elements per cacheline is more than the
cachesize, lines will be evicted before they can be reused.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/oblivious1.jpeg" width=800></img>
<p name="switchToTextMode">
  \caption{Matrix transpose operation, with simple and recursive
    traversal of the source matrix}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
In a cache oblivious implementation we divide $A$ and&nbsp;$B$ as
$2\times2$ block matrices, and recursively compute $B_{11}\leftarrow
A_{11}^t$, $B_{12}\leftarrow A_{21}^t$, et cetera; see
figure&nbsp;
1.27
. At some point in the recursion,
blocks $A_{ij}$ will now be small enough that they fit in cache, and
the cachelines of&nbsp;$A$ will be fully used. Hence, this algorithm
improves on the simple one by a factor equal to the cacheline size.
</p>

<p name="switchToTextMode">
The cache oblivious strategy can often yield improvement, but it is
not necessarily optimal. In the
<i>matrix-matrix product</i>
<!-- index -->
it improves on
the naive algorithm, but it is not as good as an algorithm that is
explicitly designed to make optimal use of
caches&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#GotoGeijn:2008:Anatomy">[GotoGeijn:2008:Anatomy]</a>
.
</p>

<p name="switchToTextMode">
See section&nbsp;
6.8.4
 for a discussion of such techniques
in stencil computations.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Casestudy:Matrix-vectorproduct">1.7.11</a> Case study: Matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Programmingstrategiesforhighperformance">Programming strategies for high performance</a> > <a href="sequential.html#Casestudy:Matrix-vectorproduct">Case study: Matrix-vector product</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
Let us consider in some detail
the 
<i>matrix-vector product</i>
\[
 \forall_{i,j}\colon y_i\leftarrow a_{ij}\cdot x_j 
\]
 This involves $2n^2$
operations on $n^2+2n$ data items, so reuse is&nbsp;$O(1)$: memory accesses
and operations are of the same order. However, we note that there is a
double loop involved, and the $x,y$ vectors have only a single index,
so each element in them is used multiple times.
</p>

<p name="switchToTextMode">
Exploiting this theoretical reuse is not trivial. In
<!-- environment: verbatim start embedded generator -->
</p>
/* variant 1 */
for (i)
  for (j)
    y[i] = y[i] + a[i][j] * x[j];
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the element 
<tt>y[i]</tt>
 seems to be reused. However, the statement
as given here would write 
<tt>y[i]</tt>
 to memory in every inner
iteration, and we have to write the loop as
<!-- environment: verbatim start embedded generator -->
</p>
/* variant 2 */
for (i) {
  s = 0;
  for (j)
    s = s + a[i][j] * x[j];
  y[i] = s;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
to ensure reuse. This variant uses $2n^2$ loads and $n$&nbsp;stores.
</p>

<p name="switchToTextMode">
This code fragment only exploits the reuse
of&nbsp;
<tt>y</tt>
 explicitly. If the cache is too small to hold the whole
vector&nbsp;
<tt>x</tt>
 plus a column of&nbsp;
<tt>a</tt>
, each element
of&nbsp;
<tt>x</tt>
 is still repeatedly loaded in every outer iteration.
</p>

<p name="switchToTextMode">
Reversing the loops as
<!-- environment: verbatim start embedded generator -->
</p>
/* variant 3 */
for (j)
  for (i)
    y[i] = y[i] + a[i][j] * x[j];
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
exposes the reuse of&nbsp;
<tt>x</tt>
, especially if we write this as
<!-- environment: verbatim start embedded generator -->
</p>
/* variant 3 */
for (j) {
  t = x[j];
  for (i)
    y[i] = y[i] + a[i][j] * t;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
but now 
<tt>y</tt>
 is no longer
reused. Moreover, we now have $2n^2+n$ loads, comparable to variant&nbsp;2,
but $n^2$&nbsp;stores, which is of a higher order.
</p>

<p name="switchToTextMode">
It is possible to get reuse both of $x$ and&nbsp;$y$, but this requires
more sophisticated programming. The key here is to split the loops into
blocks. For instance:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;M; i+=2) {
  s1 = s2 = 0;
  for (j) {
    s1 = s1 + a[i][j] * x[j];
    s2 = s2 + a[i+1][j] * x[j];
  }
  y[i] = s1; y[i+1] = s2;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
This is also called 
<i>loop</i>
{unrolling},
or 
<i>strip mining</i>
. The amount by which you unroll
loops is determined by the number of available registers.
</p>

<!-- index -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">

<h2><a id="Furthertopics">1.8</a> Further topics</h2>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a>
</p>
</p>

<h3><a id="Powerconsumption">1.8.1</a> Power consumption</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a> > <a href="sequential.html#Powerconsumption">Power consumption</a>
</p>

<!-- index -->
<p name="switchToTextMode">
\SetBaseLevel 2
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<p name="switchToTextMode">
Another important topic in high performance computers is their power
consumption. Here
we need to distinguish between the power consumption of a single
processor chip, and that of a complete cluster.
</p>

<p name="switchToTextMode">
As the number of components on a chip grows, its power consumption
would also grow. Fortunately, in a counter acting trend,
miniaturization of the chip features has simultaneously been reducing
the necessary power. Suppose that the feature size~$\lambda$ (think:
thickness of wires) is scaled down to $s\lambda$ with~$s<1$. In order
to keep the electric field in the transistor constant, the length and
width of the channel, the oxide thickness, substrate concentration
density and the operating voltage are all scaled by the same factor.
</p>

<h3><a id="Derivationofscalingproperties">1.8.2</a> Derivation of scaling properties</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a> > <a href="sequential.html#Derivationofscalingproperties">Derivation of scaling properties</a>
</p>
<p name="switchToTextMode">

The properties of 
<i>constant field scaling</i>

<!-- index -->
 or
<i>Dennard scaling</i>
~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Bohr:30yearDennard,Dennard:scaling">[Bohr:30yearDennard,Dennard:scaling]</a>
are an ideal-case description of the properties of a circuit
as it is miniaturized. One important result is that power density
stays constant as chip features get smaller, and the frequency
is simultaneously increased.
</p>

<p name="switchToTextMode">
The basic properties derived from circuit theory are that,
if we scale feature size down by~$s$:
\[
\begin{array}{|l|c|}\hline
\hbox{Feature size}&\sim s\\
\hbox{Voltage}&\sim s\\
\hbox{Current}&\sim s \\
\hbox{Frequency}&\sim s\\
\hline
\end{array}
\]
</p>

<p name="switchToTextMode">
Then we can derive that
\[
 \hbox{Power} = V\cdot I \sim s^2, 
\]
and because the total size of the circuit also goes down with~$s^2$,
the power density stays the same.
Thus, it also becomes possible to
put more transistors on a circuit, and essentially not change the cooling
problem.
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
\toprule
Year</td><td>\#transistors</td><td>Processor</td></tr>
<tr><td>
\midrule
1975 </td><td> $3, 000$ </td><td> 6502 </td></tr>
<tr><td>
1979 </td><td> $30, 000$ </td><td> 8088 </td></tr>
<tr><td>
1985 </td><td> $300, 000$ </td><td> 386 </td></tr>
<tr><td>
1989 </td><td> $1, 000, 000$ </td><td> 486 </td></tr>
<tr><td>
1995 </td><td> $6, 000, 000$ </td><td> Pentium Pro </td></tr>
<tr><td>
2000 </td><td> $40, 000, 000$ </td><td> Pentium 4 </td></tr>
<tr><td>
2005 </td><td> $100, 000, 000$ </td><td> 2-core Pentium D </td></tr>
<tr><td>
2008 </td><td> $700, 000, 000$ </td><td> 8-core Nehalem </td></tr>
<tr><td>
2014 </td><td> $6, 000, 000, 000$ </td><td> 18-core Haswell </td></tr>
<tr><td>
2017 </td><td> $20, 000, 000, 000$ </td><td> 32-core AMD Epyc </td></tr>
<tr><td>
2019 </td><td> $40, 000, 000, 000$ </td><td> 64-core AMD Rome </td></tr>
<tr><td>
\bottomrule
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
Chronology of Moore's Law (courtesy Jukka Suomela, `Programming Parallel Computers')
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{3in}
This result can be considered
the driving force behind 
<i>Moore's law</i>
,
which states that the number of transistors in a processor
doubles every 18 months.
</p>

<p name="switchToTextMode">
The frequency-dependent part of the power a processor needs
comes from charging and discharging the capacitance of the circuit, so
<!-- environment: equation start embedded generator -->
</p>
\begin{array}{|l|l|} \hline
\hbox{Charge}&q=CV\\
\hbox{Work}&W=qV=CV^2\\
\hbox{Power}&W/\hbox{time}=WF=CV^2F \\ \hline
\end{array}
\label{eq:power}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
This analysis can be used to justify the introduction of multicore processors.
</p>

<h3><a id="Multicore">1.8.3</a> Multicore</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a> > <a href="sequential.html#Multicore">Multicore</a>
</p>
<p name="switchToTextMode">

At the time of this writing (circa&nbsp;2010), miniaturization of
components has almost come to a standstill, because further lowering
of the voltage would give prohibitive leakage. Conversely, the
frequency can not be scaled up since this would raise the heat
production of the chip too far.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/chipheat0.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{Projected heat dissipation of a CPU if trends had
    continued -- this graph courtesy Pat Helsinger}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
1.29
 gives a dramatic illustration of the heat
that a chip would give off, if single-processor trends had
continued.
</p>

<p name="switchToTextMode">
One conclusion is that computer design
is running into a 
<i>power wall</i>
, where the sophistication
of a single core can not be increased any further (so we can for
instance no longer increase 
<i>ILP</i>
 and
<i>pipeline depth</i>
) and the only way to increase
performance is to increase the amount of explicitly visible
parallelism. This development has led to the current generation of
<i>multicore</i>
 processors; see section&nbsp;
1.4
. It
is also the reason 
<span title="acronym" ><i>GPUs</i></span>
 with their simplified processor design
and hence lower energy consumption
are attractive; the same holds for 
<span title="acronym" ><i>FPGAs</i></span>
.
One solution to the power wall problem is introduction
of 
<i>multicore</i>

<!-- index -->
 processors.
Recall equation&nbsp;
eq:power
, and compare a single processor to two
processors at half the frequency. That should have the same computing power, right?
Since we lowered the frequency, we can lower the voltage if we stay with the same
process technology.
</p>

<p name="switchToTextMode">
The total electric power for the two processors (cores) is, ideally,
\[
 \left.
\begin{array}{c}
C_{\mathrm{multi}} = 2C\\
F_{\mathrm{multi}} = F/2\\
V_{\mathrm{multi}} = V/2\\
\end{array}
\right\} \Rightarrow
P_{\mathrm{multi}} = P/4.
\]
In practice the capacitance will go up by a little over&nbsp;2, and the
voltage can not quite be dropped by&nbsp;2, so it is more likely that
$P_{\mathrm{multi}} \approx 0.4\times
P$&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Chandrakasa:transformations">[Chandrakasa:transformations]</a>
.  Of course the integration
aspects are a little more complicated in
practice&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Bohr:ISSCC2009">[Bohr:ISSCC2009]</a>
; the important conclusion is that now,
in order to lower the power (or, conversely, to allow further increase
in performance while keeping the power constant) we now have to start
programming in parallel.
</p>

<h3><a id="Totalcomputerpower">1.8.4</a> Total computer power</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a> > <a href="sequential.html#Totalcomputerpower">Total computer power</a>
</p>
<p name="switchToTextMode">

The total power consumption of a parallel computer is determined by
the consumption per processor and the number of processors in the full
machine. At present, this is commonly several Megawatts. By the above
reasoning, the increase in power needed from increasing the number of
processors can no longer be offset by more power-effective processors,
so power is becoming the overriding consideration as parallel
computers move from the petascale (attained in 2008 by the
<i>IBM Roadrunner</i>
) to a projected exascale.
</p>

<p name="switchToTextMode">
In the most recent generations of processors, power is becoming an
overriding consideration, with influence in unlikely places. For
instance, the 
<span title="acronym" ><i>SIMD</i></span>
 design of processors (see
section&nbsp;
2.3.1
, in particular section&nbsp;
2.3.1.2
) is
dictated by the power cost of instruction decoding.
</p>

<p name="switchToTextMode">
\SetBaseLevel 1
<!-- index -->
</p>

<h3><a id="Operatingsystemeffects">1.8.5</a> Operating system effects</h3>
<p name=crumbs>
crumb trail:  > <a href="sequential.html">sequential</a> > <a href="sequential.html#Furthertopics">Further topics</a> > <a href="sequential.html#Operatingsystemeffects">Operating system effects</a>
</p>
<p name="switchToTextMode">

HPC practitioners typically don't worry much about the
<span title="acronym" ><i>OS</i></span>
 can be
felt, influencing performance. The reason for this is the
<i>periodic interrupt</i>
, where the operating system upwards
of 100&nbsp;times per second interrupts the current process to let another
process or a system 
<i>daemon</i>
 have a 
<i>time slice</i>
.
</p>

<p name="switchToTextMode">
If you are running basically one program, you don't want the overhead and
<i>jitter</i>
, the unpredictability of process
runtimes, this introduces. Therefore, computers have existed that basically dispensed
with having an 
<span title="acronym" ><i>OS</i></span>
 to increase performance.
</p>

<p name="switchToTextMode">
The 
<i>periodic interrupt</i>
 has further negative
effects. For instance, it pollutes the cache and
<span title="acronym" ><i>TLB</i></span>
. As a fine-grained effect of jitter, it degrades performance
of codes that rely on barriers between threads, such as frequently
happens in OpenMP (section&nbsp;
2.6.2
).
</p>

<p name="switchToTextMode">
In particular in 
<i>financial applications</i>
, where very tight
synchronization is important, have adopted a Linux kernel mode where
the periodic timer ticks only once a second, rather than hundreds of
times. This is called a 
<i>tickless kernel</i>
.
</p>

<!-- environment: notready start embedded generator -->

</notready>
<!-- environment: notready end embedded generator -->
<p name="switchToTextMode">

<!-- environment: notlulu start embedded generator -->
</p>

</notlulu>
<!-- environment: notlulu end embedded generator -->
<p name="switchToTextMode">

</div>
<a href="index.html">Back to Table of Contents</a>
