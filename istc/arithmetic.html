<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Computer Arithmetic</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


3.1 : <a href="arithmetic.html#Bits">Bits</a><br>
3.2 : <a href="arithmetic.html#Integers">Integers</a><br>
3.2.1 : <a href="arithmetic.html#Integeroverflow">Integer overflow</a><br>
3.2.2 : <a href="arithmetic.html#Additionintwo'scomplement">Addition in two's complement</a><br>
3.2.3 : <a href="arithmetic.html#Subtractionintwo'scomplement">Subtraction in two's complement</a><br>
3.2.4 : <a href="arithmetic.html#Binarycodeddecimal">Binary coded decimal</a><br>
3.2.5 : <a href="arithmetic.html#Othernumberbasesforcomputerarithmetic">Other number bases for computer arithmetic</a><br>
3.3 : <a href="arithmetic.html#Realnumbers">Real numbers</a><br>
3.3.1 : <a href="arithmetic.html#They'renotreallyrealnumbers">They're not really real numbers</a><br>
3.3.2 : <a href="arithmetic.html#Representationofrealnumbers">Representation of real numbers</a><br>
3.3.2.1 : <a href="arithmetic.html#Someexamples">Some examples</a><br>
3.3.3 : <a href="arithmetic.html#Normalizedandunnormalizednumbers">Normalized and unnormalized numbers</a><br>
3.3.4 : <a href="arithmetic.html#Limitations:overflowandunderflow">Limitations: overflow and underflow</a><br>
3.3.4.1 : <a href="arithmetic.html#Gradualunderflow">Gradual underflow</a><br>
3.3.5 : <a href="arithmetic.html#Representationerror">Representation error</a><br>
3.3.6 : <a href="arithmetic.html#Machineprecision">Machine precision</a><br>
3.3.7 : <a href="arithmetic.html#TheIEEE754standardforfloatingpointnumbers">The IEEE 754 standard for floating point numbers</a><br>
3.3.8 : <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a><br>
3.3.8.1 : <a href="arithmetic.html#Not-a-Number">Not-a-Number</a><br>
3.3.8.2 : <a href="arithmetic.html#Dividebyzero">Divide by zero</a><br>
3.3.8.3 : <a href="arithmetic.html#Overflow">Overflow</a><br>
3.3.8.4 : <a href="arithmetic.html#Underflow">Underflow</a><br>
3.3.8.5 : <a href="arithmetic.html#Inexact">Inexact</a><br>
3.4 : <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a><br>
3.4.1 : <a href="arithmetic.html#Correctrounding">Correct rounding</a><br>
3.4.1.1 : <a href="arithmetic.html#Mul-Addoperations">Mul-Add operations</a><br>
3.4.2 : <a href="arithmetic.html#Addition">Addition</a><br>
3.4.3 : <a href="arithmetic.html#Multiplication">Multiplication</a><br>
3.4.4 : <a href="arithmetic.html#Subtraction">Subtraction</a><br>
3.4.5 : <a href="arithmetic.html#Associativity">Associativity</a><br>
3.5 : <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a><br>
3.5.1 : <a href="arithmetic.html#Cancellation:the`abc-formula'">Cancellation: the `abc-formula'</a><br>
3.5.2 : <a href="arithmetic.html#Summingseries">Summing series</a><br>
3.5.3 : <a href="arithmetic.html#Unstablealgorithms">Unstable algorithms</a><br>
3.5.4 : <a href="arithmetic.html#Linearsystemsolving">Linear system solving</a><br>
3.5.5 : <a href="arithmetic.html#Roundofferrorinparallelcomputations">Roundoff error in parallel computations</a><br>
3.6 : <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a><br>
3.6.1 : <a href="arithmetic.html#Fortran">Fortran</a><br>
3.6.2 : <a href="arithmetic.html#C">C</a><br>
3.6.2.1 : <a href="arithmetic.html#Bits">Bits</a><br>
3.6.2.2 : <a href="arithmetic.html#Integersandfloatingpointnumbers">Integers and floating point numbers</a><br>
3.6.2.3 : <a href="arithmetic.html#Printingbitpatterns">Printing bit patterns</a><br>
3.6.3 : <a href="arithmetic.html#C++">C++</a><br>
3.6.3.1 : <a href="arithmetic.html#Limits">Limits</a><br>
3.6.3.2 : <a href="arithmetic.html#Exceptions">Exceptions</a><br>
3.6.3.3 : <a href="arithmetic.html#Exceptions">Exceptions</a><br>
3.6.3.4 : <a href="arithmetic.html#Printingbitpatterns">Printing bit patterns</a><br>
3.6.4 : <a href="arithmetic.html#Compilerflagsforfastmath">Compiler flags for fast math</a><br>
3.6.5 : <a href="arithmetic.html#Round-offbehaviorinprogramming">Round-off behavior in programming</a><br>
3.6.6 : <a href="arithmetic.html#Changingroundingbehavior">Changing rounding behavior</a><br>
3.6.7 : <a href="arithmetic.html#Catchingexceptions">Catching exceptions</a><br>
3.6.7.1 : <a href="arithmetic.html#Compiler-specificbehavior">Compiler-specific behavior</a><br>
3.7 : <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a><br>
3.7.1 : <a href="arithmetic.html#Kahansummation">Kahan summation</a><br>
3.7.2 : <a href="arithmetic.html#Othercomputerarithmeticsystems">Other computer arithmetic systems</a><br>
3.7.3 : <a href="arithmetic.html#Extendedprecision">Extended precision</a><br>
3.7.4 : <a href="arithmetic.html#Reducedprecision">Reduced precision</a><br>
3.7.4.1 : <a href="arithmetic.html#Lowerprecisioniniterativerefinement">Lower precision in iterative refinement</a><br>
3.7.4.2 : <a href="arithmetic.html#LowerprecisioninDeepLearning">Lower precision in Deep Learning</a><br>
3.7.5 : <a href="arithmetic.html#Fixed-pointarithmetic">Fixed-point arithmetic</a><br>
3.7.6 : <a href="arithmetic.html#Complexnumbers">Complex numbers</a><br>
3.8 : <a href="arithmetic.html#Conclusions">Conclusions</a><br>
3.9 : <a href="arithmetic.html#Reviewquestions">Review questions</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>3 Computer Arithmetic</h1>
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<!-- index -->
<!-- index -->
<!-- index -->
<p name="switchToTextMode">

Of the various types of data that one normally encounters, the ones we
are concerned with in the context of scientific computing are the
numerical types: integers (or whole numbers)
$\ldots,-2,-1,0,1,2,\ldots$, real numbers $0,1,-1.5,2/3,\sqrt 2,\log
10,\ldots$, and complex numbers $1+2i,\sqrt 3-\sqrt 5i,\ldots$.
Computer hardware is organized to give only a certain amount of space to
represent each number, in multiples of bytes, each
containing 8~bits. Typical values are 4 bytes for an
integer,  4~or~8 bytes for a real number, and 8~or~16 bytes for a
complex number.
</p>

<p name="switchToTextMode">
Since only a certain amount of memory is available to store a number,
it is clear that not all numbers of a certain type can be stored. For
instance, for integers only a range is stored.
(Languages such as
<i>Python</i>
<!-- index -->
have arbitrarily 
<i>large integers</i>
, but this has no hardware support.)
In the case of real
numbers, even storing a range is not possible since any interval $[a,b]$
contains infinitely many numbers. Therefore, any
<i>representation of real numbers</i>
 will cause gaps between the
numbers that are stored.
Calculations in a computer are sometimes
described as 
<i>finite precision arithmetic</i>
.
Since many results are not representable, any computation that results in
such a number will have to be dealt with by issuing an
error or by approximating the result.
In this chapter we will look at
the ramifications of such approximations of the `true' outcome of
numerical calculations.
</p>

<p name="switchToTextMode">
For detailed discussions,
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
the ultimate reference to floating point arithmetic
  is the 
<i>IEEE 754</i>
 standard~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#754-2019">[754-2019]</a>
;
<li>
for secondary reading,
  see the book by
  Overton~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Overton:754book">[Overton:754book]</a>
; it is easy to find online copies of
  the essay by Goldberg~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#goldberg:floatingpoint">[goldberg:floatingpoint]</a>
;
<li>
for extensive
  discussions of round-off error analysis in algorithms, see the books
  by Higham~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Higham:2002:ASN">[Higham:2002:ASN]</a>
 and Wilkinson~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Wilkinson:roundoff">[Wilkinson:roundoff]</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Bits">3.1</a> Bits</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Bits">Bits</a>
</p>
</p>

<p name="switchToTextMode">
At the lowest level, computer storage and computer numbers
are organized in 
<i>bit</i>
s.
A~bit, short for `binary digit', can have the values zero and one.
Using bits we can then express numbers in 
<i>binary</i>
 notation:
\[
 \mathtt{10010}_2 \equiv \mathtt{18}_{10} 
\]
where the subscript indicates the base of the number system,
and in both cases the rightmost digit is the least significant one.
</p>

<p name="switchToTextMode">
The next organizational level of memory is the 
<i>byte</i>
:
a~byte consists of eight bits, and can therefore represent
the values~$0\cdots 255$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Use bit operations to test whether a number is odd or even.\\
  Can you think of more than one way?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">
{Bit operations}
<!-- environment: answer start embedded generator -->
</p>

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Integers">3.2</a> Integers</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a>
</p>
</p>

<p name="switchToTextMode">
In scientific computing, most operations are on real
numbers. Computations on integers rarely add up to any serious
computation load, except in applications such as cryptography.
There are also applications such as `particle-in-cell' that
can be implemented with bit operations.
However, integers are still encountered in indexing calculations.
</p>

<p name="switchToTextMode">
Integers are commonly stored in 16, 32, or 64&nbsp;bits, with 16 becoming
less common and 64 becoming more and more so. The main reason for this increase is
not the changing nature of computations, but the fact that integers
are used to index arrays. As the size of data sets grows (in
particular in parallel computations), larger indices are needed. For
instance, in 32&nbsp;bits one can store the numbers zero through
$2^{32}-1\approx 4\cdot 10^9$. In other words, a 32 bit index can
address 4 gigabytes of memory. Until recently this was enough for most
purposes; these days the need for larger data sets has made 64&nbsp;bit
indexing necessary.
</p>

<p name="switchToTextMode">
When we are indexing an array, only positive integers are needed.
In general integer computations, of course, we need to accommodate
the negative integers too. We will now discuss several strategies for
implementing negative integers. Our motivation here will be that
arithmetic on positive and negative integers should be as simple as on
positive integers only: the circuitry that we have for comparing and
operating on bitstrings should be usable for (signed) integers.
</p>

<p name="switchToTextMode">
There are several ways of implementing
negative integers. The simplest solution is to reserve one bit as a
<i>sign bit</i>
, and use the remaining 31 (or 15 or 63; from now on we will
consider 32 bits the standard) bits to store the
absolute magnitude. By comparison, we will call the straightforward
interpretation of bitstrings as 
<i>unsigned</i>
 integers.
</p>

<p name="switchToTextMode">
\[
\begin{array}{|c|rrrrrr|}
  \hline
  \hbox{bitstring}&
  00\cdots0&&hellip;&01\cdots1&
  10\cdots0&&hellip;&11\cdots1\\ \hline
  \hbox{interpretation as unsigned int}&
  0&&hellip;&2^{31}-1&
  2^{31}&&hellip;&2^{32}-1\\ \hline
  \hbox{interpretation as signed integer}&
  0&\cdots&2^{31}-1&
  -0&\cdots&-(2^{31}-1)\\
  \hline
\end{array}
\]
</p>

<p name="switchToTextMode">
This scheme has some disadvantages, one being that
there is both a positive and negative number zero. This means that a test
for equality becomes more complicated than simply testing for equality
as a bitstring. More importantly, in the second half of the
bitstring, the interpretation as signed integer decreases,
going to the right. This means that a test for greater-than becomes
complex; also
adding a positive number to a
negative number now has to be treated differently from adding it to a
positive number.
</p>

<p name="switchToTextMode">
Another solution would be to let an unsigned number $n$ be interpreted
as $n-B$ where $B$ is some plausible bias, for instance&nbsp;$2^{31}$.
</p>

<p name="switchToTextMode">
\[
\begin{array}{|c|rrrrrr|}
  \hline
  \hbox{bitstring}&
  00\cdots0&&hellip;&01\cdots1&
  10\cdots0&&hellip;&11\cdots1\\ \hline
  \hbox{interpretation as unsigned int}&
  0&&hellip;&2^{31}-1&
  2^{31}&&hellip;&2^{32}-1\\ \hline
  \hbox{interpretation as shifted int}&
  -2^{31}&&hellip;&-1&
  0&&hellip;&2^{31}-1\\ \hline
\end{array}
\]
</p>

<p name="switchToTextMode">
This shifted scheme does not suffer from the $\pm0$ problem, and
numbers are consistently ordered. However, if we compute $n-n$
by operating on the bitstring that represents&nbsp;$n$, we do not
get the bitstring for zero.
</p>

<p name="switchToTextMode">
To ensure this desired behavior,
we instead rotate the number line with positive and negative numbers
to put the pattern for zero back at zero.
The resulting scheme, which is the one that is used most commonly, is
called 
<i>2's complement</i>
. Using this scheme,
the representation of integers is
formally defined as follows.
<!-- environment: definition start embedded generator -->
</p>
<!-- TranslatingLineGenerator definition ['definition'] -->
  Let $n$ be an integer, then the 2's complement `bit pattern'
  $\beta(n)$ is a non-negative integer defined as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
If $0\leq n\leq 2^{31}-1$, the normal bit pattern for $n$ is
    used, that is
\[
 0\leq n\leq 2^{31}-1 \Rightarrow \beta(n) = n. 
\]
<li>
For $-2^{31}\leq n\leq -1$, $n$ is represented by the bit
    pattern for $2^{32}-|n|$:
\[
 -2^{31}\leq n\leq -1 \Rightarrow \beta(n) = 2^{32}-|n|. 
\]
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  We denote the inverse function that takes a bit pattern and
  interprets as integer with&nbsp;$\eta=\beta\inv$.
</p name="definition">
</definition>
<!-- environment: definition end embedded generator -->
<p name="switchToTextMode">

The following diagram shows the correspondence between bitstrings and
their interpretation as 2's complement integer:
\[
\begin{array}{|c|rrrrrr|}
  \hline
  \hbox{bitstring $n$}&
  00\cdots0&&hellip;&01\cdots1&
  10\cdots0&&hellip;&11\cdots1\\ \hline
  \hbox{interpretation as unsigned int}&
  0&&hellip;&2^{31}-1&
  2^{31}&&hellip;&2^{32}-1\\ \hline
  \hbox{interpretation $\beta(n)$ as 2's comp. integer}&
  0&\cdots&2^{31}-1&
  -2^{31}&\cdots&-1\\
  \hline
\end{array}
\]
</p>

<p name="switchToTextMode">
Some observations:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
There is no overlap between the bit patterns for positive and
  negative integers, in particular, there is only one pattern for zero.
<li>
The positive numbers have a leading bit zero, the negative
  numbers have the leading bit set.
  This makes the leading bit act like a sign bit; however note the
  above discussion.
<li>
If you have a positive number&nbsp;$n$, you get $-n$ by flipping
  all the bits, and adding&nbsp;1.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  For the `naive' scheme and the 2's complement scheme for negative
  numbers, give pseudocode for the comparison test $m&lt;n$, where $m$
  and&nbsp;$n$ are integers. Be careful to distinguish between all cases of
  $m,n$ positive, zero, or negative.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Integeroverflow">3.2.1</a> Integer overflow</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a> > <a href="arithmetic.html#Integeroverflow">Integer overflow</a>
</p>
</p>

<p name="switchToTextMode">
Adding two numbers with the same sign, or multiplying two numbers of
any sign, may lead to a result that is too
large or too small to represent. This is called 
<i>overflow</i>
;
see section&nbsp;
3.3.4
 for the corresponding floating
point phenomenon.
The following exercise lets you explore the behavior of an actual
program.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Investigate what happens when you perform such a calculation. What
  does your compiler say if you try to write down a nonrepresentable
  number explicitly, for instance in an assignment statement?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

If you program this in&nbsp;C, it is worth noting that while you
probably get an outcome that is understandable, the behavior
of overflow in the case of signed quantities is actually
<i>undefined</i>
<!-- index -->
under the C&nbsp;standard.
</p>

<h3><a id="Additionintwo'scomplement">3.2.2</a> Addition in two's complement</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a> > <a href="arithmetic.html#Additionintwo'scomplement">Addition in two's complement</a>
</p>
<p name="switchToTextMode">

Let us consider doing some simple arithmetic on 2's complement
integers. We start by assuming that we have hardware that works on unsigned integers.
The goal is to see that we can use this hardware to do calculations
on signed integers, as represented in 2's complement.
</p>

<p name="switchToTextMode">
We consider the computation of $m+n$, where $m,n$ are representable
numbers:
\[
 0\leq |m|,|n|&lt;2^{31}. 
\]
We distinguish different cases.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The easy case is $0&lt;m,n$. In that case we perform the normal
  addition, and as long as the result stays under $2^{31}$, we get the
  right result. If the result is $2^{31}$ or more, we have integer
  overflow, and there is nothing to be done about that.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/mminusn1.png" width=800></img>
<p name="caption">
FIGURE 3.1: Addition with one positive and one negative number in 2's complement
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<li>
Case $m&gt;0$, $n&lt;0$, and $m+n&gt;0$.
  Then $\beta(m)=m$ and
  $\beta(n)=2^{32}-|n|$, so the unsigned addition becomes
\[
 \beta(m)+\beta(n)=m+(2^{32}-|n|)=2^{32}+m-|n|. 
\]
  Since $m-|n|&gt;0$, this result is&nbsp;$&gt;2^{32}$.
  (See figure&nbsp;
3.1
.)
  However, we observe that
  this is basically $m+n$ with the 33rd bit set. If we ignore this
  overflowing bit, we then have the correct result.
<li>
Case $m&gt;0$, $n&lt;0$, but $m+n&lt;0$. Then
\[
 \beta(m)+\beta(n) = m+ (2^{32}-|n|) = 2^{32}-(|n|-m). 
\]
  Since $|n|-m&gt;0$ we get
\[
 \eta\bigl(2^{32}-(|n|-m)\bigr) = -\left| (|n|-m) \right| = m-|n|=m+n.
\]
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Subtractionintwo'scomplement">3.2.3</a> Subtraction in two's complement</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a> > <a href="arithmetic.html#Subtractionintwo'scomplement">Subtraction in two's complement</a>
</p>
</p>

<p name="switchToTextMode">
In exercise&nbsp;
3.2
 above you explored comparing two integers.
Let us now explore how
subtracting numbers in two's complement is implemented. Consider $0\leq
m\leq 2^{31}-1$ and $1\leq n\leq 2^{31}$ and let us see what happens
in the computation of $m-n$.
</p>

<p name="switchToTextMode">
Suppose we have an algorithm for adding and subtracting unsigned
32-bit numbers. Can we use that to subtract two's complement integers?
We start by observing that the integer subtraction $m-n$ becomes the
unsigned addition $m+(2^{32}-n)$.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Case: $m&lt;|n|$. In this case, $m-n$ is negative and
  $1\leq |m-n|\leq 2^{31}$, so the bit pattern for $m-n$ is
\[
 \beta(m-n) = 2^{32}-(n-m). 
\]
  Now, $2^{32}-(n-m)=m+(2^{32}-n)$, so we can compute
  $m-n$ in 2's complement by adding the bit patterns of $m$ and $-n$ as
  unsigned integers:
\[
 \eta\bigl( \beta(m)+\beta(-n) \bigr) =
  \eta\bigl( m+(2^{32}-|n|) \bigr) =
  \eta\bigl( 2^{32} + (m-|n|) \bigr) =
  \eta\left( 2^{32} - \bigl|m-|n|\bigr| \right) = m-|n| = m+n
  .
\]
<li>
Case: $m&gt;n$. Here we observe that
  $m+(2^{32}-n)=2^{32}+m-n$. Since $m-n&gt;0$, this is a number $&gt;2^{32}$
  and therefore not a legitimate
  representation of a negative number. However, if we store this
  number in 33 bits, we see that it is
  the correct result $m-n$, plus a single bit in the
  33-rd position. Thus, by performing the unsigned addition, and
  ignoring the 
<i>overflow bit</i>
, we again get the correct result.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
In both cases we conclude that we can perform the subtraction $m-n$ by adding
the unsigned number that represent $m$ and $-n$
and ignoring overflow if it occurs.
</p>

<h3><a id="Binarycodeddecimal">3.2.4</a> Binary coded decimal</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a> > <a href="arithmetic.html#Binarycodeddecimal">Binary coded decimal</a>
</p>

<p name="switchToTextMode">

Decimal numbers are not relevant in scientific computing, but they are
useful in financial calculations, where computations involving money
absolutely have to be exact. Binary arithmetic is at a disadvantage
here, since numbers such as $1/10$ are repeating fractions in
binary. With a finite number of bits in the mantissa
<!-- index -->
,
this means that the number $1/10$ can not be represented exactly in
binary.  For this reason, 
<i>binary-coded-decimal</i>
 schemes
were used in old IBM
<!-- index -->
 mainframes, and are in fact being
standardized in revisions of 
<i>IEEE 754</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#754-2019,ieee754-webpage">[754-2019,ieee754-webpage]</a>
; see also
section&nbsp;
3.3.7
.
</p>

<p name="switchToTextMode">
In BCD schemes, one or more decimal digits are encoded in a number of
bits. The simplest scheme would encode the digits $0&hellip;9$ in four
bits. This has the advantage that in a
BCD number each digit is readily identified; it has the disadvantage
that about $1/3$ of all bits are wasted, since 4 bits can
encode the numbers&nbsp;$0&hellip;15$.
More efficient encodings would encode $0&hellip;999$ in ten bits, which
could in principle store the numbers&nbsp;$0&hellip;1023$. While this is
efficient in the sense that few bits are wasted, identifying
individual digits in such a number takes some decoding. For this
reason, BCD arithmetic needs hardware support from the processor,
which is rarely found these days; one example is the IBM
Power architecture, starting with the 
<i>IBM Power6</i>
.
</p>

<h3><a id="Othernumberbasesforcomputerarithmetic">3.2.5</a> Other number bases for computer arithmetic</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Integers">Integers</a> > <a href="arithmetic.html#Othernumberbasesforcomputerarithmetic">Other number bases for computer arithmetic</a>
</p>
<p name="switchToTextMode">

There have been some experiments with 
<i>ternary arithmetic</i>
(see&nbsp;
<a href=http://en.wikipedia.org/wiki/Ternary_computer>http://en.wikipedia.org/wiki/Ternary_computer</a>

and&nbsp;
<a href=http://www.computer-museum.ru/english/setun.htm>http://www.computer-museum.ru/english/setun.htm</a>
), however,
no practical hardware exists.
</p>

<h2><a id="Realnumbers">3.3</a> Real numbers</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a>
</p>
<p name="switchToTextMode">

In this section we will look at how real numbers are
represented in a computer, and the limitations of various schemes. The
next section will then explore the ramifications of this for arithmetic
involving computer numbers.
</p>

<h3><a id="They'renotreallyrealnumbers">3.3.1</a> They're not really real numbers</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#They'renotreallyrealnumbers">They're not really real numbers</a>
</p>

<p name="switchToTextMode">

In the mathematical sciences, we usually work with real numbers, so
it's convenient to pretend that computers can do this too. However,
since numbers in a computer have only a finite number of bits, most
real numbers can not be represented exactly. In fact, even many
fractions can not be represented exactly, since they repeat; for
instance, $1/3=0.333&hellip;$, which is not representible in either
decimal or binary. An illustration of this is given in
appendix&nbsp;
sec:fraction-code
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Some programming languages allow you to write loops with not just an
  integer, but also with a real number as `counter'. Explain why this
  is a bad idea. Hint: when is the upper bound reached?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Whether a fraction repeats depends on the number system. (How would
you write $1/3$ in ternary, or base&nbsp;3, arithmetic?) In binary computers
this means that fractions such as $1/10$, which in decimal arithmetic
are terminating, are repeating. Since decimal arithmetic is important
in financial calculations, some
people care about the accuracy of this kind of arithmetic;
see section&nbsp;
3.2.4
 for what was done about it.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that each binary fraction, that is,
  a&nbsp;number of the form $0.01010111001_2$,
  can exactly be represented as a
  terminating decimal fraction. What is the reason that not every
  decimal fraction can be represented as a binary fraction?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Representationofrealnumbers">3.3.2</a> Representation of real numbers</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Representationofrealnumbers">Representation of real numbers</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
Real numbers are stored using a scheme that is analogous to what is
known as `scientific notation', where a number is represented as a
<i>significand</i>
 and an 
<i>exponent</i>
, for
instance&nbsp;$6.022\cdot 10^{23}$, which has a significand 
<tt>6022</tt>
 with a
<i>radix point</i>
 after the first digit, and an exponent&nbsp;
<tt>23</tt>
.
This number stands for
\[
 6.022\cdot 10^{23}= \left[
    6\times 10^0+0\times 10^{-1}+2\times10^{-2}+2\times10^{-3}
    \right] \cdot 10^{23}.
\]
We introduce a 
<i>base</i>
,
a small integer number, 10&nbsp;in the preceding example, and 2&nbsp;in computer
numbers, and write numbers in terms of it as a sum of $t$&nbsp;terms:
<!-- environment: equation start embedded generator -->
</p>
\begin{array}{rl}
    x &= \pm 1 \times
    \left[ d\_1\beta^0+d\_2\beta^{-1}+d\_3\beta^{-2}+\cdots+d\_t\beta^{-t+1}b\right]
    \times \beta^e \\
    & = \pm \Sigma\_{i=1}^t d\_i\beta^{1-i}  \times\beta^e
\end{array}
\label{eq:floatingpoint-def}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
where the components are
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
the 
<i>sign bit</i>
: a single bit storing whether the
  number is positive or negative;
<li>
$\beta$ is the base of the number system;
<li>
$0\leq d_i\leq \beta-1$ the digits of the 
<i>mantissa</i>
  or 
<i>significand</i>
 -- the location of the radix point
  (decimal point in decimal numbers) is implicitly assumed to the
  immediately following the first digit;
<li>
$t$ is the length of the mantissa;
<li>
$e\in [L,U]$ exponent; typically $L&lt;{0}&lt;{U} $ and $L\approx-U$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Note that there is an explicit sign bit for the whole number; the sign
of the exponent is handled differently.
For reasons of efficiency, $e$&nbsp;is not a signed number; instead it is
considered as an unsigned number in 
<i>excess</i>
 of a certain minimum
value. For instance, the bit pattern for the number zero is
interpreted as&nbsp;$e= L$.
</p>

<h4><a id="Someexamples">3.3.2.1</a> Some examples</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Representationofrealnumbers">Representation of real numbers</a> > <a href="arithmetic.html#Someexamples">Some examples</a>
</p>
<p name="switchToTextMode">

Let us look at some specific examples of floating point
representations. Base 10 is the most logical choice for human
consumption, but computers are binary, so base&nbsp;2 predominates
there. Old IBM
<!-- index -->
 mainframes grouped bits to make for a base&nbsp;16
representation.
</p>

<p name="switchToTextMode">
\[
\begin{array}{r|r|r|r|r}
  &\beta&t&L&U\\ \hline
  \hbox{IEEE single precision (32 bit)}&2&23&-126&127\\
  \hbox{IEEE double precision (64 bit)}&2&53&-1022&1023\\
  \hbox{Old Cray 64 bit}&2&48&-16383&16384\\
  \hbox{IBM mainframe 32 bit}&16&6&-64&63\\
  \hbox{packed decimal}&10&50&-999&999\\
\end{array}
\]
</p>

<p name="switchToTextMode">
Of these, the single and double precision formats are by far the most
common. We will discuss these in section&nbsp;
3.3.7
 and
further.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Normalizedandunnormalizednumbers">3.3.3</a> Normalized and unnormalized numbers</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Normalizedandunnormalizednumbers">Normalized and unnormalized numbers</a>
</p>

</p>

<p name="switchToTextMode">
The general definition of floating point numbers,
equation \eqref{eq:floatingpoint-def}, leaves us with the problem that numbers
have more than one representation. For instance,
$.5\times10^{2}=.05\times 10^3$. Since this would make computer
arithmetic needlessly complicated, for instance in testing equality of
numbers, we use 
  numbers}. A&nbsp;number is normalized if its first digit is nonzero.
The implies that the mantissa
<!-- index -->
 part is $\beta&gt; x_m\geq 1$.
</p>

<p name="switchToTextMode">
A practical implication in the case of binary numbers is that the
first digit is always&nbsp;1, so we do not need to store it explicitly.
In the 
<i>IEEE 754</i>
 standard, this means that every floating point number
is of the form
\[
 1.d_1d_2&hellip; d_t\times 2^{\mathrm exp}
\]
and only the digits $d_1d_2&hellip; d_t$ are stored.
</p>

<h3><a id="Limitations:overflowandunderflow">3.3.4</a> Limitations: overflow and underflow</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Limitations:overflowandunderflow">Limitations: overflow and underflow</a>
</p>

<p name="switchToTextMode">

Since we use only a finite number of bits to store floating point
numbers, not all numbers can be represented. The ones that can not be
represented fall into two categories: those that are too large or too
small (in some sense), and those that fall in the gaps.
</p>

<p name="switchToTextMode">
The second category, where a computational result has to be rounded or
truncated to be representable, is the basis of the field of round-off
error analysis. We will study this in some detail in the following
sections.
</p>

<p name="switchToTextMode">
Numbers can be
too large or too small in the following ways.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[Overflow] The largest number we can store has
  every digit equal to&nbsp;$\beta$:
\[
\begin{array}{r|c|ccc|c}
      \hline
      &\textrm{unit}&\multicolumn{3}{|c|}{\textrm{fractional}}&\textrm{exponent}\\
      \hline
      \textrm{position}&0&1&\cdots&t-1&\\
      \hline
      \textrm{digit}&\beta-1&\beta-1&\cdots&\beta-1&\\
      \hline
      \textrm{value}&1&\beta^{-1}&\cdots&\beta^{-(t-1)}&\\
      \hline
\end{array}
\]
  which adds up to
\[
 (\beta-1)\cdot1+(\beta-1)\cdot\beta\inv+\cdots
  +(\beta-1)\cdot\beta^{-(t-1)}=\beta-\beta^{-(t-1)},
\]
  and the smallest number
  (that is, the most negative)
  is
  $-\bigl(\beta-\beta^{-(t-1)}\bigr)$;
  anything larger than the
  former or smaller than the latter causes a condition called
<i>overflow</i>
.
<li>
[Underflow]The number closest to zero is $\beta^{-(t-1)}\cdot
  \beta^L$. A computation that has a result less than that (in
  absolute value) causes a condition called 
<i>underflow</i>
.
  (See section&nbsp;
3.3.3
  for 
<i>gradual underflow</i>
.)
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
The fact that only a small number of real numbers can be represented
exactly is the basis of the field of round-off error analysis. We will
study this in some detail in the following sections.
</p>

<p name="switchToTextMode">
The occurrence of overflow or underflow means that your computation
will be `wrong' from that point on. Underflow will make the computation
proceed with a zero where there should have been a nonzero;
overflow is represented as 
<tt>Inf</tt>
, short of `infinite'.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
For real numbers $x,y$, the quantity $g=\sqrt{(x^2+y^2)/2}$ satisfies
\[
 g\leq \max\{|x|,|y|\} 
\]
so it is representable if $x$ and&nbsp;$y$ are.
What can go wrong if you compute $g$ using the above formula?
Can you think of a better way?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: answer start embedded generator -->
</p>

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Computing with 
<tt>Inf</tt>
 is possible to an extent: adding two of those
quantities will again give 
<tt>Inf</tt>
. However, subtracting them gives
<tt>NaN</tt>
: `not a number'.
(See section&nbsp;
3.3.8.1
.)
</p>

<p name="switchToTextMode">
In none of these cases will the computation end:
the processor will continue, unless you tell it otherwise.
The `otherwise' consists of you telling the compiler to generate an 
<i>interrupt</i>
,
which halts the computation with an error message. See section&nbsp;
3.6.6
.
</p>

<h4><a id="Gradualunderflow">3.3.4.1</a> Gradual underflow</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Limitations:overflowandunderflow">Limitations: overflow and underflow</a> > <a href="arithmetic.html#Gradualunderflow">Gradual underflow</a>
</p>
<p name="switchToTextMode">

Another implication of the normalization scheme is that we have to amend the
definition of underflow (see section&nbsp;
3.3.4
 above):
any number less than $1\cdot\beta^L$ now causes underflow.
Trying
to compute a number less than that in absolute value is sometimes
handled by using
<i>subnormal</i>
<!-- index -->
(or 
<i>denormalized</i>
%
<!-- index -->
or 
<i>unnormalized</i>
%
<!-- index -->
) numbers,
a&nbsp;process known as 
<i>gradual underflow</i>
.
In this case, a&nbsp;special
value of the exponent indicates that the number is no longer normalized.
In the case IEEE standard arithmetic (section&nbsp;
3.3.7
)
this is done through a zero exponent field.
</p>

<p name="switchToTextMode">
However, this is typically tens or hundreds of times slower than
computing with regular floating point numbers
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {In real-time
applications such as audio processing this phenomenon is especially
noticeable;
see

<a href=http://phonophunk.com/articles/pentium4-denormalization.php?pg=3>http://phonophunk.com/articles/pentium4-denormalization.php?pg=3</a>
.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
.
At the time of this writing, only the IBM
<!-- index -->
 Power6 has
hardware support for gradual underflow.
</p>

<h3><a id="Representationerror">3.3.5</a> Representation error</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Representationerror">Representation error</a>
</p>
<p name="switchToTextMode">

Let us consider a real number that is not representable in a
computer's number system.
</p>

<p name="switchToTextMode">
An unrepresentable number is approximated either by
ordinary 
<i>rounding</i>
, rounding up or down, or 
<i>truncation</i>
.
This means that a machine number&nbsp;$\tilde x$ is the representation for
all&nbsp;$x$ in an interval around it.  With $t$ digits in the
mantissa
<!-- index -->
, this is the interval of numbers that differ
from&nbsp;$\bar x$ in the $t+1$st digit. For the mantissa
<!-- index -->
part we get:
\[
\begin{cases}
  x\in \left[\tilde x,\tilde x+\beta^{-t+1}\right)&\hbox{truncation}\\
  x\in \left[\tilde x-\frac12 \beta^{-t+1},\tilde x+\frac12 \beta^{-t+1}\right)
    &\hbox{rounding}
\end{cases}
\]
</p>

<p name="switchToTextMode">
If $x$ is a number and $\tilde x$ its representation in the computer,
we call $x-\tilde x$ the 
<i>representation error</i>
 or
<i>absolute representation error</i>
, and $\frac{x-\tilde x}{x}$
the 
<i>relative representation error</i>
. Often we are not
interested in the sign of the error, so we may apply the terms error
and relative error to $|x-\tilde x|$ and&nbsp;$|\frac{x-\tilde x}{x}|$
respectively.
</p>

<p name="switchToTextMode">
Often we are only interested in bounds on the error. If $\epsilon$ is
a bound on the error, we will write
\[
 \tilde x = x\pm\epsilon \defined
    |x-\tilde x|\leq\epsilon
    \Leftrightarrow \tilde x\in[x-\epsilon,x+\epsilon]
\]
For the relative error we note that
\[
 \tilde x =x(1+\epsilon) \Leftrightarrow
    \left|\frac{\tilde x-x}{x}\right|\leq \epsilon
\]
</p>

<p name="switchToTextMode">
Let us consider an example in decimal arithmetic, that is, $\beta=10$,
and with a 3-digit mantissa: $t=3$.  The number $x=1.256$ has a
representation that depends on whether we round or truncate: $\tilde
x_{\mathrm{round}}=1.26$, $\tilde x_{\mathrm{truncate}}=1.25$.
The error is in the 4th digit: if $\epsilon=x-\tilde x$
then $|\epsilon|&lt;\beta^{-(t-1)}$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
The number in this example had no exponent part. What are the error
and relative error if there had been one?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  In binary arithmetic the unit digit is always&nbsp;1 as remarked above.
  How does that change the representation error?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Machineprecision">3.3.6</a> Machine precision</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Machineprecision">Machine precision</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
Often we are only interested in the order of magnitude of the
representation error,
and we will write $\tilde x=x(1+\epsilon)$, where&nbsp;$|\epsilon|\leq\beta^{-t}$.
This maximum relative error is called the 
  precision}, or sometimes 
<i>machine epsilon</i>

  epsilon|see{machine precision}}. Typical values are:
\[
\begin{cases}
  \epsilon\approx10^{-7}&\hbox{32-bit single precision}\\
  \epsilon\approx10^{-16}&\hbox{64-bit double precision}
\end{cases}
\]
Machine precision can be defined another way: $\epsilon$&nbsp;is the
smallest number that can be added to&nbsp;$1$ so that $1+\epsilon$ has a
different representation than&nbsp;$1$. A&nbsp;small example shows how aligning
exponents can shift a too small operand so that it is effectively ignored in
the addition operation:
\[
\begin{array}{cll}
   &1.0000&\times 10^0\\
  +&1.0000&\times 10^{-5}\\ \hline
\end{array}
\quad\Rightarrow\quad
\begin{array}{cll}
   &1.0000&\times 10^0\\
  +&0.00001&\times 10^0\\ \hline
  =&1.0000&\times 10^0
\end{array}
\]
Yet another way of looking at this
is to observe that, in the addition $x+y$, if the ratio of $x$ and&nbsp;$y$
is too large, the result will be identical to&nbsp;$x$.
</p>

<p name="switchToTextMode">
The machine precision is the maximum attainable accuracy of
computations: it does not make sense to ask for more than 6-or-so
digits accuracy in single precision, or 15 in double.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Write a small program that computes the machine epsilon. Does it
  make any difference if you set the
<i>compiler optimization levels</i>
 low or high?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The number $e\approx 2.72$, the base for the natural logarithm, has
  various definitions. One of them is
\[
 e=\lim_{n\rightarrow\infty} (1+1/n)^n. 
\]
  Write a single precision program that tries to compute&nbsp;$e$ in this
  manner. (Do not use the 
<tt>pow</tt>
 function: code the power explicitly.)
  Evaluate the expression for an upper bound $n=10^k$ with
  $k=1,&hellip;,10$. Explain the output for large&nbsp;$n$. Comment on the
  behavior of the error.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The exponential function&nbsp;$e^x$ can be computed as
\[
 e=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots 
\]
  Code this and try it for some positive&nbsp;$x$,
  for instance $x=1,3,10,50$.
  How many terms do you compute?
</p>

<p name="switchToTextMode">
  Now compute $e^{-x}$ for those values.
  Use for instance the same number of iterations
  as for&nbsp;$e^x$.
</p>

<p name="switchToTextMode">
  What do you observe about the $e^x$ versus $e^{-x}$
  computation? Explain.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="TheIEEE754standardforfloatingpointnumbers">3.3.7</a> The IEEE 754 standard for floating point numbers</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#TheIEEE754standardforfloatingpointnumbers">The IEEE 754 standard for floating point numbers</a>
</p>

<!-- index -->
<!-- index -->
</p>

<p name="switchToTextMode">
Some decades ago, issues like the length of the
mantissa
<!-- index -->
 and the rounding behavior of operations
could differ between computer manufacturers, and even between models
from one manufacturer. This was obviously a bad situation from a point
of portability of codes and reproducibility of results.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
\[
\begin{array}{| l || l || l |}
    \hline
    \hbox{sign}&\hbox{exponent}&\hbox{mantissa}\\
    \hline
    p&e=e_1\cdots e_8 &s=s_1
&hellip;
s_{23}\\
    \hline
    31&30\cdots 23&22\cdots 0\\
    \hline
    \pm & 2^{e-127} & 2^{-s_1}+\cdots + 2^{-s_{23}} \\
    & \hbox{(except $e=0,255$)} & \\
    \hline
\end{array}
\quad
\begin{array}{| l || l || l |}
    \hline
    \hbox{sign}&\hbox{exponent}&\hbox{mantissa}\\
    \hline
    s&e_1\cdots e_{11}&s_1
&hellip;
s_{52}\\
    \hline
    63&62\cdots 52&51\cdots 0\\
    \hline
\end{array}
\]
<p name="caption">
FIGURE 3.2: Single and double precision definition
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
The 
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#754-2019">[754-2019]</a>
codified all this in 1985, for instance stipulating 24 and 53 bits for the
mantissa
<!-- index -->
 in single and double precision arithmetic,
using a storage sequence of sign bit, exponent, mantissa
see figure&nbsp;
3.2
.
Figure&nbsp;
3.3
 spells out the meanings of all possible
bit patterns of the single precision standard.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
  The full name of the 754 standard is `IEEE Standard for Binary
  Floating-Point Arithmetic (ANSI/IEEE Std 754-1985)'. It is also
  identical to IEC 559: `Binary floating-point arithmetic for
  microprocessor systems', superseded by ISO/IEC/IEEE 60559:2011.
<!-- index -->
</p>

<p name="switchToTextMode">
  IEEE 754 is a standard for binary arithmetic;
  there is a further standard,
<i>IEEE 854</i>
, that allows decimal arithmetic.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
  `It was remarkable that so many hardware
  people there, knowing how difficult p754 would be, agreed that it
  should benefit the community at large. If it encouraged the
  production of floating-point software and eased the development of
  reliable software, it would help create a larger market for
  everyone's hardware. This degree of altruism was so astonishing that
  MATLAB's creator Dr. Cleve Moler used to advise foreign visitors not
  to miss the country's two most awesome spectacles: the Grand Canyon,
  and meetings of IEEE p754.' W. Kahan,
  
<a href=http://www.cs.berkeley.edu/&nbsp;wkahan/ieee754status/754story.html>http://www.cs.berkeley.edu/&nbsp;wkahan/ieee754status/754story.html</a>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<i>least significant byte</i>
 is stored first, the system is
<i>little-endian</i>
; if the 
<i>big-endian</i>
. See
</p>

<p name="switchToTextMode">
The standard also declared the rounding behavior
to be 
<i>correct rounding</i>
: the result of an operation should be the
rounded version of the exact result. There will be much more on the
influence of rounding (and truncation) on numerical computations, below.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
\[
\begin{array}{|r|r|l|}
    \hline
    (e_1\cdots e_8)&\hbox{numerical value}&\hbox{range}\\
    \hline
    (0\cdots0)=0& \pm 0.s_1\cdots s_{23}\times 2^{-126}
            &s=0\cdots 01\Rightarrow 2^{-23}\cdot 2^{-126}=2^{-149}\approx 10^{-50}\\
    &       &s=1\cdots 11\Rightarrow (1-2^{-23})\cdot 2^{-126}\\
    \hline
    (0\cdots 01)=1& \pm 1.s_1\cdots s_{23}\times 2^{-126}
            &s=0\cdots 00\Rightarrow 1\cdot 2^{-126}\approx 10^{-37}\\
    \hline
    (0\cdots 010)=2& \pm 1.s_1\cdots s_{23}\times 2^{-125}&\\
    \hline
    \cdots&&\\
    \hline
    (01111111)=127 & \pm 1.s_1\cdots s_{23}\times 2^{0}
            &s=0\cdots 00\Rightarrow 1\cdot 2^{0}=1\\
    &       &s=0\cdots 01\Rightarrow 1+2^{-23}\cdot 2^{0}=1+\epsilon\\
    &       &s=1\cdots 11\Rightarrow (2-2^{-23})\cdot 2^{0}=2-\epsilon\\
    \hline
    (10000000)=128 & \pm 1.s_1\cdots s_{23}\times 2^{1}&\\
    \hline
    \cdots&&\\
    \hline
    (11111110)=254 & \pm 1.s_1\cdots s_{23}\times 2^{127}&\\
    \hline
    (11111111)=255 & s_1\cdots s_{23}=0 \Rightarrow \pm\infty &\\
                   & s_1\cdots s_{23}\not=0 \Rightarrow 
<tt>NaN</tt>
 &\\
    \hline
\end{array}
\]
<p name="caption">
FIGURE 3.3: Interpretation of single precision numbers depending on the exponent bit pattern
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Above (section&nbsp;
3.3.4
), we have seen the phenomena of
overflow and underflow, that is, operations leading to unrepresentable
numbers. There is a further exceptional situation that needs to be
dealt with: what result should be returned if the program asks for
illegal operations such as&nbsp;$\sqrt{-4}$? The IEEE 754 standard has two
special quantities for this: 
<tt>Inf</tt>
 and&nbsp;
<tt>NaN</tt>
 for
`infinity' and `not a number'.  Infinity is the result of overflow or
dividing by zero, not-a-number is the result of, for instance,
subtracting infinity from infinity.  If 
<tt>NaN</tt>
 appears in an
expression, the whole expression will evaluate to that value. The rule
for computing with 
<tt>Inf</tt>
 is a bit more
complicated&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#goldberg:floatingpoint">[goldberg:floatingpoint]</a>
.
</p>

<p name="switchToTextMode">
An inventory of the meaning of all bit patterns in IEEE
754 single precision is given in figure&nbsp;
3.3
.
Recall from section&nbsp;
3.3.3
 above
that for normalized numbers the first nonzero digit is a&nbsp;1, which
is not stored, so the bit pattern $d_1d_2&hellip; d_t$ is interpreted as
$1.d_1d_2&hellip; d_t$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Every programmer, at some point in their life, makes the mistake of
  storing a real number in an integer or the other way around. This
  can happen for instance if you call a function differently from how
  it was defined.
<!-- environment: verbatim start embedded generator -->
</p>
void a(double x) {....}
int main() {
  int i;
  .... a(i) ....
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
What happens when you print 
<tt>x</tt>
 in the function? Consider the bit
pattern for a small integer, and use the table in
figure&nbsp;
3.3
 to interpret it as a floating point
number. Explain that it will be a subnormal number
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {This
  is one of those errors you won't forget after you make it. In the
  future, whenever you see a number on the order of $10^{-305}$ you'll
  recognize that you probably made this error.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

These days, almost all processors adhere to the IEEE 754 standard.
Early generations of the 
<i>NVidia Tesla</i>
<i>GPU</i>
s were not standard-conforming in single precision.
The justification
for this was that single precision is more likely used for graphics,
where exact compliance matters less. For many scientific computations,
double precision is necessary, since the precision of calculations
gets worse with increasing problem size or runtime. This is true for
the sort of calculations in chapter&nbsp;
Numerical treatment of differential equations
, but not for
others such as 
<h3><a id="Floatingpointexceptions">3.3.8</a> Floating point exceptions</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a>
</p>
</p>

<p name="switchToTextMode">
Various operations can give a result that is not representable as a
floating point number. This situation is called an 
<i>exception</i>
,
and we say that an exception is 
<i>raised</i>

<!-- index -->
.
(Note: these are not C++ or python exceptions.)
The result depends on the type of error, and the computation
progresses normally. (It is possible to have the program
be interrupted: section&nbsp;
3.6.7
.)
</p>

<h4><a id="Not-a-Number">3.3.8.1</a> Not-a-Number</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a> > <a href="arithmetic.html#Not-a-Number">Not-a-Number</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Processors will represent as 
<i>NaN</i>

(`not a number')
the result of:
</p>

<!-- environment: itemize start embedded generator -->
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Addition 
<tt>Inf-Inf</tt>
, but note that 
<tt>Inf+Inf</tt>
 gives 
<tt>Inf</tt>
;
<li>
Multiplication $0\times
<tt>Inf</tt>
$;
<li>
Division $0/0$ or $
<tt>Inf</tt>
/
<tt>Inf</tt>
$;
<li>
Remainder $
<tt>Inf</tt>
\mathop{\mathrm{rem}} x$ or
  $x\mathop{\mathrm{rem}} 
<tt>Inf</tt>
$;
<li>
$\sqrt{x}$ for $x&lt;0$;
<li>
Comparison $x&lt;y$ or $x&gt;y$ where any operand is 
<tt>Nan</tt>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Since the processor can continue computing with such a number, it is
referred to as a 
<i>quiet NaN</i>
. By contrast, some NaN
quantities can cause the processor to generate an
<i>interrupt</i>
 or 
<i>exception</i>
. This is called a
<i>signalling NaN</i>
.
</p>

<p name="switchToTextMode">
There are uses for a signalling NaN. You could for instance fill
allocated memory with such a value, to indicate that it is
uninitialized for the purposes of the computation. Any use of such a
value is then a program error, and would cause an exception.
</p>

<p name="switchToTextMode">
The 
<i>2008 revision</i>
%
<!-- index -->
 of IEEE&nbsp;754 suggests using the most
significant bit of a NaN as the 
<tt>is_quiet</tt>
 bit to distinguish
between quiet and signalling NaNs.
</p>

<p name="switchToTextMode">
See 
<a href=https://www.gnu.org/software/libc/manual/html_node/Infinity-and-NaN.html>https://www.gnu.org/software/libc/manual/html_node/Infinity-and-NaN.html</a>

for treatment of 
<tt>Nan</tt>
 in the GNU compiler.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="Dividebyzero">3.3.8.2</a> Divide by zero</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a> > <a href="arithmetic.html#Dividebyzero">Divide by zero</a>
</p>
</p>

<p name="switchToTextMode">
Division by zero results in 
<tt>Inf</tt>
.
</p>

<h4><a id="Overflow">3.3.8.3</a> Overflow</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a> > <a href="arithmetic.html#Overflow">Overflow</a>
</p>
<p name="switchToTextMode">

This exception is raised if a result is not representable
as a finite number.
</p>

<h4><a id="Underflow">3.3.8.4</a> Underflow</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a> > <a href="arithmetic.html#Underflow">Underflow</a>
</p>
<p name="switchToTextMode">

This exception is raised if a number is too small to be represented.
</p>

<h4><a id="Inexact">3.3.8.5</a> Inexact</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Realnumbers">Real numbers</a> > <a href="arithmetic.html#Floatingpointexceptions">Floating point exceptions</a> > <a href="arithmetic.html#Inexact">Inexact</a>
</p>
<p name="switchToTextMode">

This exception is raised for inexact results such as square roots,
or overflow if that is not trapped.
</p>

<h2><a id="Round-offerroranalysis">3.4</a> Round-off error analysis</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a>
</p>
<!-- index -->
<p name="switchToTextMode">

Numbers that are too large or too small to be represented, leading to
overflow and underflow, are
uncommon: usually computations can be arranged so that this situation
will not occur. By contrast, the case that the result of a computation
(even something as simple as a single addition)
is not exactly representable is very common.
Thus, looking at the
implementation of an algorithm, we need to analyze the
effect of such small errors propagating through the computation.
This is commonly called
<i>round-off error analysis</i>
.
</p>

<h3><a id="Correctrounding">3.4.1</a> Correct rounding</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Correctrounding">Correct rounding</a>
</p>
<p name="switchToTextMode">

The IEEE 754 standard, mentioned in section&nbsp;
3.3.7
, does
not only declare the way a floating point number is stored, it also
gives a standard for the accuracy of operations such as addition,
subtraction, multiplication, division. The model for arithmetic in the
standard is that of 
<i>correct rounding</i>
%
<!-- index -->
<!-- index -->
:
the result of an
operation should be as if the following procedure is followed:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The exact result of the operation is computed, whether this is
  representable or not;
<li>
This result is then rounded to the nearest computer number.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
In short: the representation of the result of an
operation is the rounded exact result of that operation.
Of course,
after two operations it no longer needs to hold that the computed
result is the exact rounded version of the exact result.
</p>

<p name="switchToTextMode">
If this statement sounds trivial or self-evident, consider subtraction
as an example. In a decimal number system with two digits in the
mantissa, the computation
$1.0-\fp{9.4}{-1}=1.0-0.94=0.06=\fp{0.6}{-2}$. Note that in an
intermediate step the mantissa $.094$ appears, which has one more
digit than the two we declared for our number system. The extra digit
is called a 
<i>guard digit</i>
.
</p>

<p name="switchToTextMode">
Without a guard digit, this operation would have proceeded as
$1.0-\fp{9.4}{-1}$, where $\fp{9.4}{-1}$ would be normalized to&nbsp;$0.9$,
giving a final result of&nbsp;$0.1$, which is almost double the correct result.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the computation $1.0-\fp{9.5}{-1}$, and assume again that
  numbers are rounded to fit the 2-digit mantissa. Why is this
  computation in a way a lot worse than the example?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">
One guard digit is not enough to guarantee correct rounding. An
analysis that we will not reproduce here shows that three extra bits
are needed&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Goldberg:arithmetic">[Goldberg:arithmetic]</a>
.
</p>

<h4><a id="Mul-Addoperations">3.4.1.1</a> Mul-Add operations</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Correctrounding">Correct rounding</a> > <a href="arithmetic.html#Mul-Addoperations">Mul-Add operations</a>
</p>
<p name="switchToTextMode">

In 2008, the IEEE&nbsp;754 standard was revised to include the behavior
of 
<i>FMA</i>
 operations, that is, operations of the
form 
\[
 c \leftarrow a*b+c. 
\]
This operation has a twofold motivation.
</p>

<p name="switchToTextMode">
First, the 
<span title="acronym" ><i>FMA</i></span>
 is potentially more accurate than a separate
multiplication and addition, since it can use higher precision for the
intermediate results, for instance by using the 80-bit
<i>extended precision</i>
    precision}} format;
section&nbsp;
3.7.3
.
</p>

<p name="switchToTextMode">
The standard here defines correct rounding to be that the result of
this combined computation should be the rounded correct
result. A&nbsp;naive implementation of this operations would involve two
roundings: one after the multiplication and one after the
addition\footnoteenv
{On the other hand, if the behavior of an
  application was `certified' using a non-FMA architecture, the
  increased precision breaks the certification. Chip manufacturers
  have been known to get requests for a `double-rounding' FMA to
  counteract this change in numerical behavior.}.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you come up with an example where correct rounding of
  an 
<span title="acronym" ><i>FMA</i></span>
 is considerably more accurate than rounding the
  multiplication and addition separately? Hint: let the 
<tt>c</tt>
 term be
  of opposite sign as 
<tt>a*b</tt>
, and try to force cancellation in the
  subtraction.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Secondly, 
<span title="acronym" ><i>FMA</i></span>
 instructions are a way of getting higher
performance: through pipelining we asymptotially get two operations
per cycle. An 
<span title="acronym" ><i>FMA</i></span>
 unit is then cheaper to construct than separate
addition and multiplication units. Fortunately, the 
<span title="acronym" ><i>FMA</i></span>
 occures
frequently in practical calculations.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you think of some linear algebra operations that features
<span title="acronym" ><i>FMA</i></span>
 operations?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

See section&nbsp;
1.2.1.2
 for historic use of 
<span title="acronym" ><i>FMA</i></span>
 in processors.
</p>

<h3><a id="Addition">3.4.2</a> Addition</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Addition">Addition</a>
</p>
<p name="switchToTextMode">

Addition of two floating point numbers is done in a couple of steps.
First the exponents are aligned: the smaller of the two numbers is
written to have the same exponent as the larger number. Then the
mantissas
<!-- index -->
 are added. Finally, the result is adjusted
so that it again is a normalized number.
</p>

<p name="switchToTextMode">
As an example, consider $1.00+2.00\times 10^{-2}$. Aligning the
exponents, this becomes $1.00+0.02=1.02$, and this result requires no
final adjustment. We note that this computation was exact, but
the sum $1.00+2.55\times 10^{-2}$ has the same result, and here the
computation is clearly not exact: the exact result is $1.0255$, which
is not representable with three digits to the mantissa.
</p>

<p name="switchToTextMode">
In the example $6.15\times 10^1+3.98\times 10^1=10.13\times 10^1=1.013\times
10^2\rightarrow 1.01\times 10^2$ we see that after addition of the mantissas an
adjustment of the exponent is needed. The error again comes from
truncating or rounding the first digit of the result that does not fit
in the mantissa: if $x$ is the true sum and $\tilde x$ the computed
sum, then $\tilde x=x(1+\epsilon)$ where, with a 3-digit mantissa
$|\epsilon|&lt;10^{-3}$.
</p>

<p name="switchToTextMode">
Formally, let us consider the computation of
$s=x_1+x_2$, and we assume that the numbers&nbsp;$x_i$ are represented
as $\tilde x_i=x_i(1+\epsilon_i)$.
Then the sum&nbsp;$s$ is represented as
\[
\begin{array}{rl}
\tilde s&=(\tilde x_1+\tilde x_2)(1+\epsilon_3)\\
&=x_1(1+\epsilon_1)(1+\epsilon_3)+x_2(1+\epsilon_2)(1+\epsilon_3)\\
&\approx x_1(1+\epsilon_1+\epsilon_3)+x_2(1+\epsilon_2+\epsilon_3)\\
&\approx s(1+2\epsilon)
\end{array}
\]
under the assumptions that all&nbsp;$\epsilon_i$ are small and of roughly
equal size, and that both $x_i&gt;0$.
We see that the relative errors are added under addition.
</p>

<h3><a id="Multiplication">3.4.3</a> Multiplication</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Multiplication">Multiplication</a>
</p>
<p name="switchToTextMode">

Floating point multiplication, like addition, involves several steps.
In order to multiply two numbers $m_1\times\beta^{e_1}$
and&nbsp;$m_2\times\beta^{e_2}$, the following steps are needed.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The exponents are added: $e\leftarrow e_1+e_2$.
<li>
The mantissas
<!-- index -->
 are multiplied: $m\leftarrow
  m_1\times m_2$.
<li>
The mantissa is normalized, and the exponent adjusted accordingly.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

For example: $\fp{1.23}{0}\times\fp{5.67}1=\fp{0.69741}1\rightarrow
\fp{6.9741}0\rightarrow\fp{6.97}0$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze the relative error of multiplication.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Subtraction">3.4.4</a> Subtraction</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Subtraction">Subtraction</a>
</p>

</p>

<p name="switchToTextMode">
Subtraction behaves very differently from addition. Whereas in
addition errors are added, giving only a gradual increase of overall
roundoff error, subtraction has the potential for greatly increased
error in a single operation.
</p>

<p name="switchToTextMode">
For example, consider subtraction with 3 digits to the mantissa:
$1.24-1.23=0.01\rightarrow \fp{1.00}{-2}$. While the result is exact,
it has only one significant digit\footnote
{Normally, a number with 3
  digits to the mantissa suggests an error corresponding to rounding
  or truncating the fourth digit. We say that such a number has 3
<i>significant digits</i>
. In this case, the last two digits have no
  meaning, resulting from the normalization process.}.
To see this, consider
the case where the first operand&nbsp;$1.24$ is actually the rounded result
of a computation that should have resulted in&nbsp;$1.235$:
\[
\begin{array}{rll}
  .5\times 2.47-1.23&=1.235-1.23&=0.005\\
  \downarrow&&=5.0\cdot 10^{-3}\\
  1.24-1.23&=0.01=1.\cdot 10^{-2}\\
\end{array}
\]
Now, there is a 100\% error, even though the relative error of the
inputs was as small as could be expected. Clearly, subsequent
operations involving the result of this subtraction will also be
inaccurate.
We conclude that subtracting almost equal numbers is a likely cause of
numerical roundoff.
Section&nbsp;
3.5.1
 discusses a practical example.
</p>

<p name="switchToTextMode">
There are some subtleties about this example. Subtraction of almost
equal numbers is exact, and we have the correct rounding behavior of
IEEE arithmetic. Still, the correctness of a single operation does not
imply that a sequence of operations containing it will be
accurate. While the addition example showed only modest decrease of
numerical accuracy, the cancellation in this example can have
disastrous effects. You'll see an example in section&nbsp;
3.5.1
.
</p>

<!-- environment: comment start embedded generator -->

</comment>
<!-- environment: comment end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the iteration
\[
 x_{n+1}=f(x_n) = \
\begin{cases}
    2x_n&\hbox{if $2x_n&lt;1$}\\
    2x_n-1&\hbox{if $2x_n\&gt;1$}\\
\end{cases}
\]
  Does this function have a fixed point, $x_0\equiv f(x_0)$, or is there a cycle
  $x_1=f(x_0),\,x_0\equiv x_2=f(x_1)$ et cetera?
</p>

<p name="switchToTextMode">
  Now code this function. Is it possible to reproduce the fixed
  points?
  What happens with various starting
  points&nbsp;$x_0$. Can you explain this?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Associativity">3.4.5</a> Associativity</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Round-offerroranalysis">Round-off error analysis</a> > <a href="arithmetic.html#Associativity">Associativity</a>
</p>

</p>

<p name="switchToTextMode">
Another effect of the way floating point numbers are treated
is on the
<i>associativity</i>
<!-- index -->
of operations such as summation.
While summation is mathematically associative, this is no longer
the case in computer arithmetic.
</p>

<p name="switchToTextMode">
Let's consider a simple example, showing how this can be caused
by the rounding behavior of floating point numbers.
Let floating point numbers be stored as a single digit for the mantissa,
one digit for the exponent, and one 
<i>guard digit</i>
;
now consider the computation of $4+6+7$.
Evaluation left-to-right gives:
\[
\begin{array}{l@{{}\Rightarrow{}}lp{3in}}
(4\cdot10^0 + 6\cdot10^0)+7\cdot10^0&10\cdot10^0+7\cdot10^0&addition\\
&1\cdot 10^1 + 7\cdot10^0&rounding\\
&1.0\cdot 10^1 + 0.7\cdot10^1&using guard digit\\
&1.7\cdot 10^1\\
&2\cdot10^1&rounding
\end{array}
\]
On the other hand, evaluation right-to-left gives:
\[
\begin{array}{l@{{}\Rightarrow{}}lp{3in}}
4\cdot10^0 + (6\cdot10^0 + 7\cdot10^0)&4\cdot 10^0 + 13\cdot10^0&addition\\
&4\cdot 10^0 + 1\cdot10^1&rounding\\
&0.4\cdot 10^1 + 1.0\cdot10^1&using guard digit\\
&1.4\cdot 10^1\\
&1\cdot10^1&rounding
\end{array}
\]
The conclusion is that the sequence in which rounding and truncation is applied to
intermediate results makes a difference. You can also observe that starting with the smaller
numbers gives a more accurate result.
In section&nbsp;
3.5.2
 you'll see a more elaborate example of this principle.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The above example used rounding. Can you come up with a similar example
  in an arithmetic system using truncation?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Usually, the evaluation order of expressions is determined by the definition of the
programming language, or at least by the compiler.
In section&nbsp;
3.5.5
 we will see how in parallel computations the associativity
is not so uniquely determined.
</p>

<h2><a id="Examplesofround-offerror">3.5</a> Examples of round-off error</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a>
</p>
<p name="switchToTextMode">

From the above, the reader may got the impression that roundoff errors
only lead to serious problems in exceptional circumstances. In this
section we will discuss some very practical examples where the
inexactness of computer arithmetic becomes visible in the result of a
computation. These will be fairly simple examples; more complicated
examples exist that are outside the scope of this book, such as the
instability of matrix inversion. The interested reader is referred
to&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Higham:2002:ASN,Wilkinson:roundoff">[Higham:2002:ASN,Wilkinson:roundoff]</a>
.
</p>

<h3><a id="Cancellation:the`abc-formula'">3.5.1</a> Cancellation: the `abc-formula'</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a> > <a href="arithmetic.html#Cancellation:the`abc-formula'">Cancellation: the `abc-formula'</a>
</p>

<p name="switchToTextMode">

As a practical example, consider the quadratic equation $ax^2+bx+c=0$
which has solutions $x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$.
Suppose $b&gt;0$ and $b^2\gg 4ac$ then $\sqrt{b^2-4ac}\approx b$ and
the `$+$' solution will be
inaccurate. In this case it is better
to compute $x_-=\frac{-b-\sqrt{b^2-4ac}}{2a}$ and use $x_+\cdot x_-=c/a$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Normalize $x$ by finding an integer $e$ such that
<li>
Now truncate this number to $d$ digits by
<li>
Multiply this truncated number by&nbsp;$10^{-e}$ to revert the
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Explore computing the roots of 
\[
 \epsilon x^2 -(1+\epsilon^2)x + \epsilon 
\]
  by the `textbook' method and as described above.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
What are the roots?
<li>
Why does the textbook method compute one root as zero,
    for $\epsilon$ small enough? Can you think of an example where this
    is very bad?
<li>
What are the computed function values in both methods? Relative errors?
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Write a program that computes the roots of the quadratic equation, both
  the `textbook' way, and as described above.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Let $b=-1$ and $a=-c$, and $4ac\downarrow 0$ by taking progressively smaller
    values for $a$ and&nbsp;$c$.
<li>
Print out the computed root, the root using the stable computation,
    and the value of $f(x)=ax^2+bx+c$ in the computed root.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  Now suppose that you don't care much about the actual value of the root:
  you want to make sure the residual&nbsp;$f(x)$ is small in the computed root.
  Let $x^*$&nbsp;be the exact root, then
\[
 f(x^*+h)\approx f(x^*)+hf'(x^*) = hf'(x^*). 
\]
  Now investigate separately the cases $a\downarrow 0,c=-1$ and $a=-1,c\downarrow0$.
  Can you explain the difference?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the functions
\[
\begin{cases}
    f(x) = \sqrt{x+1}-\sqrt{x}\\
    g(x) = 1/\bigl(\sqrt{x+1}+\sqrt{x}\bigr)
\end{cases}
\]
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Show that they are the same in exact arithmetic; however:
<li>
Show that $f$ can exhibit cancellation and that $g$ has no such problem.
<li>
Write code to show the difference between $f$ and&nbsp;$g$. You may
    have to use large values for&nbsp;$x$.
<li>
Analyze the cancellation in terms of $x$ and machine
    precision.  When are $\sqrt{x+1}$ and $\sqrt{x}$ less than
    $\epsilon$ apart?  What happens then? (For a more refined
    analysis, when are they $\sqrt\epsilon$ apart, and how does that
    manifest itself?)
<li>
The inverse function of $y=f(x)$ is
\[
 x = (y^2-1)^2 / (4y^2) 
\]
    Add this to your code. Does this give any indication of the
    accuracy of the calculations?
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  Make sure to test your code in single and double precision.
  If you speak python, try the 
<tt>bigfloat</tt>
 package.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Summingseries">3.5.2</a> Summing series</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a> > <a href="arithmetic.html#Summingseries">Summing series</a>
</p>

</p>

<p name="switchToTextMode">
The previous example was about preventing a large roundoff error in a
single operation. This example shows that even gradual buildup of
roundoff error can be handled in different ways.
</p>

<p name="switchToTextMode">
Consider the sum $\sum_{n=1}^{10000}\frac{1}{n^2}=1.644834$
and assume we are working with single precision, which on most computers
means a machine precision of&nbsp;$10^{-7}$. The problem with this example
is that both the ratio between terms, and the ratio of terms to
partial sums, is ever increasing. In section&nbsp;
3.3.6
 we
observed that a too large ratio can lead to one operand of an addition in
effect being ignored.
</p>

<p name="switchToTextMode">
If we sum the series in the sequence it is given, we observe that
the first term is&nbsp;1, so all partial sums ($\sum_{n=1}^N$&nbsp;where $N&lt;10000$)
are at least&nbsp;1. This means that any term where $1/n^2&lt;10^{-7}$ gets
ignored since it is less than the machine precision.
Specifically, the last 7000 terms are ignored, and
the computed sum is&nbsp;$1.644725$. The first 4 digits are correct.
</p>

<p name="switchToTextMode">
However, if we evaluate the sum in reverse order
we obtain the exact result in single precision. We are still adding
small quantities to larger ones, but now the ratio will never be as
bad as one-to-$\epsilon$, so the smaller number is never ignored.
To see this,
consider the ratio of two terms subsequent terms:
\[
 \frac{n^2}{(n-1)^2}=\frac{n^2}{n^2-2n+1}=\frac1{1-2/n+1/n^2}
    \approx 1+\frac2n
\]
Since we only sum $10^5$ terms and the machine precision is $10^{-7}$,
in the addition $1/n^2+1/(n-1)^2$ the second term will not be wholly
ignored as it is when we sum from large to small.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  There is still a step missing in our reasoning. We have shown that
  in adding two subsequent terms, the smaller one is not
  ignored. However, during the calculation we add partial sums to the
  next term in the sequence. Show that this does not worsen the situation.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The lesson here is that series that are monotone (or close to
monotone) should be summed from small to large, since the error is
minimized if the quantities to be added are closer in magnitude. Note
that this is the opposite strategy from the case of subtraction, where
operations involving similar quantities lead to larger errors. This
implies that if an application asks for adding and subtracting series
of numbers, and we know a priori which terms are positive and
negative, it may pay off to rearrange the algorithm accordingly.
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The sine function is defined as
\[
\begin{array}{rcl}
    \sin(x) &=& x-\frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots\\
    &=& \sum_{i\geq 0}^\infty (-1)^{i}\frac{x^{2i+1}}{(2i+1)!}.
\end{array}
\]
  Here are two code fragments that compute this sum (assuming that 
<tt>x</tt>

  and 
<tt>nterms</tt>
 are given):
<!-- environment: multicols start embedded generator -->
</p>
<!-- TranslatingLineGenerator multicols ['multicols'] -->
\unitindent=0pt \scriptsize
<!-- environment: verbatim start embedded generator -->
</p>
double term = x, sum = term;
for (int i=1; i&lt;=nterms; i+=2) {
  term *=
    - x*x / (double)((i+1)*(i+2));
  sum += term;
}
printf("Sum: %e\n\n",sum);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

<!-- environment: verbatim start embedded generator -->
</p>
double term = x, sum = term;
double power = x, factorial = 1., factor = 1.;
for (int i=1; i&lt;=nterms; i+=2) {
  power *= -x*x;
  factorial *= (factor+1)*(factor+2);
  term = power / factorial;
  sum += term; factor += 2;
}
printf("Sum: %e\n\n",sum);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</multicols>
<!-- environment: multicols end embedded generator -->
<!-- environment: itemize start embedded generator -->
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
    Explain what happens if you compute a large number of terms for
    $x&gt;1$.
<li>
Does either code make sense for a large number of terms?
<li>
Is it possible to sum the terms starting at the smallest?
    Would that be a good idea?
<li>
Can you come with other schemes to improve the computation
    of&nbsp;$\sin(x)$?
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Unstablealgorithms">3.5.3</a> Unstable algorithms</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a> > <a href="arithmetic.html#Unstablealgorithms">Unstable algorithms</a>
</p>
</p>

<p name="switchToTextMode">
We will now consider an example where we can give a direct argument
that the algorithm can not cope with problems due to inexactly
represented real numbers.
</p>

<p name="switchToTextMode">
Consider the recurrence
$y_n=\int_0^1 \frac{x^n}{x-5}dx = \frac1n-5y_{n-1}$,
which is monotonically decreasing; the first term can
be computed as&nbsp;$y_0=\ln 6 - \ln 5$.
</p>

<p name="switchToTextMode">
Performing the computation in 3 decimal digits we get:
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  computation</td><td></td><td>correct result</td></tr>
<tr><td>
  $y_0=\ln 6 - \ln 5=.182|322\times 10^{1}</td><td>hellip;$</td><td></td><td>1.82</td></tr>
<tr><td>
  $y_1=.900\times 10^{-1}$</td><td></td><td>.884</td></tr>
<tr><td>
  $y_2=.500\times 10^{-1}$</td><td></td><td>.0580</td></tr>
<tr><td>
  $y_3=.830\times 10^{-1}$</td><td>going up?</td><td>.0431</td></tr>
<tr><td>
  $y_4=-.165$</td><td>negative?</td><td>.0343
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

We see that the computed results are quickly not just inaccurate, but
actually nonsensical. We can analyze why this is the case.
</p>

<p name="switchToTextMode">
If we define the error $\epsilon_n$ in the $n$-th step as
\[
 \tilde y_n-y_n=\epsilon_n,
\]
 then
\[
 \tilde y_n=1/n-5\tilde y_{n-1}=1/n+5n_{n-1}+5\epsilon_{n-1}
    = y_n+5\epsilon_{n-1} 
\]
so $\epsilon_n\geq 5\epsilon_{n-1}$. The error made by this
computation shows exponential growth.
</p>

<h3><a id="Linearsystemsolving">3.5.4</a> Linear system solving</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a> > <a href="arithmetic.html#Linearsystemsolving">Linear system solving</a>
</p>
<p name="switchToTextMode">

Sometimes we can make statements about the numerical precision of a
problem even without specifying what algorithm we use. Suppose we want
to solve a linear system, that is, we have an $n\times n$ matrix&nbsp;$A$
and a vector&nbsp;$b$ of size&nbsp;$n$, and we want to compute the vector&nbsp;$x$
such that $Ax=b$. (We will actually considering algorithms for this in
chapter&nbsp;
Numerical linear algebra
.) Since the vector&nbsp;$b$ will the result of some
computation or measurement, we are actually dealing with a
vector&nbsp;$\tilde b$, which is some perturbation of the ideal&nbsp;$b$:
\[
 \tilde b =  b+\Delta b. 
\]
The perturbation vector&nbsp;$\Delta b$ can be of the order of the machine
precision if it only arises from representation error, or it can be
larger, depending on the calculations that produced&nbsp;$\tilde b$.
</p>

<p name="switchToTextMode">
We now ask what the relation is between the exact value of&nbsp;$x$, which
we would have obtained from doing an exact calculation with $A$
and&nbsp;$b$, which is clearly impossible, and
the computed value&nbsp;$\tilde x$, which we get from computing with $A$
and&nbsp;$\tilde b$. (In this discussion we will assume that $A$ itself is
exact, but this is a simplification.)
</p>

<p name="switchToTextMode">
Writing $\tilde x= x+\Delta x$, the result of our computation is now
\[
 A\tilde x = \tilde b 
\]
 or 
\[
 A(x+\Delta x)=b+\Delta b. 
\]
Since $Ax=b$, we get $A\Delta x=\Delta b$. From this, we get
(see appendix&nbsp;
app:norms
 for details)
<!-- environment: equation start embedded generator -->
</p>
 \left \{
\begin{array}{rl}
  \Delta x&=A\inv \Delta b\\ Ax&=b
\end{array}
 \right\} \Rightarrow \left\{
\begin{array}{rl}
  \|A\| \|x\|&\geq\|b\| \\ \|\Delta x\|&\leq \|A\inv\| \|\Delta b\|
\end{array}
 \right.
\Rightarrow
\frac{\|\Delta x\|}{\|x\|}
\leq
\|A\| \|A\inv\| \frac{\|\Delta b\|}{\|b\|}
\label{eq:xbound}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
The quantity $\|A\| \|A\inv\|$ is called the 
  number} of a matrix. The bound \eqref{eq:xbound} then says that any
perturbation in the right hand side can lead to a perturbation in the
solution that is at most larger by the condition number of the
matrix&nbsp;$A$. Note that it does not say that the perturbation in&nbsp;$x$
<i>needs</i>
out, and in some cases it indeed happens that this bound is attained.
</p>

<p name="switchToTextMode">
Suppose that $b$ is exact up to machine precision, and the
condition number of&nbsp;$A$ is&nbsp;$10^4$. The bound \eqref{eq:xbound} is
often interpreted as saying that the last 4 digits of&nbsp;$x$ are
unreliable, or that the computation `loses 4 digits of accuracy'.
</p>

<p name="switchToTextMode">
Equation&nbsp;\eqref{eq:xbound} can also be interpreted as follows: when we
solve a linear system $Ax=b$ we get an approximate solution $x+\Delta
x$ which is the 
<i>exact</i>
 solution of a perturbed system
$A(x+\Delta x)=b+\Delta b$. The fact that the perturbation in the
solution can be related to the perturbation in the system, is
expressed by saying that the algorithm exhibits 
  stability}.
</p>

<p name="switchToTextMode">
The analysis of the accuracy of linear algebra algorithms is a field
of study in itself; see for instance the book by
Higham&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Higham:2002:ASN">[Higham:2002:ASN]</a>
.
</p>

<h3><a id="Roundofferrorinparallelcomputations">3.5.5</a> Roundoff error in parallel computations</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Examplesofround-offerror">Examples of round-off error</a> > <a href="arithmetic.html#Roundofferrorinparallelcomputations">Roundoff error in parallel computations</a>
</p>

<!-- index -->
<p name="switchToTextMode">

As we discussed in section&nbsp;
3.4.5
, and as you saw
in the above example of summing a series, addition in
computer arithmetic is not
<i>associative</i>
<!-- index -->
.
A&nbsp;similar fact holds for
multiplication.  This has an interesting consequence for parallel
computations: the way a computation is spread over parallel processors
influences the result.
</p>

<p name="switchToTextMode">
As a very simple example, consider computing the sum of four numbers $a+b+c+d$.
On a single processor, ordinary execution corresponds to the following associativity:
\[
 ((a+b)+c)+d. 
\]
On the other hand, spreading this computation over two processors,
where processor&nbsp;0 has $a,b$ and processor&nbsp;1 has&nbsp;$c,d$,
corresponds to
\[
 ((a+b)+(c+d)). 
\]
Generalizing this, we see that reduction operations will most likely
give a different result on different numbers of processors. (The MPI
standard declares that two program runs on the same set of processors
should give the same result.)
It is possible to circumvent this problem by replace a reduction operation
by a 
<i>gather</i>
 operation to all processors, which subsequently
do a local reduction. However, this increases the memory requirements for the
processors.
</p>

<p name="switchToTextMode">
There is an intriguing other solution to the parallel summing problem.
If we use a mantissa of 4000 bits to store a floating point number, we
do not need an exponent, and all calculations with numbers thus stored
are exact since they are a form of fixed-point
calculation&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Kulish:dotproduct,Kulisch:2011:VFE">[Kulish:dotproduct,Kulisch:2011:VFE]</a>
.  While doing a
whole application with such numbers would be very wasteful, reserving
this solution only for an occasional inner product calculation may be
the solution to the reproducibility problem.
</p>

<!-- index -->
<!-- index -->
<p name="switchToTextMode">

<h2><a id="Computerarithmeticinprogramminglanguages">3.6</a> Computer arithmetic in programming languages</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a>
</p>
</p>

<p name="switchToTextMode">
Different languages have different approaches to declaring integers and
floating point numbers. Here we study some of the issues.
</p>

<h3><a id="Fortran">3.6.1</a> Fortran</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Fortran">Fortran</a>
</p>
<p name="switchToTextMode">

In Fortran, variable declarations can take
various forms. For instance, it is possible for a
<i>type identifier</i>
<!-- index -->
to declare the number of
bytes that it takes to store a variable: 
<tt>INTEGER*2</tt>
,

<tt>REAL*8</tt>
. One advantage of this approach is the easy interoperability
with other languages, or the MPI library.
</p>

<p name="switchToTextMode">
Often it is possible to write a code using only 
<tt>INTEGER</tt>
,

<tt>REAL</tt>
, and use 
<i>compiler flags</i>
 to indicate the
size of an integer and real number in bytes.
</p>

<p name="switchToTextMode">
More sophisticated, modern versions of Fortran can indicate the number
of digits of precision a floating point number needs to have:
<!-- environment: verbatim start embedded generator -->
</p>
integer, parameter :: k9 = selected_real_kind(9)
real(kind=k9) :: r
r = 2._k9; print *, sqrt(r) ! prints 1.4142135623730
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
The `kind' values will usually be 4,8,16 but this is compiler
dependent.
</p>

<p name="switchToTextMode">

<b>C99 and Fortran2003</b><br>

Recent standards of the C and Fortran
languages incorporate the C/Fortran interoperability standard, which
can be used to declare a type in one language so that it is compatible
with a certain type in the other language.
</p>

<h3><a id="C">3.6.2</a> C</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C">C</a>
</p>
<p name="switchToTextMode">

<h4><a id="Bits">3.6.2.1</a> Bits</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C">C</a> > <a href="arithmetic.html#Bits">Bits</a>
</p>
</p>

<p name="switchToTextMode">
The C logical operators and their  bit variants are:
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  </td></tr>
<tr><td>
  </td><td>boolean</td><td>bitwise </td></tr>
<tr><td>
  and</td><td>
 <tt></td><td></td><td></tt>  </td><td> 
 <tt></td><td></tt>  </td></tr>
<tr><td>
  or </td><td>
 <tt>||</tt>  </td><td> 
 <tt>|</tt>  </td></tr>
<tr><td>
  not</td><td>
 <tt>!</tt>   </td><td>          </td></tr>
<tr><td>
  xor</td><td>          </td><td> 
 <tt>^</tt>  </td></tr>
<tr><td>
  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

The following 
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  </td></tr>
<tr><td>
  left  shift</td><td> 
 <tt></td><td>lt;</td><td>lt;</tt>  </td></tr>
<tr><td>
  right shift</td><td> 
 <tt></td><td>gt;</td><td>gt;</tt>  </td></tr>
<tr><td>
  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

You can do arithmetic with bit operations:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Left-shift is multiplication by&nbsp;2:\\
<!-- environment: lstlisting start embedded generator -->
</p>
i_times_2 = i&lt;&lt;1;
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<li>
Extract bits:
<!-- environment: lstlisting start embedded generator -->
</p>
i_mod_8 = i & 7
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
(How does that last one work?)
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Bit shift operations are normally applied to unsigned quantities.
  Are there extra complications when you use bitshifts to multiply or
  divide by&nbsp;2 in 2's-complement?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Integersandfloatingpointnumbers">3.6.2.2</a> Integers and floating point numbers</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C">C</a> > <a href="arithmetic.html#Integersandfloatingpointnumbers">Integers and floating point numbers</a>
</p>
</p>

<p name="switchToTextMode">
In C, the commonly used
<i>type identifiers</i>
<!-- index -->
do not correspond to a standard length. For integers there
is 
<tt>short int, int, long int</tt>
, and for floating point

<tt>float, double</tt>
. The 
<tt>sizeof()</tt>
 operator gives the
number of bytes used to store a datatype.
</p>

<p name="switchToTextMode">
The 
<i>numerical ranges</i>

<!-- index -->
of C&nbsp;integers are defined in 
<tt>limits.h</tt>
,
typically giving an upper or lower bound.
For instance, 
<tt>INT_MAX</tt>
 is defined to be 32767&nbsp;or greater.
</p>

<p name="switchToTextMode">
Floating point types are specified in 
<tt>float.h</tt>
.
</p>

<p name="switchToTextMode">
C&nbsp;integral types with specified storage exist: constants such as

<tt>int64_t</tt>
 are defined by 
<tt>typedef</tt>
 in 
<tt>stdint.h</tt>
.
</p>

<p name="switchToTextMode">
The constant 
<tt>NAN</tt>
 is declared in 
<tt>math.h</tt>
.
For checking whether a value is 
<tt>NaN</tt>
, use 
<tt>isnan()</tt>
.
</p>

<h4><a id="Printingbitpatterns">3.6.2.3</a> Printing bit patterns</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C">C</a> > <a href="arithmetic.html#Printingbitpatterns">Printing bit patterns</a>
</p>
<p name="switchToTextMode">

The following code fragment is useful for printing the bit pattern of numbers:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#cprintbits" aria-expanded="false" aria-controls="cprintbits">
        C Code: cprintbits
      </button>
    </h5>
  </div>
  <div id="cprintbits" class="collapse">
  <pre>
// printbits.c
void printBits(size_t const size, void const * const ptr)
{
  unsigned char *b = (unsigned char*) ptr;
  unsigned char byte;
  int i, j;

  for (i=size-1;i&gt;=0;i--)
    for (j=7;j&gt;=0;j--) {
      byte = (b[i] &gt;&gt; j) & 1;
      printf("%u", byte);
    }
}
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
Sample usage:
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#printbits5" aria-expanded="false" aria-controls="printbits5">
        C Code: printbits5
      </button>
    </h5>
  </div>
  <div id="printbits5" class="collapse">
  <pre>
// bits.c
  int five = 5;
  printf("Five=%d, in bits: ",five);
  printBits(sizeof(five),&five);
  printf("\n");
</pre>
</div>
</div>
<p name="switchToTextMode">

<h3><a id="C++">3.6.3</a> C++</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C++">C++</a>
</p>
</p>

<p name="switchToTextMode">
The C++ language has the following
<i>floating point types</i>
<!-- index -->
:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
<tt>float</tt>
: this is normally realized as an IEEE 754 32-bit
  floating point number.
<li>
<tt>double</tt>
: this defined to be at least as precise as a 
<tt>float</tt>
,
  and normally realized as an IEEE 754 64-bit floating point
  number.
<li>
<tt>long double</tt>
: this is again defined as being
  as least as precise as 
<tt>double</tt>
.
  On some architectures it can be 80-bit extended precision, on others
  full 128-bit precision.  Processors typically implement this latter
  with a combination of software and hardware functionality, so
  performance will be considerably lower than the previous two types.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Limits">3.6.3.1</a> Limits</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C++">C++</a> > <a href="arithmetic.html#Limits">Limits</a>
</p>
</p>

<p name="switchToTextMode">
You can still use the C&nbsp;header 
<tt>limits.h</tt>
 or
<tt>climits</tt>
, but it's better to use
<tt>std::numeric_limits</tt>
, which is templated over the types.
For instance
<!-- environment: lstlisting start embedded generator -->
</p>
std::numerical_limits&lt;int&gt;.max();
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

The following functions are available:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
 <tt>std::numeric_limits&lt;T&gt;::max()</tt>  for the largest number.
<li>
 <tt>std::numeric_limits&lt;T&gt;::min()</tt>  for the smallest normalized positive number.
<li>
 <tt>std::numeric_limits&lt;T&gt;::lowest()</tt>  for the most negative number.
<li>
 <tt>std::numeric_limits&lt;T&gt;::epsilon()</tt>  for machine epsilon.
<li>
 <tt>std::numeric_limits&lt;T&gt;::denorm_min()</tt>  for smallest subnormal.
  (See also  <tt>std::numeric_limits&lt;T&gt;::has_denorm</tt> .)
<li>
 <tt>std::nextafter(x,y)</tt> 
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Exceptions">3.6.3.2</a> Exceptions</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C++">C++</a> > <a href="arithmetic.html#Exceptions">Exceptions</a>
</p>
</p>

<p name="switchToTextMode">
Exceptions defined:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
 pole error occurred in an earlier
  floating-point operation.
<li>
 inexact result: rounding was
  necessary to store the result of an earlier floating-point
  operation.
<li>
 domain error occurred in an earlier
  floating-point operation.
<li>
 the result of the earlier
  floating-point operation was too large to be representable.
<li>
 the result of the earlier
  floating-point operation was subnormal with a loss of precision.
<li>
 bitwise OR of all supported
  floating-point exceptions .
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Usage:
</p>

<!-- environment: lstlisting start embedded generator -->
std::feclearexcept(FE_ALL_EXCEPT);
if(std::fetestexcept(FE_UNDERFLOW)) { /* ... */ }
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

In&nbsp;C++, 
<tt>std::numeric_limits&lt;double&gt;::quiet_NaN()</tt>
 is declared in

<tt>limits</tt>
, which is meaningful if

<tt>std::numeric_limits::has_quiet_NaN</tt>
 is true, which is the case if

<tt>std::numeric_limits::is_iec559</tt>
 is true. (ICE&nbsp;559 is essentially
IEEE&nbsp;754; see section&nbsp;
3.3.7
.)
</p>

<p name="switchToTextMode">
The same module also has 
<tt>infinity()</tt>
 and 
<tt>signaling_NaN()</tt>
.
</p>

<p name="switchToTextMode">
For checking whether a value is 
<tt>NaN</tt>
, use 
<tt>std::isnan()</tt>
 from 
<tt>cmath</tt>
 in&nbsp;C++.
</p>

<p name="switchToTextMode">
See further 
<a href=http://en.cppreference.com/w/cpp/numeric/math/nan>http://en.cppreference.com/w/cpp/numeric/math/nan</a>
.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="Exceptions">3.6.3.3</a> Exceptions</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C++">C++</a> > <a href="arithmetic.html#Exceptions">Exceptions</a>
</p>
</p>

<p name="switchToTextMode">
Both the IEEE 754 standard and the C++ language define a concept
<i>exception</i>
<!-- index -->
 which differ from each
other. The 754 exception means the occurence of an operation that has `no outcome
suitable for every reasonable application'. This does not necessarily
translate to the language-defined exception.
</p>

<h4><a id="Printingbitpatterns">3.6.3.4</a> Printing bit patterns</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#C++">C++</a> > <a href="arithmetic.html#Printingbitpatterns">Printing bit patterns</a>
</p>
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#cppbitprint" aria-expanded="false" aria-controls="cppbitprint">
        C++ Code: cppbitprint
      </button>
    </h5>
  </div>
  <div id="cppbitprint" class="collapse">
  <pre>
// bitprint.cxx
void format(const std::string  &s)
{
  // sign bit
  std::cout &lt;&lt; s.substr(0,1)  &lt;&lt; ' ';
  // exponent
  std::cout &lt;&lt; s.substr(1,8);
  // mantissa in groups of 4
  for(int walk=9;walk&lt;32;walk+=4)
    std::cout &lt;&lt; ' ' &lt;&lt; s.substr(walk,4);
  // newline
  std::cout &lt;&lt; "\n";
}
  uint32_t    u;
  std::memcpy(&u,&d,sizeof(u));
  std::bitset&lt;32&gt; b{u};
  std::stringstream s;
  s &lt;&lt; std::hexfloat &lt;&lt; b &lt;&lt; '\n';
  format(s.str());
  //codesnippet cppbitprint

  return 0;
}
</pre>
</div>
</div>
</p>

<h3><a id="Compilerflagsforfastmath">3.6.4</a> Compiler flags for fast math</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Compilerflagsforfastmath">Compiler flags for fast math</a>
</p>
<p name="switchToTextMode">

Various compilers have an option for
<i>fast math</i>
<!-- index -->
optimizations.
</p>

<!-- environment: itemize start embedded generator -->
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
GCC and Clang: 
<tt>-ffast-math</tt>

<li>
Intel: 
<tt>-fp-model=fast</tt>
 (default)
<li>
MSVC: 
<tt>/fp:fast</tt>

</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

This typically covers the following cases:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Finite math: assume that 
<tt>Inf</tt>
 and 
<tt>Nan</tt>
 don't occur.
  This means that the test 
<tt>x==x</tt>
 is always true.
<li>
Associative math: this allows rearranging the order of operations
  in an arithmetic expression.
  This is known as 
<i>re-association</i>
, and is for instance
  beneficial for vectorization.
  However, as you saw in  section&nbsp;
3.4.5
,
  this can change the result of a computation.
  Also, it makes 
<i>compensated summation</i>
 impossible;
  section&nbsp;
3.7.1
.
<li>
Flushing subnormals to zero.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Extensive discussion: 
<a href=https://simonbyrne.github.io/notes/fastmath/>https://simonbyrne.github.io/notes/fastmath/</a>

</p>

<h3><a id="Round-offbehaviorinprogramming">3.6.5</a> Round-off behavior in programming</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Round-offbehaviorinprogramming">Round-off behavior in programming</a>
</p>

<!-- index -->
<p name="switchToTextMode">

From the above discussion it should be clear that some simple statements
that hold for mathematical real numbers do not hold for floating-point numbers.
For instance, in floating-point arithmetic
\[
 (a+b)+c\not=a+(b+c).
\]
This implies that a compiler can not perform certain optimizations
without it having an effect on round-off behavior
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {This
section borrows from documents by
Microsoft 
<a href=http://msdn.microsoft.com/en-us/library/aa289157(vs.71).aspx>http://msdn.microsoft.com/en-us/library/aa289157(vs.71).aspx</a>

and
Intel 
<a href=http://software.intel.com/sites/default/files/article/164389/fp-consistency-122712_1.pdf>http://software.intel.com/sites/default/files/article/164389/fp-consistency-122712_1.pdf</a>
;
for detailed discussion the reader is referred to these.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
.  In some
codes such slight differences can be tolerated, for instance because
the method has built-in safeguards. For instance, the stationary
iterative methods of section&nbsp;
5.5
 damp out any error
that is introduced.
</p>

<p name="switchToTextMode">
On the other hand, if the programmer has written code to account for
round-off behavior, the compiler has no such liberties. This was
hinted at in exercise&nbsp;
3.3.6
 above.
We use the concept of 
<i>value safety</i>
 to describe
how a compiler is allowed to change the interpretation of a computation.
At its strictest, the compiler is not allowed
to make any changes that affect the result of a
computation.
</p>

<p name="switchToTextMode">
Compilers typically have an option controlling whether optimizations
are allowed that may change the numerical behavior. For the Intel
compiler that is 
<tt>-fp-model=...</tt>
. On the other hand, options
such as 
<tt>-Ofast</tt>
 are aimed at performance improvement only, and
may affect numerical behavior severely.  For the Gnu compiler full
754 compliance takes the option 
<tt>-frounding-math</tt>
 whereas

<tt>-ffast-math</tt>
 allows for performance-oriented compiler
transformations that violate 754 and/or the language standard.
</p>

<p name="switchToTextMode">
These matters are also of importance if you care about
<i>reproducibility</i>
 of results. If a code is compiled with
two different compilers, should runs with the same input give
the same output? If a code is run in parallel on two different
processor configurations? These questions are very subtle.
In the first case, people sometimes insist on
<i>bitwise reproducibility</i>
, whereas
in the second case some differences are allowed, as long
as the result stays `scientifically' equivalent. Of course,
that concept is hard to make rigorous.
</p>

<p name="switchToTextMode">
Here are some issues that are relevant
when considering the influence of the compiler
on code behavior and reproducibility.
</p>

<p name="switchToTextMode">

<b>Re-association</b><br>

<!-- index -->
Foremost among changes that a compiler can
make to a computation is 
<i>re-association</i>
, the technical
term for grouping $a+b+c$ as $a+(b+c)$. The 
  standard} and the 
<i>C++ language standard</i>
 prescribe
strict left-to-right evaluation of expressions without parentheses, so
re-association is in fact not allowed by the standard. The
<i>Fortran language standard</i>
 has no such prescription,
but there the compiler has to respect the evaluation order that is
implied by parentheses.
</p>

<p name="switchToTextMode">
A common source of re-association is 
<i>loop unrolling</i>
; see
section&nbsp;
1.7.2
. Under strict value safety,
a compiler is limited in how it can unroll a loop, which has implications
for performance. The amount of loop unrolling,
and whether it's performed at all, depends on the compiler optimization level,
the choice of compiler, and the target platform.
</p>

<p name="switchToTextMode">
A more subtle source of re-association is parallel execution;
see section&nbsp;
3.5.5
. This implies that the output
of a code need not be strictly reproducible between two runs on
different parallel configurations.
</p>

<!-- index -->
<p name="switchToTextMode">


<b>Constant expressions</b><br>

It is a common compiler optimization to compute constant expressions
during compile time. For instance, in
<!-- environment: verbatim start embedded generator -->
</p>
float one = 1.;
...
x = 2. + y + one;
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
the compiler change the assignment to 
<tt>x = y+3.</tt>
. However, this
violates the re-association rule above, and it ignores any dynamically
set rounding behavior.
</p>

<p name="switchToTextMode">

<b>Expression evaluation</b><br>

In evaluating the expression $a+(b+c)$, a processor will generate an
intermediate result for $b+c$ which is not assigned to any variable.
Many processors are able to assign a higher
<i>precision of the intermediate result</i>
.
A&nbsp;compiler can have a flag to dictate whether to use this facility.
</p>

<p name="switchToTextMode">

<b>Behavior of the floating point unit</b><br>

Rounding behavior (truncate versus round-to-nearest) and treatment of
<i>gradual underflow</i>
 may be controlled by library
functions or compiler options.
</p>

<p name="switchToTextMode">

<b>Library functions</b><br>

The IEEE 754 standard only prescribes simple operations;
there is as yet no standard that treats sine or log functions.
Therefore, their implementation may be a source of variability.
</p>

<p name="switchToTextMode">
For more discussion, see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Lionel:reproducibility">[Lionel:reproducibility]</a>
.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Changingroundingbehavior">3.6.6</a> Changing rounding behavior</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Changingroundingbehavior">Changing rounding behavior</a>
</p>

</p>

<p name="switchToTextMode">
The 
<i>IEEE 754</i>

<!-- index -->
standard also declares that a processor should be
able to switch its rounding behavior between ordinary rounding,
rounding up or down (sometimes phrased as `towards plus infinity'
and `towards minus infinity' respectively) or truncation.
In&nbsp;C99, the API for this is contained in 
<tt>fenv.h</tt>
 (or for
C++ 
<tt>cfenv</tt>
):
<!-- environment: verbatim start embedded generator -->
</p>
#include &lt;fenv.h&gt;


int roundings[] =
  {FE_TONEAREST, FE_UPWARD, FE_DOWNWARD, FE_TOWARDZERO};
rchoice = ....
int status = fesetround(roundings[rchoice]);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
In Fortran2003 the function 
<tt>IEEE_SET_ROUNDING_MODE</tt>

is available in the 
<tt>IEEE_ARITHMETIC</tt>
 module.
</p>

<p name="switchToTextMode">
Setting the rounding behavior can serve as a quick test for the stability
of an algorithm: if the result changes appreciably between two different
rounding strategies, the algorithm is likely not stable.
</p>

<h3><a id="Catchingexceptions">3.6.7</a> Catching exceptions</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Catchingexceptions">Catching exceptions</a>
</p>

<p name="switchToTextMode">

The word `exception' has several meanings:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Floating point exceptions are the occurrence of `invalid numbers',
  such as through overflow or divide-by-zero (see section&nbsp;
3.3.8.1
);
<li>
Programming languages can `throw an exception', that is, interrupt regular
  program control flow, if any type of unexpected event occurs.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

The behavior on 
<i>overflow</i>
 can also be set to generate an
<i>exception</i>
.
In&nbsp;C, you specify this with a library call:
<!-- environment: verbatim start embedded generator -->
</p>
#include &lt;fenv.h&gt;
int main() {
 ...
 feenableexcept(FE_DIVBYZERO | FE_INVALID | FE_OVERFLOW);
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Compiler-specificbehavior">3.6.7.1</a> Compiler-specific behavior</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Computerarithmeticinprogramminglanguages">Computer arithmetic in programming languages</a> > <a href="arithmetic.html#Catchingexceptions">Catching exceptions</a> > <a href="arithmetic.html#Compiler-specificbehavior">Compiler-specific behavior</a>
</p>
</p>

<p name="switchToTextMode">
Trapping exceptions can sometimes be specified by the compiler.
For instance, the 
<i>gcc</i>
 compiler can
<i>trap</i>
<!-- index -->
exceptions by the flag

<tt>-ffpe-trap=list</tt>
;
see 
<a href=https://gcc.gnu.org/onlinedocs/gfortran/Debugging-Options.html>https://gcc.gnu.org/onlinedocs/gfortran/Debugging-Options.html</a>
.
</p>

<h2><a id="Moreaboutfloatingpointarithmetic">3.7</a> More about floating point arithmetic</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a>
</p>
<p name="switchToTextMode">

<h3><a id="Kahansummation">3.7.1</a> Kahan summation</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Kahansummation">Kahan summation</a>
</p>

</p>

<p name="switchToTextMode">
The example in section&nbsp;
3.4.5
 made visible some of
the problems of computer arithmetic: rounding can cause results that
are quite wrong, and very much dependent on evaluation order. A&nbsp;number
of algorithms exist that try to compensate for these problems, in
particular in the case of addition. We briefly discuss
<i>Kahan summation</i>
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Kahan:1965:summation">[Kahan:1965:summation]</a>
, named after
<i>William Kahan</i>
, which is one example of a
<i>compensated summation</i>
 algorithm.
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>$sum\leftarrow0$\;  $correction\leftarrow0$\;  \While{there is another input}{    $oldsum\leftarrowsum$\;    $input\leftarrowinput-correction$\;    $sum\leftarrowoldsum+input$\;    $correction\leftarrow(sum-oldsum)-input$\</p>
}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Go through the example in section&nbsp;
3.4.5
, adding a
  final term&nbsp;3; that is compute $4+6+7+3$ and $6+7+4+3$ under the
  conditions of that example.
  Show that the correction is precisely the $3$&nbsp;undershoot when 17 is rounded
  to&nbsp;20, or the $4$&nbsp;overshoot when 14 is rounded to&nbsp;10; in both cases
  the correct result of&nbsp;20 is computed.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Othercomputerarithmeticsystems">3.7.2</a> Other computer arithmetic systems</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Othercomputerarithmeticsystems">Other computer arithmetic systems</a>
</p>
</p>

<p name="switchToTextMode">
Other systems have been proposed to dealing with the problems of
inexact arithmetic on computers. One solution is
<i>extended precision</i>
arithmetic, where numbers are stored in more bits than usual. A common
use of this is in the calculation of inner products of vectors: the
accumulation is internally performed in extended precision, but
returned as a regular floating point number. Alternatively, there are
libraries such as GMPlib&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#gmplib">[gmplib]</a>
 that allow for any calculation
to be performed in higher precision.
</p>

<p name="switchToTextMode">
Another solution to the imprecisions of computer arithmetic is `interval
arithmetic'&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#wikipedia:interval-arithmetic">[wikipedia:interval-arithmetic]</a>
, where for each
calculation interval bounds are maintained. While this has been
researched for considerable time, it is not practically used other
than through specialized libraries&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#boost:interval-arithmetic">[boost:interval-arithmetic]</a>
.
</p>

<h3><a id="Extendedprecision">3.7.3</a> Extended precision</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Extendedprecision">Extended precision</a>
</p>

<p name="switchToTextMode">

When the IEEE 754 standard was drawn up, it was envisioned that
processors could have a whole range of precisions. In practice,
only single and double precision as defined have been used.
However, one instance of 
<i>extended precision</i>
 still survives:
Intl processors have 80-bit registers for storing intermediate results.
(This goes back to the 
<i>Intel 80287 co-processor</i>
.)
This strategy makes sense in 
<i>FMA</i>
 instructions,
and in the accumulation of inner products.
</p>

<p name="switchToTextMode">
These 80-bit registers have a strange structure
with an 
<i>significand integer</i>
 bit
that can give rise to bit patterns that are not
a valid representation of any defined number&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#mostlyharmless80bit">[mostlyharmless80bit]</a>
.
</p>

<h3><a id="Reducedprecision">3.7.4</a> Reduced precision</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Reducedprecision">Reduced precision</a>
</p>
<p name="switchToTextMode">

You can ask `does double precision always give benefits over single
precision' and the answer is not always `yes' but rather: `it
depends'.
</p>

<h4><a id="Lowerprecisioniniterativerefinement">3.7.4.1</a> Lower precision in iterative refinement</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Reducedprecision">Reduced precision</a> > <a href="arithmetic.html#Lowerprecisioniniterativerefinement">Lower precision in iterative refinement</a>
</p>
<p name="switchToTextMode">

In iterative linear system solving (section&nbsp;
5.5
, the
accuracy is determined by how precise the residual is calculated, not
how precise the solution step is done. Therefore, one could do
operations such as applying the preconditioner
(section&nbsp;
5.5.6
) in reduced
precision&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Dongarra:mixed-refinement">[Dongarra:mixed-refinement]</a>
. This is a form of
<i>iterative refinement</i>
; see
section&nbsp;
5.5.6
.
</p>

<h4><a id="LowerprecisioninDeepLearning">3.7.4.2</a> Lower precision in Deep Learning</h4>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Reducedprecision">Reduced precision</a> > <a href="arithmetic.html#LowerprecisioninDeepLearning">Lower precision in Deep Learning</a>
</p>

<p name="switchToTextMode">

IEEE 754-2008 has a definition for the 
<i>binary16</i>
 half
precision format, which has a 5-bit exponent and 11-bit mantissa.
</p>

<p name="switchToTextMode">
In 
<i>DL</i>
 it is more important to express the range of values
than to be precise about the exact value. (This is the opposite of
traditional scientific applications, where close values need to be
resolved.)  This has led to the definition of the
<i>bfloat16</i>
 `brain float' format

<a href=https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>https://en.wikipedia.org/wiki/Bfloat16_floating-point_format</a>

which is a 16-bit
floating point format. It uses 8&nbsp;bits for the exponent and 7&nbsp;bits for
the mantissa. This means that it shares the same exponent range as the
IEEE single precision format; see figure&nbsp;
3.4
.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bfloat16def.png" width=800></img>
<p name="switchToTextMode">
  \caption{Comparison of fp32, fp16, and bfloat16
    formats. (Illustration from 
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Intel:bfloat16">[Intel:bfloat16]</a>
)}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Since bfloat16 and fp32 have the same structure in the first two bytes,
  a bfloat16 number can be derived from an fp32 number by
  truncating the third and fourth byte. However, rounding may give
  better results in practice.
<li>
Conversely, casting a bloat16 to fp32 only requires filling the
  final two bytes with zeros.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

The limited precision of bfloat16 is probably enough to represent
quantities in 
<i>DL</i>
 applications, but in order not to lose
further precision it is envisioned that 
<span title="acronym" ><i>FMA</i></span>
 hardware uses 32-bit
numbers internally: the product of two bfloat16 number is a regular
32-bit number. In order to compute inner products (which happens as
part of matrix-matrix multiplication in 
<i>DL</i>
), we then need an
<span title="acronym" ><i>FMA</i></span>
 unit as in figure&nbsp;
3.5
.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bfloat16fma.png" width=800></img>
<p name="switchToTextMode">
  \caption{An FMA unit taking two bloat16 and one fp32
    number. (illustration from 
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Intel:bfloat16">[Intel:bfloat16]</a>
)}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The
  
<i>Intel Knights Mill</i>

<!-- index -->
, based
  on the 
<i>Intel Knights Landing</i>

<!-- index -->
,
  has support for reduced precision.
<li>
The 
<i>Intel Cooper Lake</i>
 implements the
<i>bfloat16</i>
 format&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Intel:bfloat16">[Intel:bfloat16]</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Even further reduction to 8-bit was discussed in&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#dettmers:8bit">[dettmers:8bit]</a>
.
</p>

<h3><a id="Fixed-pointarithmetic">3.7.5</a> Fixed-point arithmetic</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Fixed-pointarithmetic">Fixed-point arithmetic</a>
</p>
<p name="switchToTextMode">

A fixed-point number (for a more thorough discussion than found here,
see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#YatesFixedPoint">[YatesFixedPoint]</a>
) can be represented as $\fxp NF$ where
$N\geq\beta^0$ is the integer part and $F&lt;1$ is the fractional
part. Another way of looking at this, is that a fixed-point number is
an integer stored in $N+F$ digits, with an implied decimal point after
the first $N$ digits.
</p>

<p name="switchToTextMode">
Fixed-point calculations can overflow, with no possibility to adjust
an exponent. Consider the multiplication $\fxp{N_1}{F_1}\times
\fxp{N_2}{F_2}$, where $N_1\geq \beta^{n_1}$ and $N_2\geq
\beta^{n_2}$. This overflows if $n_1+n_2$ is more than the number of
positions available for the integer part. (Informally, the number of
digits of the product is the sum of the number of digits of the operands.)
This means that, in a program
that uses fixed-point, numbers will need to have a number of
leading zero digits, if you are ever going to multiply them,
which lowers the numerical accuracy.
It also means that the programmer has to think harder about
calculations, arranging them in such a way that overflow will not
occur, and that numerical accuracy is still preserved to a reasonable
extent.
</p>

<p name="switchToTextMode">
So why would people use fixed-point numbers? One important application
is in embedded low-power devices, think a battery-powered digital
thermometer. Since fixed-point calculations are essentially identical
to integer calculations, they do not require a floating-point unit,
thereby lowering chip size and lessening power demands. Also, many
early video game systems had a processor that either had no
floating-point unit, or where the integer unit was considerably faster
than the floating-point unit. In both cases, implementing non-integer
calculations as fixed-point, using the integer unit, was the key to
high throughput.
</p>

<p name="switchToTextMode">
Another area where fixed point arithmetic is still used is in signal
processing. In modern CPUs, integer and floating point operations
are of essentially the same speed, but converting between them is
relatively slow. Now, if the sine function is implemented through
table lookup, this means that in $\sin(\sin x)$
the output of a function is used to index the next function
application. Obviously, outputting the sine function in fixed point
obviates the need for conversion between real and integer quantities,
which simplifies the chip logic needed, and speeds up calculations.
</p>

<h3><a id="Complexnumbers">3.7.6</a> Complex numbers</h3>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Moreaboutfloatingpointarithmetic">More about floating point arithmetic</a> > <a href="arithmetic.html#Complexnumbers">Complex numbers</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Some programming languages have 
<i>complex numbers</i>
 as a built-in data type,
others not, and others are in between. For instance, in Fortran you
can declare
<!-- environment: verbatim start embedded generator -->
</p>
COMPLEX z1,z2, z(32)
COMPLEX*16 zz1, zz2, zz(36)
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
A complex number is a pair of real numbers, the real and imaginary
part, allocated adjacent in memory. The first declaration then uses
8&nbsp;bytes to store to 
<tt>REAL*4</tt>
 numbers, the second one has 
<tt>REAL*8</tt>
s
for the real and imaginary part.  (Alternatively, use \n{DOUBLE
  COMPLEX} or in Fortran90 
<tt>COMPLEX(KIND=2)</tt>
 for the second line.)
</p>

<p name="switchToTextMode">
By contrast, the 
<tt>C</tt>
 language does not directly have complex
numbers, but both 
<tt>C99</tt>
 and 
<tt>C++</tt>
 have a 
<tt>complex.h</tt>
 header
file
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {These two header files are not identical, and in fact
  not compatible. Beware, if you compile C code with a C++
  compiler&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#DobbsComplex">[DobbsComplex]</a>
.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
. This defines as complex number as in
Fortran, as two real numbers.
</p>

<p name="switchToTextMode">
Storing a complex number like this is easy, but sometimes it is
computationally not the best solution. This becomes apparent when we
look at arrays of complex numbers.
If a computation often relies
on access to the real (or imaginary) parts of complex numbers
exclusively, striding through an array of complex numbers, has a
stride two, which is disadvantageous (see
section&nbsp;
1.3.4.7
). In this case, it is better to allocate one
array for the real parts, and another for the imaginary parts.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Suppose arrays of complex numbers are stored the Fortran
  way. Analyze the memory access pattern of pairwise multiplying the
  arrays, that is, $\forall_i\colon c_i\leftarrow a_i\cdot b_i$, where
  
<tt>a(), b(), c()</tt>
 are arrays of complex numbers.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that an $n\times n$ linear system $Ax=b$ over the complex numbers
  can be written as a $2n\times 2n$ system over the real
  numbers. Hint: split the matrix and the vectors in their real and
  imaginary parts. Argue for the efficiency of storing arrays of
  complex numbers as separate arrays for the real and imaginary parts.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h2><a id="Conclusions">3.8</a> Conclusions</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Conclusions">Conclusions</a>
</p>
<p name="switchToTextMode">

Computations done on a computer are invariably beset with numerical error.
In a way, the reason for the error is the imperfection of computer
arithmetic: if we could calculate with actual real numbers there would
be no problem. (There would still be the matter of measurement error
in data, and approximations made in numerical methods; see the next
chapter.) However, if we accept roundoff as a fact of life, then
various observations hold:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Mathematically equivalent operations need not behave identically
  from a point of stability; see the `abc-formula' example.
<li>
Even rearrangements of the same computations do not behave
  identically; see the summing example.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Thus it
becomes imperative to analyze computer algorithms with regard to their
roundoff behavior: does roundoff increase as a slowly growing
function of problem parameters, such as the number of terms evalauted,
or is worse behavior possible? We will not address such questions in
further detail in this book.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Reviewquestions">3.9</a> Review questions</h2>
<p name=crumbs>
crumb trail:  > <a href="arithmetic.html">arithmetic</a> > <a href="arithmetic.html#Reviewquestions">Review questions</a>
</p>
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  True or false?
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
For integer types, the `most negative' integer is the negative
    of the `most positive' integer.
<li>
For floating point types, the `most negative' number is the negative
    of the `most positive' one.
<li>
For floating point types, the smallest positive number is the reciprocal
    of the largest positive number.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

</div>
<a href="index.html">Back to Table of Contents</a>
