<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Linear algebra</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


13.1 : <a href="norms.html#Norms">Norms</a><br>
13.1.1 : <a href="norms.html#Vectornorms">Vector norms</a><br>
13.1.2 : <a href="norms.html#Matrixnorms">Matrix norms</a><br>
13.2 : <a href="norms.html#Gram-Schmidtorthogonalization">Gram-Schmidt orthogonalization</a><br>
13.3 : <a href="norms.html#Thepowermethod">The power method</a><br>
13.4 : <a href="norms.html#Nonnegativematrices;Perronvectors">Nonnegative matrices; Perron vectors</a><br>
13.5 : <a href="norms.html#TheGershgorintheorem">The Gershgorin theorem</a><br>
13.6 : <a href="norms.html#Householderreflectors">Householder reflectors</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>13 Linear algebra</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In this course it is assumed that you know what a matrix and a vector
are, simple algorithms such as how to multiply them, and some
properties such as invertibility of a matrix. This appendix introduces
some concepts and theorems that are not typically part of a first
course in linear algebra.
</p>

<h2><a id="Norms">13.1</a> Norms</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Norms">Norms</a>
</p>
<p name="switchToTextMode">

A 
<i>norm</i>
 is a way to generalize the concept of absolute
value to multi-dimensional objects such as vectors and matrices. There
are many ways of defining a norm, and there is theory of how different
norms relate. Here we only give the basic definitions; for more detail
see any linear algebra textbook, for instance~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#golo83">[golo83]</a>
.
</p>

<h3><a id="Vectornorms">13.1.1</a> Vector norms</h3>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Norms">Norms</a> > <a href="norms.html#Vectornorms">Vector norms</a>
</p>
<!-- index -->
<p name="switchToTextMode">

A norm is any function $n(\cdot)$ on a vector space $V$ with the
following properties:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$n(x)\geq0$ for all $x\in V$ and $n(x)=0$ only for $x=0$,
<li>
$n(\lambda x)=|\lambda|n(x)$ for all $x\in V$ and
  $\lambda\in\bbR$.
<li>
$n(x+y)\leq n(x)+n(y)$
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For any $p\geq1$, the following defines a vector norm:
\[
 |x|_p = \sqrt{\sum_i|x_i|^p}. 
\]
Common norms are $\|\cdot\|_1$ (`sum of absolute values') and
$\|\cdot\|_2$ (`square root of sum of squares'); the
$\|\cdot\|_\infty$ norm is defined as
$\lim_{p\rightarrow\infty}\|\cdot\|_p$, and it is not hard to see that
this equals
\[
 \|x\|_\infty=\max_i |x_i|. 
\]
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Matrixnorms">13.1.2</a> Matrix norms</h3>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Norms">Norms</a> > <a href="norms.html#Matrixnorms">Matrix norms</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
By considering a matrix of size~$n$ as a vector of length $n^2$, we
can define the Frobenius matrix norm:
\[
 \|A\|_F=\sqrt{\sum_{i,j}|a_{ij}|^2}. 
\]
However, we will mostly look at 
<i>associated matrix   norms</i>

<!-- index -->
:
\[
 \|A\|_p=\sup_{\|x\|_p=1}\|Ax\|_p=
   \sup_x\frac{\|Ax\|_p}{\|x\|_p}.
\]
From their definition, it then follows that
\[
 \|Ax\|\leq\|A\|\|x\| 
\]
for associated norms.
</p>

<p name="switchToTextMode">
The following are easy to derive:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$\|A\|_1=\max_j\sum_i|a_{ij}|$,
<li>
$\|A\|_\infty=\max_i\sum_j|a_{ij}|$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
By observing that $\|A\|_2=\sup_{\|x\|_2=1} x^tA^tAx$, it is not hard
to derive that $\|A\|_2$ is the maximal singular value of~$A$, which
is the root of the maximal eigenvalue of~$A^tA$.
</p>

<p name="switchToTextMode">
The matrix 
<i>condition number</i>
 is defined as
\[
 \kappa(A)=\|A\|\,\|A\inv\|. 
\]
In the symmetric case, and using the 2-norm, this is the ratio between the largest and
smallest eigenvalue.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Gram-Schmidtorthogonalization">13.2</a> Gram-Schmidt orthogonalization</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Gram-Schmidtorthogonalization">Gram-Schmidt orthogonalization</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
The 
<span title="acronym" ><i>GS</i></span>
 algorithm takes a series of vectors and inductively
orthogonalizes them. This can be used to turn an arbitrary basis of a
vector space into an orthogonal basis; it can also be viewed as
transforming a matrix $A$ into one with orthogonal columns. If $Q$ has
orthogonal columns, $Q^tQ$ is diagonal, which is often a convenient
property to have.
</p>

<p name="switchToTextMode">
The basic principle of the 
<span title="acronym" ><i>GS</i></span>
 algorithm
can be demonstrated with two
vectors~$u,v$. Suppose we want a vector $v'$ so that $u,v$ and $u,v'$
span the same space, but $v'\perp u$. For this we let
\[
 v'\leftarrow v-\frac{u^tv}{u^tu}u. 
\]
It is easy to see that this satisfies the requirements.
</p>

<p name="switchToTextMode">
Suppose we have an set of vectors
$u_1,\ldots,u_n$ that we want to orthogonalize. We do this by
successive applications of the above transformation:
</p>

<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    For $i=1,\ldots,n$:<br>
&nbsp;         For $j=1,\ldots i-1$:<br>
&nbsp;&nbsp;        let $c_{ji}=u_j^tu_i/u_j^tu_j$<br>
&nbsp;         For $i=1,\ldots,n$:<br>
&nbsp;&nbsp;         update $u_i\leftarrow u_i-u_jc_{ji}$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">

Often the vector $v$ in the algorithm above is normalized; this adds a line
<!-- environment: quote start embedded generator -->
</p>
<!-- TranslatingLineGenerator quote ['quote'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    $u_i\leftarrow u_i/\|u_i\|$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
to the algorithm.
<span title="acronym" ><i>GS</i></span>
 orthogonalization with this normalization, applied to a
matrix, is also known as the 
<i>QR factorization</i>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Suppose that we apply the 
<span title="acronym" ><i>GS</i></span>
 algorithm to the columns of a
  rectangular matrix~$A$, giving a matrix~$Q$. Prove that there is an
  upper triangular matrix~$R$ such that $A=QR$. (Hint: look at the
  $c_{ji}$ coefficients above.) If we normalize the
  orthogonal vector in the algorithm above, $Q$~has the additional property
  that $Q^tQ=I$. Prove this too.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

The 
<span title="acronym" ><i>GS</i></span>
 algorithm as given above computes the desired result,
but only in exact arithmetic. A~computer implementation can be quite
inaccurate if the angle between $v$ and one of the $u_i$ is small. In
that case, the 
<span title="acronym" ><i>MGS</i></span>
<!-- index -->
 algorithm will
perform better:
</p>

<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    For $i=1,\ldots,n$:<br>
&nbsp;         For $j=1,\ldots i-1$:<br>
&nbsp;&nbsp;        let $c_{ji}=u_j^tu_i/u_j^tu_j$<br>
&nbsp;&nbsp;         update $u_i\leftarrow u_i-u_jc_{ji}$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">

To contrast it with 
<span title="acronym" ><i>MGS</i></span>
, the original 
<span title="acronym" ><i>GS</i></span>
 algorithm is also
known as 
<span title="acronym" ><i>CGS</i></span>
.
</p>

<p name="switchToTextMode">
As an illustration of the difference between the two methods, consider
the matrix
\[
 A=
\begin{pmatrix}
  1&1&1\\ \epsilon&0&0\\ 0&\epsilon&0\\ 0&0&\epsilon
\end{pmatrix}
\]
where $\epsilon$ is of the order of the machine precision, so that
$1+\epsilon^2=\nobreak1$ in machine arithmetic.
The 
<span title="acronym" ><i>CGS</i></span>
 method proceeds as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The first column is of length~1 in machine arithmetic, so
\[
 q_1 = a_1 =
\begin{pmatrix}
    1\\ \epsilon\\ 0\\ 0
\end{pmatrix}
  . 
\]
<li>
The second column gets orthogonalized as $v\leftarrow a_2-1\cdot
  q_1$, giving
\[
 v=
\begin{pmatrix}
    0\\ -\epsilon\\ \epsilon\\ 0
\end{pmatrix}
,
  \quad\hbox{normalized:}\quad
  q_2 =
\begin{pmatrix}
    0\\ -\frac{\sqrt 2}2\\ \frac{\sqrt2}2\\ 0
\end{pmatrix}
\]
<li>
The third column gets orthogonalized as $v\leftarrow
  a_3-c_1q_1-c_2q_2$, where
\[
\begin{cases}
    c_1 = q_1^ta_3 = 1\\ c_2 = q_2^ta_3=0
\end{cases}
  \Rightarrow v=
\begin{pmatrix}
    0\\ -\epsilon\\ 0\\ \epsilon
\end{pmatrix}
;\quad \hbox{normalized:}\quad q_3=
\begin{pmatrix}
    0\\ \frac{\sqrt2}2\\ 0\\ \frac{\sqrt2}2
\end{pmatrix}
\]
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
It is easy to see that $q_2$ and $q_3$ are not orthogonal at all.
By contrast, the 
<span title="acronym" ><i>MGS</i></span>
 method differs in the last step:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
As before, $q_1^ta_3=1$, so
\[
 v\leftarrow a_3-q_1 =
\begin{pmatrix}
    0\\ -\epsilon\\ \epsilon\\ 0
\end{pmatrix}
.
\]
  Then, $q_2^tv=\frac{\sqrt2}2\epsilon$ (note that $q_2^ta_3=0$
  before), so the second update gives
\[
 v\leftarrow v- \frac{\sqrt2}2\epsilon q_2=
\begin{pmatrix}
    0\\ \frac\epsilon2\\ -\frac\epsilon2\\ \epsilon
\end{pmatrix}
  ,\quad\hbox{normalized:}\quad
\begin{pmatrix}
    0\\ \frac{\sqrt6}6\\ -\frac{\sqrt6}6\\ 2\frac{\sqrt6}6
\end{pmatrix}
\]
  Now all $q_i^tq_j$ are on the order of $\epsilon$
  for~$i\not=\nobreak j$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h2><a id="Thepowermethod">13.3</a> The power method</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Thepowermethod">The power method</a>
</p>

<p name="switchToTextMode">

The vector sequence
\[
 x_i = Ax_{i-1}, 
\]
where $x_0$ is some starting vector, is called the 
  method} since it computes the product of subsequent matrix powers
times a vector:
\[
 x_i = A^ix_0. 
\]
There are cases where the relation between the $x_i$ vectors is
simple. For instance, if $x_0$ is an eigenvector of~$A$, we have for
some scalar~$\lambda$
\[
 Ax_0=\lambda x_0 \qquad\hbox{and}\qquad x_i=\lambda^i x_0. 
\]
However, for an arbitrary vector $x_0$, the sequence $\{x_i\}_i$ is
likely to consist of independent vectors. Up to a point.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Let $A$ and $x$ be the $n\times n$ matrix and dimension~$n$ vector
\[
  A = 
\begin{pmatrix}
    1&1\\ &1&1\\ &&\ddots&\ddots\\ &&&1&1\\&&&&1
\end{pmatrix}
,\qquad
  x =  (0,\ldots,0,1)^t.
\]
  Show that the sequence $[x,Ax,\ldots,A^ix]$ is an independent set
  for $i<n$. Why is this no longer true for~$i\geq n$?
</p>

<p name="switchToTextMode">
  Now consider the matrix $B$:
\[
 B=\left(
\begin{array}{cccc|cccc}
    1&1&&\\ &\ddots&\ddots&\\ &&1&1\\ &&&1\\ \hline
    &&&&1&1\\ &&&&&\ddots&\ddots\\ &&&&&&1&1\\ &&&&&&&1
\end{array}
\right),\qquad
  y = (0,\ldots,0,1)^t
\]
  Show that the set $[y,By,\ldots,B^iy]$ is an independent set for
  $i<n/2$, but not for any larger values of~$i$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

While in general the vectors $x,Ax,A^2x,\ldots$ can be expected to be
independent, in computer arithmetic this story is no longer so clear.
</p>

<p name="switchToTextMode">
Suppose the matrix has eigenvalues $\lambda_0>\lambda_1\geq\cdots
\lambda_{n-1}$ and corresponding eigenvectors~$u_i$ so that
\[
 Au_i=\lambda_i u_i. 
\]
Let the vector~$x$ be written as
\[
 x=c_0u_0+\cdots +c_{n-1}u_{n-1}, 
\]
then
\[
 A^ix = c_0\lambda_0^iu_i+\cdots +c_{n-1}\lambda_{n-1}^iu_{n-1}. 
\]
If we write this as
\[
 A^ix = \lambda_0^i\left[
    c_0u_i+c_1\left(\frac{\lambda_1}{\lambda_0}\right)^i+
    \cdots +c_{n-1}\left(\frac{\lambda_{n-1}}{\lambda_0}\right)^i
    \right],
\]
we see that, numerically, $A^ix$ will get progressively closer
to a multiple of~$u_0$, the 
<i>dominant eigenvector</i>
. Hence,
any calculation that uses independence of the $A^ix$ vectors is likely
to be inaccurate.
</p>

<h2><a id="Nonnegativematrices;Perronvectors">13.4</a> Nonnegative matrices; Perron vectors</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Nonnegativematrices;Perronvectors">Nonnegative matrices; Perron vectors</a>
</p>

<p name="switchToTextMode">

If $A$ is a nonnegative matrix, the maximal eigenvalue has the
property that its eigenvector is nonnegative: this is the the
<i>Perron-Frobenius theorem</i>
.
</p>

<!-- environment: theorem start embedded generator -->
<!-- TranslatingLineGenerator theorem ['theorem'] -->
  If a nonnegative matrix $A$ is irreducible, its eigenvalues
  satisfy
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The eigenvalue $\alpha_1$ that is largest in magnitude is real
    and simple:
\[
 \alpha_1> |\alpha_2|\geq\cdots. 
\]
<li>
The corresponding eigenvector is positive.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</theorem>
<!-- environment: theorem end embedded generator -->
<p name="switchToTextMode">

</p>

<h2><a id="TheGershgorintheorem">13.5</a> The Gershgorin theorem</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#TheGershgorintheorem">The Gershgorin theorem</a>
</p>

<p name="switchToTextMode">

Finding the eigenvalues of a matrix is usually complicated. However,
there are some tools to estimate eigenvalues. In this section you will
see a theorem that, in some circumstances, can give useful information
on eigenvalues.
</p>

<p name="switchToTextMode">
Let $A$ be a square matrix, and $x,\lambda$ an eigenpair: $Ax=\lambda
x$. Looking at one component, we have
\[
 a_{ii}x_i+\sum_{j\not=i} a_{ij}x_j=\lambda x_i. 
\]
Taking norms:
\[
 (a_{ii}-\lambda) \leq \sum_{j\not=i} |a_{ij}|
   \left|\frac{x_j}{x_i}\right|
\]
Taking the value of~$i$ for which $|x_i|$ is maximal, we find
\[
 (a_{ii}-\lambda) \leq \sum_{j\not=i} |a_{ij}|.
\]
This statement can be interpreted as follows:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
  The eigenvalue $\lambda$ is located in the circle around~$a_{ii}$
  with radius $\sum_{j\not=i}|a_{ij}|$.
</p name="quotation">
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">
Since we do not know for which value of~$i$ $|x_i|$ is maximal, we can
only say that there is 
<i>some</i>
 value of~$i$ such that $\lambda$
lies in such a circle. This is the Gershgorin theorem.
</p>

<!-- environment: theorem start embedded generator -->
<!-- TranslatingLineGenerator theorem ['theorem'] -->
  Let $A$ be a square matrix, and let $D_i$ be the circle with center
  $a_{ii}$ and radius $\sum_{j\not=i}|a_{ij}|$, then the eigenvalues
  are contained in the union of circles~$\cup_i D_i$.
</p name="theorem">
</theorem>
<!-- environment: theorem end embedded generator -->
<p name="switchToTextMode">

We can conclude that the eigenvalues are in the interior of these
discs, if the constant vector is not an eigenvector.
</p>

<h2><a id="Householderreflectors">13.6</a> Householder reflectors</h2>
<p name=crumbs>
crumb trail:  > <a href="norms.html">norms</a> > <a href="norms.html#Householderreflectors">Householder reflectors</a>
</p>
<!-- index -->

<p name="switchToTextMode">

In some contexts the question comes up how to transform one subspace into another.
<i>Householder reflectors</i>
a unit vector~$u$, and let 
\[
 H= I-2uu^t. 
\]
For this matrix we have $Hu=-u$, and if $u\perp v$, then $Hv=v$.
In other words, the subspace of multiples of~$u$ is
flipped, and the orthogonal subspace stays invariant.
</p>

<p name="switchToTextMode">
Now for the original problem of mapping one space into another. Let the original space be spanned
by a vector~$x$ and the resulting by~$y$, then note that
\[
\begin{cases}
x = (x+y)/2 + (x-y)/2\\ y = (x+y)/2 - (x-y)/2
\end{cases}
\]
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/reflector.jpeg" width=800></img>
<p name="caption">
FIGURE 13.1: Householder reflector
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
In other words, we can map $x$ into~$y$ with the reflector based on
$u=(x-y)/2$.
</p>

<p name="switchToTextMode">
We can generalize Householder reflectors to a form 
\[
 H=I-2uv^t. 
\]
The matrices $L_i$ used in LU factorization (see section~
5.3
)
can then be seen to be of the form $L_i = I-\ell_ie_i^t$ where $e_i$~has a single one
in the $i$-th location, and $\ell_i$~only has nonzero below that location.
That form also makes it easy to see that $L_i\inv = I+\ell_ie_i^t$:
\[
 (I-uv^t)(I+uv^t) = I-uv^tuv^t = 0 
\]
if $v^tu=0$.
</p>

<!-- index -->
<p name="switchToTextMode">

</div>
<a href="index.html">Back to Table of Contents</a>
