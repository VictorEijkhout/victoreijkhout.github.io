<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>N-body problems</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


10.1 : <a href="discrete.html#TheBarnes-Hutalgorithm">The Barnes-Hut algorithm</a><br>
10.2 : <a href="discrete.html#TheFastMultipoleMethod">The Fast Multipole Method</a><br>
10.3 : <a href="discrete.html#Fullcomputation">Full computation</a><br>
10.4 : <a href="discrete.html#Implementation">Implementation</a><br>
10.4.1 : <a href="discrete.html#Vectorization">Vectorization</a><br>
10.4.2 : <a href="discrete.html#Sharedmemoryimplementation">Shared memory implementation</a><br>
10.4.3 : <a href="discrete.html#Distributedmemoryimplementation">Distributed memory implementation</a><br>
10.4.4 : <a href="discrete.html#1.5Dimplementationofthefullmethod">1.5D implementation of the full method</a><br>
10.4.4.1 : <a href="discrete.html#Problemdescription">Problem description</a><br>
10.4.4.2 : <a href="discrete.html#Particledistribution">Particle distribution</a><br>
10.4.4.3 : <a href="discrete.html#Workdistribution">Work distribution</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>10 N-body problems</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">
In chapter 
Numerical treatment of differential equations
 we looked at continuous phenomena, such as
the behaviour of a heated rod in the entire interval $[0,1]$ over a
certain time period. There are also applications where you may be
interested in a finite number of points. One such application is the
study of collections of particles, possibly very big particles such as
planets or stars, under the influence of a force such as gravity or
the electrical force. (There can also be external forces, which we
will ignore; also we assume there are no collisions, otherwise we need
to incorporate nearest-neighbour interactions.) This type of problems
is known as N-body problems; for an introduction
see 
<a href=http://www.scholarpedia.org/article/N-body_simulations_(gravitational)>http://www.scholarpedia.org/article/N-body_simulations_(gravitational)</a>
.
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/nbody_pic.gif" width=800></img>
<p name="caption">
WRAPFIGURE 10.1: Summing all forces on a particle
</p>
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2.5in}
A basic algorithm for this problem is easy enough:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
choose some small time interval,
<li>
calculate the forces on each particle, given the locations of
  all the particles,
<li>
move the position of each particle as if the force on it stays
  constant throughout that interval.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For a small enough time interval this algorithm gives a reasonable approximation to the truth.
</p>

<p name="switchToTextMode">
The last step, updating the particle positions, is easy and completely
parallel: the problem is in evaluating the forces. In a naive way this
calculation is simple enough, and even completely parallel:
</p>

<!-- environment: tabbing start embedded generator -->
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for each particle $i$<br>
&nbsp;        for  each particle $j$<br>
&nbsp;&nbsp;         let $\bar r_{ij}$ be the vector between $i$ and $j$;<br>
&nbsp;&nbsp;         then the force on $i$ because of $j$ is<br>
&nbsp;&nbsp;         $\quad f_{ij} = -\bar r_{ij}\frac{m_im_j}{|r_{ij}|}$<br>
&nbsp;&nbsp;         (where $m_i,m_j$ are the masses or charges) and<br>
&nbsp;&nbsp;         $f_{ji}=-f_{ij}$.<br>
</p>
<!-- environment: tabbing end embedded generator -->
<p name="switchToTextMode">

The main objection to this algorithm is that it has quadratic computational
complexity: for $N$ particles, the number of operations is~$O(N^2)$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  If we had $N$ processors, the computations for one update step would
  take time~$O(N)$.  What is the communication complexity? Hint: is
  there a collective operations you can use?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Several algorithms have been invented to get the sequential complexity
down to $O(N\log N)$ or even $O(N)$. As might be expected, these are
harder to implement than the naive algorithm. We will discuss a
popular method: the 
<i>Barnes-Hut algorithm</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#BarnesHut">[BarnesHut]</a>
,
which has $O(N\log N)$ complexity.
</p>

<h2><a id="TheBarnes-Hutalgorithm">10.1</a> The Barnes-Hut algorithm</h2>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#TheBarnes-Hutalgorithm">The Barnes-Hut algorithm</a>
</p>
<!-- index -->
<p name="switchToTextMode">

The basic observation that leads to a reduction in complexity is the
following. If you are calculating the forces on two particles
$i_1,i_2$ that are close together, coming from two particles $j_1,j_2$
that are also close together, you can clump $j_1,j_2$ together into
one particle, and use that for both&nbsp;$i_1,i_2$.
</p>

<p name="switchToTextMode">
Next, the algorithm uses a recursive division of space, in two
dimensions in quadrants and in three dimensions in octants; see
figure&nbsp;
10.2
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
  \hbox{%
<img src="graphics/bh-quadrants.jpeg" width=800></img>
<img src="graphics/bh-quadrants-filled.jpeg" width=800></img>
  }
  \caption{Recursive subdivision of a domain in quadrants with levels
    indicated (left); actual subdivision with one particle per box (right)}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The algorithm is then as follows. First total mass and center of mass
are computed for all cells on all levels:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for each level $\ell$, from fine to coarse:<br>
    \&gt;for each cell $c$ on level $\ell$:<br>
    \&gt;\&gt; compute the total mass and center of mass<br>
    \&gt;\&gt;\&gt; for cell $c$ by considering its children<br>
    \&gt; if there are no particles in this cell,<br>
    \&gt;\&gt; set its mass to zero<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">
Then the levels are used to compute the interaction with each particle:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for each particle $p$:<br>
    \&gt;for each cell $c$ on the top level<br>
    \&gt;\&gt;if $c$ is far enough away from $p$:<br>
    \&gt;\&gt;\&gt;use the total mass and center of mass of $c$;<br>
    \&gt;\&gt;otherwise consider the children of $c$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">

<!-- environment: wrapfigure start embedded generator -->
</p>
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/bh-quadrants-ratio.jpeg" width=800></img>
<p name="caption">
WRAPFIGURE 10.3: Boxes with constant distance/diameter ratio
</p>

</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2.5in}
The test on whether a cell is far enough away is typically implemented
as the ratio of its diameter to its distance being small enough. This
is sometimes referred to as the `cell opening criterium'. In this manner,
each particle interacts with a number of concentric rings of cells,
each next ring of double width; see figure&nbsp;
10.3
.
</p>

<p name="switchToTextMode">
This algorithm is easy to realize if the cells are
organized in a tree. In the three-dimensional case, each cell has
eight children, so this is known as an 
<i>octtree</i>
.
</p>

<p name="switchToTextMode">
The computation of centres of masses has to be done each time after
the particles move. Updating can be less expensive than computing from
scratch. Also, it can happen that a particle crosses a cell border, in
which case the data structure needs to be updated. In the worst case,
a particle moves into a cell that used to be empty.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="TheFastMultipoleMethod">10.2</a> The Fast Multipole Method</h2>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#TheFastMultipoleMethod">The Fast Multipole Method</a>
</p>
</p>

<p name="switchToTextMode">
The 
<i>FMM</i>
 computes an expression for the potential at every point,
not the force as does Barnes-Hut.  
<span title="acronym" ><i>FMM</i></span>
 uses more information than the
mass and center of the particles in a box. This more complicated
expansion is more accurate, but also more expensive. In compensation,
the FMM uses a fixed set of boxes to compute the potential, rather
than a set varying with the accuracy parameter theta, and location of
the center of mass.
</p>

<p name="switchToTextMode">
However, computationally the 
<span title="acronym" ><i>FMM</i></span>
 is much like the Barnes-Hut
method so we will discuss their implementation jointly.
</p>

<h2><a id="Fullcomputation">10.3</a> Full computation</h2>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Fullcomputation">Full computation</a>
</p>
<p name="switchToTextMode">

Despite the above methods for judicious approximation, there are also
efforts at full calculation of the $N^2$ interactions; see for
instance the 
<i>NBODY6</i>
 code of Sverre Aarseth;
see 
<a href=http://www.ast.cam.ac.uk/&nbsp;sverre/web/pages/home.htm>http://www.ast.cam.ac.uk/&nbsp;sverre/web/pages/home.htm</a>
.
Such codes use high order integrators and adaptive time steps.
Fast implementation on the 
<i>Grape computer</i>
 exist;
general parallelization is typically hard because of the need
for regular load balancing.
</p>

<h2><a id="Implementation">10.4</a> Implementation</h2>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a>
</p>
<p name="switchToTextMode">

Octtree methods offer some challenges on high performance
architectures. First of all, the problem is irregular, and secondly,
the irregularity dynamically changes. The second aspect is mostly a
problem in distributed memory, and it
needs 
<i>load rebalancing</i>
; see section&nbsp;
2.10
. In
this section we concentrated on the force calculation in a single
step.
</p>

<h3><a id="Vectorization">10.4.1</a> Vectorization</h3>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#Vectorization">Vectorization</a>
</p>
<p name="switchToTextMode">

The structure of a problem as in figure&nbsp;
10.2
 is
quite irregular. This is a problem for vectorization on the small
scale of 
<i>SSE</i>
/
<i>AVX</i>
 instructions and on the large
scale of vector pipeline processors (see section&nbsp;
2.3.1
 for an
explanation of both). Program steps `for all children of a certain box
do something' will be of irregular length, and data will possibly be
not stored in a regular manner.
</p>

<p name="switchToTextMode">
This problem can be alleviated by subdividing the grid even if this
means having empty boxes. If the bottom level is fully divided, there
will always be eight (in three dimension) particles to operate
on. Higher levels can also be filled in, but this means an increasing
number of empty boxes on the lower levels, so there is a trade-off
between increased work and increasing efficiency.
</p>

<h3><a id="Sharedmemoryimplementation">10.4.2</a> Shared memory implementation</h3>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#Sharedmemoryimplementation">Shared memory implementation</a>
</p>
<p name="switchToTextMode">

Executed on a sequential architecture, this algorithm has complexity
$O(N\log N)$. It is clear that this algorithm will also work on shared
memory if each particle is turned into a task. Since not all cells
contain particles, tasks will have a different running time.
</p>

<h3><a id="Distributedmemoryimplementation">10.4.3</a> Distributed memory implementation</h3>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#Distributedmemoryimplementation">Distributed memory implementation</a>
</p>
<p name="switchToTextMode">

The above shared-memory version of the Barnes-Hut algorithm can not
immediately be used in a distributed memory context, since each
particle can in principle access information from any part of the
total data. It is possible to realize an implementation along these
lines using a 
<i>hashed octtree</i>
, but we will not persue
this.
</p>

<p name="switchToTextMode">
We observe data access is more structured than it seems at
first. Consider a particle $p$ and the cells on level&nbsp;$\ell$ that it
interacts with. Particles located close to $p$ will interact with the
same cells, so we can rearrange interaction by looking at cells on
level&nbsp;$\ell$ and the other cells on the same level that they interact
with.
</p>

<p name="switchToTextMode">
This gives us the following algorithm&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Katzenelson:nbody">[Katzenelson:nbody]</a>
: the
calculation of centres of mass become a calculation of the force
$g^{(\ell)}_p$ exerted 
<i>by</i>
 a particle&nbsp;$p$ on level&nbsp;$\ell$:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for level $\ell$ from one above the finest to the coarsest:<br>
    \&gt;for each cell $c$ on level $\ell$<br>
    \&gt;\&gt;let $g^{(\ell)}_c$ be the combination of the $g^{(\ell+1)}_i$<br>
    for all children $i$ of $c$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">
With this we compute the force 
<i>on</i>
 a cell:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for level $\ell$ from one below the coarses to the finest:<br>
    \&gt;for each cell $c$ on level $\ell$:<br>
    \&gt;\&gt;let $f^{(\ell)}_c$ be the sum of<br>
    \&gt;\&gt;\&gt;1. the force $f^{(\ell-1)}_p$ on the parent $p$ of $c$, and<br>
    \&gt;\&gt;\&gt;2. the sums $g^{(\ell)}_i$ for all $i$ on level $\ell$ that<br>
    \&gt;\&gt;\&gt;\&gt;satisfy the cell opening criterium<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">

We see that on each level, each cell now only interacts with a small
number of neighbours on that level. In the first half of the algorithm
we go up the tree using only parent-child relations between
cells. Presumably this is fairly easy.
</p>

<p name="switchToTextMode">
The second half of the algorithm uses more complicated data
access. The cells&nbsp;$i$ in the second term are all at some distance from
the cell&nbsp;$c$ on which we are computing the force. In graph terms these
cells can be described as cousins: children of a sibling of $c$'s
parent. If the opening criterium is made sharper, we use second
cousins: grandchildren of the sibling of $c$'s grandparent, et cetera.
</p>

<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Argue that this force calculation operation has much in common,
  structurally, with the sparse matrix-vector product.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In the shared memory case we already remarked that different subtrees
take different time to process, but, since we are likely to have more
tasks than processor cores, this will all even out. With distributed
memory we lack the possibility to assign work to arbitrary processors,
so we need to assign load carefully. 
<span title="acronym" ><i>SFCs</i></span>
 can be used here
to good effect (see section&nbsp;
2.10.5.2
).
</p>

<h3><a id="1.5Dimplementationofthefullmethod">10.4.4</a> 1.5D implementation of the full method</h3>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#1.5Dimplementationofthefullmethod">1.5D implementation of the full method</a>
</p>
<p name="switchToTextMode">

It is possible to make a straightforward parallel implementation of
the full $N^2$ method by distributing the particles, and let each
particle evaluate the forces from every other particle.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Since forces are symmetric, we can save a factor of two in
  work by letting particle $p_i$ only interact with particles $p_j$
  with&nbsp;$j&gt;i$. In this scheme, an equal distribution of data, e.g.,
  particles, leads to an uneven distribution of work, e.g.,
  interactions. How do the particles need to be distributed to get an
  even load distribution?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Assuming we don't care about the factor of two gain from using
symmetry of forces, and using an even distribution of the particles,
this scheme has a more serious problem in that it asymptotically does
not scale.
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

The communication cost of this is
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$O(P)$ latency since messages need to be received from all processors.
<li>
$O(N)$ bandwidth, since all particles need to be received.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
If we however distribute the force calculations, rather than the particles,
we come to different bounds; see~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Driscoll:optimal-nbody">[Driscoll:optimal-nbody]</a>
 and
references cited therein.
</p>

<p name="switchToTextMode">
With every processor computing the interactions in a block with sides $N/\sqrt P$,
there is replication of particles and forces need to be collected.
Thus, the cost is now
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$O(\log p)$ latency for the broadcast and reduction
<li>
$O(N/\sqrt P\cdot \log P)$ bandwidth, since that is how much
  force data each processor contributes to the final sum.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Problemdescription">10.4.4.1</a> Problem description</h4>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#1.5Dimplementationofthefullmethod">1.5D implementation of the full method</a> > <a href="discrete.html#Problemdescription">Problem description</a>
</p>
</p>

<p name="switchToTextMode">
We abstract the N-body problem as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
We assume that there is a vector~$c(\cdot)$
  of the particle charges/masses and location information.
<li>
To compute the location update, we need the pairwise
  interactions $F(\cdot,\cdot)$ based on storing tuples
  $C_{ij}=\langle c_i,c_j\rangle$.
<li>
The interactions are then summed to a force vector~$f(\cdot)$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
In the 
<span title="acronym" ><i>IMP</i></span>
 model, the algorithm is sufficiently described
if we know the respective data distributions; we are not immediately concerned with the
local computations.
</p>

<h4><a id="Particledistribution">10.4.4.2</a> Particle distribution</h4>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#1.5Dimplementationofthefullmethod">1.5D implementation of the full method</a> > <a href="discrete.html#Particledistribution">Particle distribution</a>
</p>
<p name="switchToTextMode">

\def\dottimes{\mathbin{\cdot_\times}}
</p>

<p name="switchToTextMode">
An implementation based on particle distribution takes as its starting
point the particle vector~$c$ distributed as~$c(u)$, and
directly computes the force vector $f(u)$ on the same distribution.
We describe the
computation as a sequence of three kernels, two of data movement and
one of local computation\footnote{In the full 
<span title="acronym" ><i>IMP</i></span>
  theory~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Eijkhout:ICCS2012">[Eijkhout:ICCS2012]</a>
 the first two kernels can be
  collapsed.}:
<!-- environment: equation start embedded generator -->
</p>
\begin{cases}
    \{ \alpha\colon c(u) \}&\hbox{initial distribution for $c$ is $u$}\\
    C(u,*) = c(u)\dottimes c(*)&\hbox{replicate $c$ on each processor}\\
    \hbox{local computation of $F(u,*)$ from $C(u,*)$}\\
    f(u) = \sum\_2 F(u,*)&\hbox{local reduction of partial forces}\\
    \hbox{particle position update}
\end{cases}
\label{eq:nbody-1d}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
The final kernel is a reduction with identical $\alpha$ and
$\beta$-distribution, so it does not involve data motion.
The local computation kernel also has not data motion.
That leaves the gathering of&nbsp;$C(u,*)$ from the initial
distribution&nbsp;$c(u)$. Absent any further information on the
distribution&nbsp;$u$ this is an allgather of $N$ elements,
at a cost of $\alpha\log p+\beta N$.
In particular, the communication cost does not go down
with&nbsp;$p$.
</p>

<p name="switchToTextMode">
With a suitable programming system based on distributions,
the system of equations&nbsp;\eqref{eq:nbody-1d} can be
translated into code.
</p>

<h4><a id="Workdistribution">10.4.4.3</a> Work distribution</h4>
<p name=crumbs>
crumb trail:  > <a href="discrete.html">discrete</a> > <a href="discrete.html#Implementation">Implementation</a> > <a href="discrete.html#1.5Dimplementationofthefullmethod">1.5D implementation of the full method</a> > <a href="discrete.html#Workdistribution">Work distribution</a>
</p>
<p name="switchToTextMode">

The particle distribution implementation used a one-dimensional
distribution $F(u,*)$ for the forces conformally with the particle
distribution. Since $F$&nbsp;is a two-dimensional object, it is also
possible to use a two-dimensional distribution. We will now explore
this option, which was earlier described
in&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Driscoll:optimal-nbody">[Driscoll:optimal-nbody]</a>
; our purpose here is to show how this
strategy can be implemented and analyzed in the 
<span title="acronym" ><i>IMP</i></span>
 framework.
Rather than expressing the algorithm as
distributed computation and replicated data, we use a
distributed temporary, and we derive the replication scheme
as collectives on subcommunicators.
</p>

<p name="switchToTextMode">
For a two-dimensional distribution of&nbsp;$F$ we need $(N/b)\times (N/b)$ processors
where $b$&nbsp;is a blocksize parameter. For ease of exposition we use $b=1$,
giving a number of processors $P=N^2$.
</p>

<p name="switchToTextMode">
We will now go through the necessary reasoning. The full algorithm in
close-to-implementable form is given in figure&nbsp;
10.4
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: equation start embedded generator -->
\begin{array}{ll}
      \{\alpha\colon C( D\colon &lt;I,I&gt;) \}&\hbox{initial distribution on the processor diagonal}\\
      C(I,I) = C(I,p)+C(q,I)&\hbox{row and column broadcast of the processor diagonal}\\
      F(I,I)\leftarrow C(I,I)&\hbox{local interaction calculation}\\
      f(D\colon&lt;I,I&gt;)=\sum\_2 F(I,D\colon *)&\hbox{summation from row replication on the diagonal}
\end{array}
\label{eq:nbody-1.5d}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="caption">
FIGURE 10.4: IMP realization of the 1.5D N-body problem
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
For simplicity we use the identify distribution $I\colon p\mapsto\{p\}$.
</p>

<p name="switchToTextMode">

<b>Initial store on the diagonal of the processor grid</b><br>

</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/all-pairs-init.jpeg" width=800></img>
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2in}
We first consider the gathering of the tuples $C_{ij}=\langle c_i,c_j\rangle$.
Initially, we decide to let the $c$&nbsp;vector be stored on the diagonal of the processor grid;
in other words,
for all&nbsp;$p$, processor&nbsp;$\langle p,p\rangle$ contains&nbsp;$C_{pp}$,
and the content of all other processors is undefined.
</p>

<p name="switchToTextMode">
To express this formally, we let $D$ be the diagonal $\{\langle p,p\rangle\colon p\in P\}$.
Using the mechanism of partially defined distributions
the initial $\alpha$-distribution is then
<!-- environment: equation start embedded generator -->
</p>
C( D\colon &lt;I,I&gt;)\equiv D\ni \langle p,q\rangle\mapsto
C(I(p),I(q))=C\_{pq}.
\label{eq:c-alpha}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">


<b>Replication for force computation</b><br>

</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/all-pairs-bcst.jpeg" width=800></img>
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2in}
The local force computation $f_{pq}=f(C_{pq})$ needs for each processor $\langle p,q\rangle$
the quantity&nbsp;$C_{pq}$, so we need a $\beta$-distribution of&nbsp;$C(I,I)$.
The key to our story is the realization that
\[
 C_{pq} = C_{pp}+C_{qq} 
\]
so that
\[
 C(I,I) = C(I,p)+C(q,I) 
\]
(where $p$ stands for the distribution that maps each processor
to the index value&nbsp;$p$ and similarly for&nbsp;$q$.)
</p>

<p name="switchToTextMode">
To find the transformation from $\alpha$ to $\beta$-distribution
we consider the
transformation of the expression $C(D\colon\langle I,I\rangle)$.
In general, any set&nbsp;$D$ can be written as a mapping from the first coordinate to
sets of values of the second:
\[
 D\equiv p\mapsto D_p\qquad\hbox{where}\qquad
  D_p=\{q\colon \langle p,q\rangle\in D\}.
\]
In our particular case of the diagonal we have
\[
 D\equiv p\mapsto\{p\}.
\]
With this we write
<!-- environment: equation start embedded generator -->
</p>
C(D\colon \langle I,I\rangle) = C(I,D\_p\colon I) = C(I,\{p\}\colon p).
\label{eq:c-alpha-rewrite}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">

Note that equation&nbsp;\eqref{eq:c-alpha-rewrite} is still the $\alpha$-distribution.
The $\beta$-distribution is $C(I,p)$, and it is an exercise in pattern matching to
see that this is attained by a broadcast in each row, which carries a cost of
\[
 \alpha \log\sqrt p + \beta N/\sqrt P. 
\]
Likewise, $C(q,I)$&nbsp;is found from the $\alpha$-distribution by
column broadcasts. We conclude that this variant does
have a communication cost that goes down proportionally
with the number of processors.
</p>

<p name="switchToTextMode">

<b>Local interaction calculation</b><br>

</p>

<p name="switchToTextMode">
The calculation of $F(I,I)\leftarrow C(I,I)$ has identical $\alpha$ and $\beta$
distributions, so it is trivialy parallel.
</p>

<p name="switchToTextMode">

<b>Summing of the forces</b><br>

</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/all-pairs-f.jpeg" width=800></img>
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2in}
We have to extend the force vector&nbsp;$f(\cdot)$ to our two-dimensional
processor grid as $f(\cdot,\cdot)$. However, conforming to the initial
particle distribution on the diagonal, we only sum on the diagonal.
The instruction
\[
 f(D\colon&lt;I,I&gt;)=\sum_2 F(I,D\colon *) 
\]
has a $\beta$-distribution of $I,D:*$, which is formed from the $\alpha$-distribution
of $I,I$ by gathering in the rows.
</p>

<p name="switchToTextMode">

<!-- index -->
</p>

</div>
<a href="index.html">Back to Table of Contents</a>
