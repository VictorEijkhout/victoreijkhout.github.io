<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>High performance linear algebra</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


6.1 : <a href="parallellinear.html#Collectiveoperations">Collective operations</a><br>
6.1.1 : <a href="parallellinear.html#Broadcast">Broadcast</a><br>
6.1.2 : <a href="parallellinear.html#Reduction">Reduction</a><br>
6.1.3 : <a href="parallellinear.html#Allreduce">Allreduce</a><br>
6.1.4 : <a href="parallellinear.html#Allgather">Allgather</a><br>
6.1.5 : <a href="parallellinear.html#Reduce-scatter">Reduce-scatter</a><br>
6.2 : <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a><br>
6.2.1 : <a href="parallellinear.html#Implementingtheblock-rowcase">Implementing the block-row case</a><br>
6.2.2 : <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a><br>
6.2.2.1 : <a href="parallellinear.html#Matrix-vectorproduct,partitioningbyrows">Matrix-vector product, partitioning by rows</a><br>
6.2.2.1.1 : <a href="parallellinear.html#Anoptimist'sview">An optimist's view</a><br>
6.2.2.1.2 : <a href="parallellinear.html#Apessimist'sview">A pessimist's view</a><br>
6.2.2.1.3 : <a href="parallellinear.html#Arealist'sview">A realist's view</a><br>
6.2.2.2 : <a href="parallellinear.html#Matrix-vectorproduct,partitioningbycolumns">Matrix-vector product, partitioning by columns</a><br>
6.2.2.2.1 : <a href="parallellinear.html#Costanalysis">Cost analysis</a><br>
6.2.2.3 : <a href="parallellinear.html#Two-dimensionalpartitioning">Two-dimensional partitioning</a><br>
6.2.2.3.1 : <a href="parallellinear.html#Algorithm">Algorithm</a><br>
6.2.2.3.2 : <a href="parallellinear.html#Costanalysis">Cost analysis</a><br>
6.3 : <a href="parallellinear.html#LUfactorizationinparallel">LU factorization in parallel</a><br>
6.3.1 : <a href="parallellinear.html#Solvingatriangularsystem">Solving a triangular system</a><br>
6.3.2 : <a href="parallellinear.html#Factorization,densecase">Factorization, dense case</a><br>
6.3.3 : <a href="parallellinear.html#Factorization,sparsecase">Factorization, sparse case</a><br>
6.4 : <a href="parallellinear.html#Matrix-matrixproduct">Matrix-matrix product</a><br>
6.4.1 : <a href="parallellinear.html#Gotomatrix-matrixproduct">Goto matrix-matrix product</a><br>
6.4.2 : <a href="parallellinear.html#Cannon'salgorithmforthedistributedmemorymatrix-matrixproduct">Cannon's algorithm for the distributed memory matrix-matrix product</a><br>
6.5 : <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a><br>
6.5.1 : <a href="parallellinear.html#Thesingle-processorsparsematrix-vectorproduct">The single-processor sparse matrix-vector product</a><br>
6.5.2 : <a href="parallellinear.html#Theparallelsparsematrix-vectorproduct">The parallel sparse matrix-vector product</a><br>
6.5.3 : <a href="parallellinear.html#Parallelefficiencyofthesparsematrix-vectorproduct">Parallel efficiency of the sparse matrix-vector product</a><br>
6.5.4 : <a href="parallellinear.html#Memorybehaviorofthesparsematrix-vectorproduct">Memory behavior of the sparse matrix-vector product</a><br>
6.5.5 : <a href="parallellinear.html#Thetransposeproduct">The transpose product</a><br>
6.5.6 : <a href="parallellinear.html#Setupofthesparsematrix-vectorproduct">Setup of the sparse matrix-vector product</a><br>
6.6 : <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a><br>
6.6.1 : <a href="parallellinear.html#Vectoroperations">Vector operations</a><br>
6.6.1.1 : <a href="parallellinear.html#Vectoradditions">Vector additions</a><br>
6.6.1.2 : <a href="parallellinear.html#Innerproducts">Inner products</a><br>
6.6.2 : <a href="parallellinear.html#Finiteelementmatrixconstruction">Finite element matrix construction</a><br>
6.6.3 : <a href="parallellinear.html#Asimplemodelforiterativemethodperformance">A simple model for iterative method performance</a><br>
6.7 : <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a><br>
6.7.1 : <a href="parallellinear.html#Jacobipreconditioning">Jacobi preconditioning</a><br>
6.7.2 : <a href="parallellinear.html#ThetroublewithILUinparallel">The trouble with ILU in parallel</a><br>
6.7.3 : <a href="parallellinear.html#BlockJacobimethods">Block Jacobi methods</a><br>
6.7.4 : <a href="parallellinear.html#ParallelILU">Parallel ILU</a><br>
6.8 : <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a><br>
6.8.1 : <a href="parallellinear.html#Nesteddissection">Nested dissection</a><br>
6.8.1.1 : <a href="parallellinear.html#Domaindecomposition">Domain decomposition</a><br>
6.8.1.2 : <a href="parallellinear.html#Complexity">Complexity</a><br>
6.8.1.3 : <a href="parallellinear.html#Parallelism">Parallelism</a><br>
6.8.1.4 : <a href="parallellinear.html#Preconditioning">Preconditioning</a><br>
6.8.2 : <a href="parallellinear.html#Variablereorderingandcoloring:independentsets">Variable reordering and coloring: independent sets</a><br>
6.8.2.1 : <a href="parallellinear.html#Red-blackcoloring">Red-black coloring</a><br>
6.8.2.2 : <a href="parallellinear.html#Generalcoloring">General coloring</a><br>
6.8.2.3 : <a href="parallellinear.html#Multi-colorparallelILU">Multi-color parallel ILU</a><br>
6.8.3 : <a href="parallellinear.html#Irregulariterationspaces">Irregular iteration spaces</a><br>
6.8.4 : <a href="parallellinear.html#Orderingforcacheefficiency">Ordering for cache efficiency</a><br>
6.8.5 : <a href="parallellinear.html#Operatorsplitting">Operator splitting</a><br>
6.9 : <a href="parallellinear.html#Parallelismandimplicitoperations">Parallelism and implicit operations</a><br>
6.9.1 : <a href="parallellinear.html#Wavefronts">Wavefronts</a><br>
6.9.2 : <a href="parallellinear.html#Recursivedoubling">Recursive doubling</a><br>
6.9.3 : <a href="parallellinear.html#Approximatingimplicitbyexplicitoperations,seriesexpansion">Approximating implicit by explicit operations, series expansion</a><br>
6.10 : <a href="parallellinear.html#Gridupdates">Grid updates</a><br>
6.10.1 : <a href="parallellinear.html#Analysis">Analysis</a><br>
6.10.2 : <a href="parallellinear.html#Communicationandworkminimizingstrategy">Communication and work minimizing strategy</a><br>
6.11 : <a href="parallellinear.html#Blockalgorithmsonmulticorearchitectures">Block algorithms on multicore architectures</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>6 High performance linear algebra</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In this section we will discuss  a number of issues pertaining to
linear algebra on parallel computers. We will take a realistic view of
this topic, assuming that the number of processors is finite, and that
the problem data is always large, relative to the number of
processors. We will also pay attention to the physical aspects of the
communication network between the processors.
</p>

<p name="switchToTextMode">
We will analyze various linear algebra operations, including iterative
methods, and their behavior in the presence of a network with finite
bandwidth and finite connectivity. This chapter will conclude with
various short remarks regarding complications in algorithms that arise
due to parallel execution.
</p>

<h2><a id="Collectiveoperations">6.1</a> Collective operations</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a>
</p>

<!-- index -->
<!-- index -->

<p name="switchToTextMode">

Collective operations play an important part in linear algebra
operations. In fact, the scalability of the operations can depend on
the cost of these collectives as you will see below.  Here we give a
short discussion of the essential ideas; see~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Chan2007Collective">[Chan2007Collective]</a>
for details.
</p>

<!-- index -->
<!-- index -->
<!-- index -->
<p name="switchToTextMode">

In computing the cost of a collective operation, three architectural
constants are enough to give lower bounds: $\alpha$,~the 
<!-- index -->
 of
sending a single message, $\beta$,~the inverse of the 
<!-- index -->
 for
sending data (see section~
1.3.2
), and $\gamma$,
the inverse of the 
<!-- index -->
, the
time for performing an arithmetic operation. Sending $n$ data
items then takes time $\alpha +\beta n$. We further assume that a
processor can only send one message at a time. We make no assumptions
about the connectivity of the processors; thus, the lower bounds
derived here will hold for a wide range of architectures.
</p>

<p name="switchToTextMode">
The main implication of the architectural model above is that the
number of active processors can only double in each step of an
algorithm. For instance, to do a broadcast, first processor~0 sends
to~1, then 0~and~1 can send to 2~and~3, then 0--3 send to 4--7, et
cetera. This cascade of messages is called
a 
<i>minimum spanning tree</i>
 of the processor network, and
it follows that any collective algorithm has at least $\alpha\log_2p$
cost associated with the accumulated latencies.
</p>

<h3><a id="Broadcast">6.1.1</a> Broadcast</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a> > <a href="parallellinear.html#Broadcast">Broadcast</a>
</p>
<!-- index -->
<p name="switchToTextMode">

In a 
<i>broadcast</i>
 operation, a single processor has $n$ data elements
that is needs to send to all others: the other processors need a full
copy of all $n$ elements.  By
the above doubling argument, we conclude that a broadcast to $p$
processors takes time at least $\lceil\log_2 p\rceil$ steps with a
total latency of $\lceil\log_2 p\rceil \alpha$. Since $n$ elements are
sent, this adds at least a time $n\beta$ for all elements to leave the
sending processor, giving a total cost lower bound of
\[
 \lceil\log_2 p\rceil \alpha+n\beta. 
\]
</p>

<p name="switchToTextMode">
We can illustrate the spanning tree method as follows:
\[
\begin{array}{|c|ccc|}
\hline
  &t=1&t=2&t=3\\ \hline
p_0&x_0\downarrow,x_1\downarrow,x_2\downarrow,x_3\downarrow
   &x_0\downarrow,x_1\downarrow,x_2\downarrow,x_3\downarrow
   &x_0,x_1,x_2,x_3\\
p_1&&x_0\downarrow,x_1\downarrow,x_2\downarrow,x_3\downarrow
   &x_0,x_1,x_2,x_3\\
p_2&&&x_0,x_1,x_2,x_3\\
p_3&&&x_0,x_1,x_2,x_3\\
\hline
\end{array}
\]
(On $t=1$, $p_0$ sends to $p_1$; on $t=2$ $p_0,p_1$ send to $p_2,p_3$.)
This algorithm has the correct $\log_2p\cdot\alpha$ term, but processor~0 repeatedly
sends the whole vector, so the bandwidth cost is $\log_2p\cdot n\beta$.
If $n$ is small, the latency cost dominates, so we may characterize this
as a 
<i>short vector collective operation</i>
</p>

<p name="switchToTextMode">
The following algorithm implements the broadcast as a combination of a scatter
and a 
<i>bucket brigade algorithm</i>
. First the scatter:
\[
\begin{array}{|c|cccc|}
\hline
  &t=0&t=1&t=2&t=3\\ \hline
 t=0
 t=1
 t=2
 t=3
  \\
 t=0
  &x_1&&\\
 t=0,1
  &x_2&\\
 t=0,1,2
  &x_3\\
\hline
\end{array}
\]
takes $p-1$ messages of size $N/p$, for a total time of
\[
 T_{\scriptstyle\mathrm{scatter}}(N,P) = (p-1)\alpha +
(p-1)\cdot\frac{N}{p}\cdot beta.
\]
</p>

<p name="switchToTextMode">
Then the bucket brigade has each processor active in every step,
accepting a partial message (except in the first step), and passing it
on to the next processor.
\[
\begin{array}{|c|lll|}
\hline
  &t=0&t=1&et cetera\\ \hline
p_0&x_0\downarrow\hphantom{,x_1\downarrow,x_2\downarrow,x_3\downarrow}
   &x_0\hphantom{\downarrow,x_1\downarrow,x_2\downarrow,}x_3\downarrow
   &x_0,\hphantom{x_1,}x_2,x_3\\
p_1&\hphantom{x_0\downarrow,}x_1\downarrow\hphantom{,x_2\downarrow,x_3\downarrow}
   &x_0\downarrow,x_1\hphantom{\downarrow,x_2\downarrow,x_3\downarrow}
   &x_0,x_1,\hphantom{x_2,}x_3\\
p_2&\hphantom{x_0\downarrow,x_1\downarrow,}x_2\downarrow\hphantom{,x_3\downarrow}
   &\hphantom{x_0\downarrow,}x_1\downarrow,x_2\hphantom{\downarrow,x_3\downarrow}
   &x_0,x_1,x_2\hphantom{,x_3}\\
p_3&\hphantom{x_0\downarrow,x_1\downarrow,x_2\downarrow,}x_3\downarrow
   &\hphantom{x_0\downarrow,x_1\downarrow,}x_2\downarrow,x_3\hphantom{\downarrow}
   &\hphantom{x_0,}x_1,x_2,x_3\\
\hline
\end{array}
\]
Each partial message gets sent $p-1$ times, so this stage also has a
complexity of
\[
 T_{\scriptsize\mathrm{bucket}}(N,P) = (p-1)\alpha +
(p-1)\cdot\frac{N}{p}\cdot beta.
\]
</p>

<p name="switchToTextMode">
The complexity now becomes 
\[
 2(p-1)\alpha+2\beta n(p-1)/p 
\]
which is not optimal in latency, but is a better algorithm if $n$ is large,
making this a 
<i>long vector collective operation</i>
.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Reduction">6.1.2</a> Reduction</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a> > <a href="parallellinear.html#Reduction">Reduction</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
In a 
<i>reduction</i>
 operation, each processor has $n$ data elements, and
one processor needs to combine them elementwise, for instance
computing $n$ sums or products.
</p>

<p name="switchToTextMode">
By running the broadcast backwards in time, we see that a reduction
operation has the same lower bound on the communication of
$\lceil\log_2 p\rceil \alpha+n\beta$.  A~reduction operation also
involves computation, which would take a total time of $(p-1)\gamma n$
sequentially: each of
$n$ items gets reduced over $p$ processors. Since these operations can
potentially be parallelized, the lower bound on the computation is
$\frac{p-1}p \gamma n$, giving a total of
\[
 \lceil\log_2 p\rceil \alpha+n\beta +\frac{p-1}p \gamma n. 
\]
</p>

<p name="switchToTextMode">
We illustrate the spanning tree algorithm,
using the notation $x_i^{(j)}$ for the data item~$i$
that was originally on processor~$j$, and $x_i^{(j:k)}$ for the sum of
the items~$i$ of processors $j\ldots k$.
\[
\begin{array}{|c|ccc|}
\hline
  &t=1&t=2&t=3\\ \hline
p_0&x_0^{(0)},x_1^{(0)},x_2^{(0)},x_3^{(0)}
   &x_0^{(0:1)},x_1^{(0:1)},x_2^{(0:1)},x_3^{(0:1)}
   &x_0^{(0:3)},x_1^{(0:3)},x_2^{(0:3)},x_3^{(0:3)}\\
p_1&x_0^{(1)}\uparrow,x_1^{(1)}\uparrow,x_2^{(1)}\uparrow,x_3^{(1)}\uparrow
   &&\\
p_2&x_0^{(2)},x_1^{(2)},x_2^{(2)},x_3^{(2)}
   &x_0^{(2:3)}\uparrow,x_1^{(2:3)}\uparrow,x_2^{(2:3)}\uparrow,x_3^{(2:3)}\uparrow
   &\\
p_3&x_0^{(3)}\uparrow,x_1^{(3)}\uparrow,x_2^{(3)}\uparrow,x_3^{(3)}\uparrow
   &&\\
\hline
\end{array}
\]
On time $t=1$ processors $p_0,p_2$ receive from $p_1,p_3$, and on
$t=2$ $p_0$ receives from~$p_2$.
</p>

<p name="switchToTextMode">
As with the broadcast above,
this algorithm does not achieve the lower bound; instead it has a complexity
\[
 \lceil\log_2 p\rceil (\alpha+n\beta +\frac{p-1}p \gamma n). 
\]
For short vectors the $\alpha$ term dominates, so this algorithm is sufficient.
For long vectors one can, as above, use other algorithms~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Chan2007Collective">[Chan2007Collective]</a>
.
</p>

<p name="switchToTextMode">
A 
<i>long vector reduction</i>
 can be done using a bucket
brigade followed by a gather. The complexity is as above, except that
the bucket brigade performs partial reductions, for a time
of~$\gamma(p-1)N/p$. The gather performs no further operations.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Allreduce">6.1.3</a> Allreduce</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a> > <a href="parallellinear.html#Allreduce">Allreduce</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
An 
<i>allreduce</i>
 operation computes the same elementwise reduction of $n$
elements on each processor, but leaves the result on each processor,
rather than just on the root of the spanning tree. This could be
implemented as a reduction followed by a broadcast, but more clever
algorithms exist.
</p>

<p name="switchToTextMode">
The lower bound on the cost of an allreduce is, somewhat remarkably,
almost the same as of a simple reduction: since in a reduction not all
processors are active at the same time, we assume that the extra work
can be spread out perfectly. This means that the lower bound on the
latency and computation stays the same. For the bandwidth we reason as
follows: in order for the communication to be perfectly parallelized,
$\frac{p-1}p n$ items have to arrive at, and leave each
processor. Thus we have a total time of
\[
 \lceil \log_2 p\rceil\alpha +2\frac{p-1}pn\beta
    +\frac{p-1}pn\gamma. 
\]
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Allgather">6.1.4</a> Allgather</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a> > <a href="parallellinear.html#Allgather">Allgather</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
In a 
<i>gather</i>
 operation on $n$ elements, each processor has
$n/p$ elements, and one processor collects them all, without combining
them as in a reduction. The 
<i>allgather</i>
 computes the same gather,
but leaves the result on all processors.
</p>

<p name="switchToTextMode">
Again we assume that gathers with multiple
targets are active simultaneously. Since every processor originates a
minimum spanning tree, we have $\log_2p\alpha$ latency; since each
processor receives $n/p$ elements from $p-1$ processors, there is
$(p-1)\times(n/p)\beta$ bandwidth cost. The total cost for constructing a
length~$n$ vector by allgather is then
\[
 \lceil \log_2 p\rceil\alpha +\frac{p-1}pn\beta. 
\]
We illustrate this:
\[
\begin{array}{|c|ccc|}
\hline
  &t=1&t=2&t=3\\ \hline
p_0&x_0\downarrow&x_0x_1\downarrow&x_0x_1x_2x_3\\
p_1&x_1\uparrow&x_0x_1\downarrow&x_0x_1x_2x_3\\
p_2&x_2\downarrow&x_2x_3\uparrow&x_0x_1x_2x_3\\
p_3&x_3\uparrow&x_2x_3\uparrow&x_0x_1x_2x_3\\
\hline
\end{array}
\]
At time $t=1$, there is an exchange between neighbors $p_0,p_1$ and
likewise $p_2,p_3$; at $t=2$ there is an exchange over distance two
between $p_0,p_2$ and likewise~$p_1,p_3$.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Reduce-scatter">6.1.5</a> Reduce-scatter</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Collectiveoperations">Collective operations</a> > <a href="parallellinear.html#Reduce-scatter">Reduce-scatter</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
In a 
<i>reduce-scatter</i>
 operation, each processor has $n$
elements, and an $n$-way reduction is done on them. Unlike in the
reduce or allreduce, the result is then broken up, and distributed as
in a scatter operation.
</p>

<p name="switchToTextMode">
Formally, processor~$i$ has an
item~$x_i^{(i)}$, and it needs $\sum_j x_i^{(j)}$. We could implement
this by doing a size~$p$ reduction, collecting the vector $(\sum_i
x_0^{(i)},\sum_i x_1^{(i)},\ldots)$ on one processor, and scattering
the results. However it is possible to combine these operations in a
so-called 
<i>bidirectional exchange</i>
 algorithm:
</p>

<p name="switchToTextMode">
\[
\begin{array}{|c|ccc|}
\hline
  &t=1&t=2&t=3\\ \hline
p_0&x_0^{(0)},x_1^{(0)},x_2^{(0)}\downarrow,x_3^{(0)}\downarrow
   &x_0^{(0:2:2)},x_1^{(0:2:2)}\downarrow
    \hphantom{x_0^{(0:2:2)},x_1^{(0:2:2)}\downarrow}
   &x_0^{(0:3)}
    \hphantom{x_3^{(0:3)}x_3^{(0:3)}x_3^{(0:3)}}\\
p_1&x_0^{(1)},x_1^{(1)},x_2^{(1)}\downarrow,x_3^{(1)}\downarrow
   &x_0^{(1:3:2)}\uparrow,x_1^{(1:3:2)}
    \hphantom{x_0^{(0:2:2)},x_1^{(0:2:2)}\downarrow}
   &\hphantom{x_3^{(0:3)}} x_1^{(0:3)}
    \hphantom{x_3^{(0:3)}x_3^{(0:3)}} \\
p_2&x_0^{(2)}\uparrow,x_1^{(2)}\uparrow,x_2^{(2)},x_3^{(2)}
   &\hphantom{x_0^{(0:2:2)},x_1^{(0:2:2)}\downarrow}
    x_2^{(0:2:2)},x_3^{(0:2:2)}\downarrow
   &\hphantom{x_3^{(0:3)}x_3^{(0:3)}} x_2^{(0:3)}
    \hphantom{x_3^{(0:3)}}\\
p_3&x_0^{(3)}\uparrow,x_1^{(3)}\uparrow,x_2^{(3)},x_3^{(3)}
   &\hphantom{x_0^{(0:2:2)},x_1^{(0:2:2)}\downarrow}
    x_0^{(1:3:2)}\uparrow,x_1^{(1:3:2)}
   &\hphantom{x_3^{(0:3)}x_3^{(0:3)}x_3^{(0:3)}}
    x_3^{(0:3)}\\
\hline
\end{array}
\]
</p>

<p name="switchToTextMode">
The reduce-scatter can be considered as a allgather run in reverse,
with arithmetic added, so the cost is
\[
 \lceil \log_2 p\rceil\alpha +\frac{p-1}pn(\beta+\gamma). 
\]
</p>

<!-- index -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
</p>

<h2><a id="Paralleldensematrix-vectorproduct">6.2</a> Parallel dense matrix-vector product</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a>
</p>

<!-- index -->
<!-- index -->
<p name="switchToTextMode">

In this section we will go into great detail into the performance, and
in particular the scalability, of the parallel dense matrix-vector
product. First we will consider a simple case, and discuss the
parallelism aspects in some amount of detail.
</p>

<h3><a id="Implementingtheblock-rowcase">6.2.1</a> Implementing the block-row case</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Implementingtheblock-rowcase">Implementing the block-row case</a>
</p>

<p name="switchToTextMode">

In designing a parallel version of an algorithm, one often proceeds by
making a 
the case of a matrix-vector operations such as the product $y=Ax$, we
have the choice of starting with a vector decomposition, and exploring
its ramifications on how the matrix can be decomposed, or rather to
start with the matrix, and deriving the vector decomposition from it.
In this case, it seems natural to start with decomposing the matrix
rather than the vector, since it will be most likely of larger
computational significance. We now have two choices:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
We make a one-dimensional decomposition of the matrix, splitting
  it in block rows or block columns, and assigning each of these --~or
  groups of them~-- to a processor.
<li>
Alternatively, we can make a two-dimensional decomposition,
  assigning to each processor one or more general submatrices.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

We start by considering the decomposition in block rows. Consider
a processor~$p$ and the set $I_p$ of indices of rows that it
owns
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {For ease of exposition we will let $I_p$ be a contiguous
    range of indices, but any general subset is allowed.}
</p>

<p name="switchToTextMode">
, and
let $i\in I_p$ be a row that is assigned to this processor.
The elements in
row~$i$ are used in the operation
\[
 y_i=\sum_ja_{ij}x_j 
\]
We now reason:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
If processor $p$ has all $x_j$ values, the matrix-vector product
  can trivially be executed, and upon completion, the processor has
  the correct values~$y_j$ for~$j\in I_p$.
<li>
This means that every processor needs to have a copy of~$x$,
  which is wasteful. Also it raises the question of data integrity:
  you need to make sure that each processor has the correct value
  of~$x$.
<li>
In certain practical applications (for instance iterative
  methods, as you have seen before), the output of the matrix-vector
  product is, directly or indirectly, the input for a next
  matrix-vector operation. This is certainly the case for the power
  method which computes $x, Ax, A^2x,\ldots$. Since our operation
  started with each processor having the whole of~$x$, but ended with
  it owning only the local part of~$Ax$, we have a mismatch.
<li>
Maybe it is better to assume that each processor, at the start
  of the operation, has only the local part of~$x$, that is,
  those~$x_i$ where~$i\in I_p$, so that the start state and end state
  of the algorithm are the same. This means we have to change the
  algorithm to include some communication that allows each processor
  to obtain those values~$x_i$ where~$i\not\in\nobreak I_p$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Go through a similar reasoning for the case where the matrix is
  decomposed in block columns. Describe the parallel algorithm in
  detail, like above, but without giving pseudo code.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">
Let us now look at the communication in detail: we will consider a
fixed processor&nbsp;$p$ and consider the operations it performs and the
communication that necessitates.
According to the above analysis,
in executing the statement $y_i=\sum_ja_{ij}x_j$ we have
to be aware what processor the $j$ values `belong to'. To acknowledge
this, we write
<!-- environment: equation start embedded generator -->
</p>
  y\_i=\sum\_{j\in I\_p}a\_{ij}x\_j+\sum\_{j\not\in I\_p}a\_{ij}x\_j
\label{eq:yi=sum-in-and-not}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
If $j\in I_p$, the instruction $y_i \leftarrow y_i + a_{aij} x_j$
involves only quantities that are already local to
the processor.
Let us therefore concentrate on the case
$j\not\in I_p$.
It would be nice if we could just write the statement
<!-- environment: verbatim start embedded generator -->
</p>
y(i) = y(i) + a(i,j)*x(j)
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
and some lower layer would automatically transfer 
 <tt>x(j)</tt>  from
whatever processor it is stored on to a local register. (The PGAS
languages of section&nbsp;
2.6.5
 aim to do this, but their
efficiency is far from guaranteed.) An implementation, based on this
optimistic view of parallelism, is given in figure&nbsp;
6.1
.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: displayprocedure start embedded generator -->
<p>\KwIn{Processor number&nbsp;$p$; the elements $x\_i$ with $i\in I\_p$; matrix      elements $A\_{ij}$ with $i\in I\_p$.}    \KwOut{The elements $y\_i$ with $i\in I\_p$}    \For{$i\in I\_p$}{$s\leftarrow0$\;      \For{$j\in I\_p$}{$s\leftarrow s+a\_{ij}x\_{j}$}      \For{$j\not\in I\_p$}{send $x\_j$ from the processor that owns it to        the current one, then\</p>
$s\leftarrow s+a\_{ij}x\_{j}$}      $y\_i\leftarrow s$    }
<!-- environment: displayprocedure end embedded generator -->
<p name="switchToTextMode">
{Naive Parallel MVP}{$A,x\sublocal,y\sublocal,p$}
<p name="caption">
FIGURE 6.1: A na\"\i vely coded parallel matrix-vector product
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The immediate problem with such a
`local' approach is that too much communication will take place.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
If the matrix $A$ is dense, the element $x_j$ is necessary once
  for each row $i\in I_p$, and it will thus be fetched once for every
  row&nbsp;$i\in I_p$.
<li>
For each processor $q\not=p$, there will be (large) number of
  elements $x_j$ with $j\in I_q$ that need to be transferred from
  processor&nbsp;$q$ to&nbsp;$p$. Doing this in separate messages, rather than
  one bulk transfer, is very wasteful.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
With shared memory these issues are not much of a problem, but in the
context of distributed memory it is better to take a
<i>buffering</i>
 approach.
</p>

<p name="switchToTextMode">
Instead of communicating individual elements of&nbsp;$x$, we use a local
buffer $B_{pq}$ for each processor&nbsp;$q\not=p$ where we collect the
elements from&nbsp;$q$ that are needed to perform the product on&nbsp;$p$. (See
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/distmvp.jpeg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{The parallel matrix-vector product with a blockrow
    distribution.}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
figure&nbsp;
6.2
 for an illustration.) The parallel algorithm
is given in figure&nbsp;
6.3
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: displayprocedure start embedded generator -->
<p>\KwIn{Processor number&nbsp;$p$; the elements $x\_i$ with $i\in I\_p$; matrix    elements $A\_{ij}$ with $i\in I\_p$.}  \KwOut{The elements $y\_i$ with $i\in I\_p$}  \For{$q\not=p$}{Send elements of&nbsp;$x$ from processor $q$ to&nbsp;$p$,    receive in buffer&nbsp</p>
$B\_{pq}$.}  $y\sublocal\leftarrow A x\sublocal$\\  \For{$q\not=p$}{$y\sublocal\leftarrow y\sublocal+A\_{pq}B\_q$}
<!-- environment: displayprocedure end embedded generator -->
<p name="switchToTextMode">
{Parallel MVP}{$A,x\sublocal,y\sublocal,p$}
  \caption{A buffered implementation of the parallel matrix-vector
    product}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

In addition to preventing an element from being fetched more than
once, this also combines many small messages into one large message,
which is usually more efficient; recall our discussion of bandwidth
and latency in section&nbsp;
2.7.8
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Give pseudocode for the matrix-vector product using
  nonblocking operations (section&nbsp;
2.6.3.6
)
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Above we said that having a copy of the whole of&nbsp;$x$ on each processor
was wasteful in space. The implicit argument here is that, in general,
we do not want local storage to be function of the number of
processors: ideally it should be only a function of the local
data. (This is related to weak scaling; section&nbsp;
2.2.5
.)
</p>

<p name="switchToTextMode">
You see that, because of communication considerations, we have
actually decided that it is unavoidable, or at least preferable, for
each processor to store the whole input vector.  Such trade-offs
between space and time efficiency are fairly common in parallel
programming. For the dense matrix-vector product we can actually
defend this overhead, since the vector storage is of lower order than
the matrix storage, so our over-allocation is small by ratio. Below
(section&nbsp;
6.5
), we will see that for the sparse
matrix-vector product the overhead can be much less.
</p>

<p name="switchToTextMode">
It is easy to see that the parallel dense matrix-vector product, as
described above, has perfect speedup 
<i>if we are allowed to ignore the time for communication</i>
. In the next couple of sections you will
see that the block row implementation above is not optimal if we take
communication into account. For scalability we need a two-dimensional
decomposition. We start with a discussion of collectives.
</p>

<h3><a id="Scalabilityofthedensematrix-vectorproduct">6.2.2</a> Scalability of the dense matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a>
</p>

<p name="switchToTextMode">

</p>

<p name="switchToTextMode">
In this section, we will give a full analysis of the parallel
computation of $ y \becomes A x $,
where $ x, y \in \Rn $ and $ A \in \Rnxn $.
We will assume that $ p $ nodes will be used, but we make no
assumptions on their connectivity. We will see that the way the matrix
is distributed makes a big difference for the scaling of the
algorithm; for the original research
see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#HeWo:94,Schreiber:scalability92,Stewart90">[HeWo:94,Schreiber:scalability92,Stewart90]</a>
, and see
section&nbsp;
2.2.5
 for the definitions of the various forms of
scaling.
</p>

<h4><a id="Matrix-vectorproduct,partitioningbyrows">6.2.2.1</a> Matrix-vector product, partitioning by rows</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbyrows">Matrix-vector product, partitioning by rows</a>
</p>

<p name="switchToTextMode">

Partition
\[
A \rightarrow \defcolvector{A}{p}
\quad
x \rightarrow \defcolvector{x}{p} ,
\quad
\mbox{and}
\quad
y \rightarrow \defcolvector{y}{p} ,
\]
where $ A_i \in \bbR^{m_i \times n} $ and $ x_i, y_i \in \bbR^{m_i} $ with
$ \sum_{i=0}^{p-1} m_i = n $ and $ m_i \approx n / p $.
We will start by assuming
that $ A_i $, $ x_i $, and $ y_i $ are originally assigned to $ {\cal P}_i $.
</p>

<p name="switchToTextMode">
The computation is characterized by the fact that each processor needs
the whole vector&nbsp;$x$, but owns only an $n/p$ fraction of it. Thus, we
execute an 
<i>allgather</i>
 of&nbsp;$x$. After this, the processor can
execute the local product $y_i\becomes A_ix$; no further communication
is needed after that.
</p>

<p name="switchToTextMode">
An algorithm with cost computation
for $ y = A x $ in parallel is then given by
</p>

<p name="switchToTextMode">
\[
 \vcenter{\hskip\unitindent
<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
Step </td><td> Cost (lower bound) \ \whline
Allgather $ x_i $ so that $ x $ is available on all nodes </td><td>
$ \lceil \log_2(p)\rceil \alpha + \frac{p-1}{p} n \beta
  \approx \log_2(p) \alpha + n \beta $ </td></tr>
<tr><td>
Locally compute $ y_i = A_i x $ </td><td>
$ \approx 2 \frac{n^2}{p} \gamma $ \ </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
}
\]
</p>

<p name="switchToTextMode">
\paragraph*{Cost analysis}
</p>

<p name="switchToTextMode">
The total cost of the algorithm is given by, approximately,
\[
T_p(n) = T_p^{\mbox{1D-row}}(n) =
2 \frac{n^2}{p} \gamma +
\begin{array}[t]{c}
\underbrace{\log_2(p) \alpha + n \beta.}
\\
\mbox{Overhead}
\end{array}
\]
Since the sequential cost is $ T_1(n) = 2 n^2 \gamma $, the speedup is given by
\[
S_p^{\mbox{1D-row}}(n) =
\frac{T_1(n)}
{T_p^{\mbox{1D-row}}(n)} =
\frac{2 n^2 \gamma}
{ 2 \frac{n^2}{p} \gamma +
\log_2(p) \alpha + n \beta}
=
\frac{p}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{p}{2 n} \frac{\beta}{\gamma} }
\]
and the parallel efficiency by
\[
E_p^{\mbox{1D-row}}(n) =
\frac{S_p^{\mbox{1D-row}}(n)}{p}
=
\frac{1}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{p}{2 n} \frac{\beta}{\gamma} }.
\]
</p>

<h5><a id="Anoptimist'sview">6.2.2.1.1</a> An optimist's view</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbyrows">Matrix-vector product, partitioning by rows</a> > <a href="parallellinear.html#Anoptimist'sview">An optimist's view</a>
</p>
<p name="switchToTextMode">

Now, if one fixes $ p $ and lets $ n $ get large,
\[
\lim_{n \rightarrow \infty} E_p( n ) =
\lim_{n \rightarrow \infty}
\left[
\frac{1}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{p}{2 n} \frac{\beta}{\gamma} }
\right]
=
1.
\]
Thus, if one can make the problem large enough, eventually the parallel
efficiency is nearly perfect. However, this assumes unlimited memory,
so this analysis is not practical.
</p>

<p name="switchToTextMode">

</p>

<h5><a id="Apessimist'sview">6.2.2.1.2</a> A pessimist's view</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbyrows">Matrix-vector product, partitioning by rows</a> > <a href="parallellinear.html#Apessimist'sview">A pessimist's view</a>
</p>
<p name="switchToTextMode">

In a 
<i>strong scalability</i>
 analysis,
one fixes $ n $ and lets $ p $ get large, to get
\[
\lim_{p \rightarrow \infty} E_p( n ) =
\lim_{p \rightarrow \infty}
\left[
\frac{1}
{1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{p}{2 n} \frac{\beta}{\gamma} }
\right]
=
0.
\]
Thus, eventually the parallel efficiency becomes nearly nonexistent.
</p>

<h5><a id="Arealist'sview">6.2.2.1.3</a> A realist's view</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbyrows">Matrix-vector product, partitioning by rows</a> > <a href="parallellinear.html#Arealist'sview">A realist's view</a>
</p>
<p name="switchToTextMode">

In a more realistic view we increase the number of processors with the
amount of data. This is called 
<i>weak scalability</i>
, and
it makes the amount of memory that is available to
store the problem scale linearly with&nbsp;$ p $.
</p>

<p name="switchToTextMode">
Let $ M $ equal the
number of floating point numbers that can be stored in a single node's
memory.  Then the aggregate memory is given by $ M p $.  Let $
n_{\max}(p) $ equal the largest problem size that can be stored
in the aggregate memory of $ p $ nodes.  Then, if {\em all} memory can
be used for the matrix,
\[
(n_{\max}(p))^2 = M p
\quad
\mbox{or}
\quad
n_{\max}(p) = \sqrt{Mp}.
\]
The question now becomes what the parallel
efficiency for the largest problem that can be stored on $ p $ nodes:
\[
\begin{array}{r@{{}={}}l}
E_p^{\mbox{1D-row}}(n_{\max}(p)) &
\frac{1}
{1 + \frac{p \log_2(p)}{2 (n_{\max}(p))^2} \frac{\alpha}{\gamma}
+ \frac{p}{2 n_{\max}(p)} \frac{\beta}{\gamma} }
\\
&
\frac{1}
{ 1 + \frac{\log_2(p)}{2 M} \frac{\alpha}{\gamma}
+ \frac{\sqrt{p}}{2 \sqrt{M}} \frac{\beta}{\gamma} }.
\end{array}
\]
Now, if one analyzes what happens when the number of nodes
becomes large, one finds that
\[
\lim_{p \rightarrow \infty} E_p( n_{\max}(p) )
=
\lim_{p \rightarrow \infty}
\left[
\frac{1}
{1 + \frac{\log_2(p)}{2 M} \frac{\alpha}{\gamma}
+ \frac{\sqrt{p}}{2 \sqrt{M}} \frac{\beta}{\gamma} }
\right]
=
0.
\]
Thus, this parallel algorithm for matrix-vector multiplication does
not scale.
</p>

<p name="switchToTextMode">
If you take a close look at this expression for efficiency,
you'll see that the main problem is the $1/\sqrt p$ part of the
expression. This terms involves a factor&nbsp;$\beta$, and if you follow
the derivation backward you see that it comes from the time to send
data between the processors. Informally this can be described as
saying that the message size is too large to make the problem
scalable. In fact, the message size is constant&nbsp;$n$, regardless the
number of processors.
</p>

<p name="switchToTextMode">
Alternatively, a realist realizes that there is a limited amount of time,
$ T_{\max} $, to get a computation done.
Under the best of circumstances,
that is, with zero communication overhead, the largest problem
that we can solve in time $ T_{\max} $ is given by
\[
T_p(n_{\max}(p)) = 2 \frac{(n_{\max}(p))^2}{p} \gamma = T_{\max} .
\]
Thus
\[
(n_{\max}(p))^2 = \frac{T_{\max} p}{2 \gamma}
\quad
\mbox{or}
\quad
n_{\max}(p) = \frac{\sqrt{T_{\max}} \sqrt{p}}{\sqrt{2 \gamma}}.
\]
Then the parallel efficiency that is attained by the algorithm for the largest
problem that can be solved in time $ T_{\max} $ is given by
\[
  E_{p,n_{\max}}=\frac1
  {1+\frac{\log_2p}T\alpha+\sqrt{\frac pT\frac \beta\gamma}}
\]
and the parallel efficiency as the number of nodes becomes large approaches
\[
\lim_{p\rightarrow\infty}E_p= \sqrt{\frac{T\gamma}{p\beta}}.
\]
Again, efficiency cannot be maintained as the number of processors
increases and the execution time is capped.
</p>

<p name="switchToTextMode">
We can also compute the 
<i>iso-efficiency curve</i>
 for this
operation, that is, the relationship between $n,p$ for which the
efficiency stays constant (see section&nbsp;
2.2.5.1
). If
we simplify the efficiency above as
$E(n,p)=\frac{2\gamma}{\beta}\frac{n}{p}$, then $E\equiv c$ is
equivalent to $n=O(p)$ and therefore
\[
 M=O\left(\frac{n^2}{p}\right)=O(p). 
\]
Thus, in order to maintain efficiency we need to increase the memory
per processors pretty quickly. This makes sense, since that downplays
the importance of the communication.
</p>

<h4><a id="Matrix-vectorproduct,partitioningbycolumns">6.2.2.2</a> Matrix-vector product, partitioning by columns</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbycolumns">Matrix-vector product, partitioning by columns</a>
</p>
<p name="switchToTextMode">

Partition
\[
A \rightarrow \defrowvector{A}{p}
\quad
x \rightarrow \defcolvector{x}{p} ,
\quad
\mbox{and}
\quad
y \rightarrow \defcolvector{y}{p} ,
\]
where $ A_j \in \bbR^{n \times n_j} $ and $ x_j, y_j \in \bbR^{n_j} $ with
$ \sum_{j=0}^{p-1} n_j = n $ and $ n_j \approx n / p $.
</p>

<p name="switchToTextMode">
We will start by assuming that $ A_j $, $ x_j $, and $ y_j $ are
originally assigned to $ {\cal P}_j $ (but now $ A_i $ is a block of
columns). In this algorithm by columns, processor&nbsp;$i$ can compute the
length&nbsp;$n$ vector $A_ix_i$ without prior communication. These partial
results then have to be added together
\[
 y\leftarrow \sum_i A_ix_i 
\]
in a 
<i>reduce-scatter</i>
 operation: each processor&nbsp;$i$ scatters
a part $(A_ix_i)_j$ of its result to processor&nbsp;$j$. The receiving
processors then perform a reduction, adding all these fragments:
\[
 y_j = \sum_i (A_ix_i)_j. 
\]
</p>

<p name="switchToTextMode">
The algorithm with costs is then given by:
</p>

<p name="switchToTextMode">
\[
 \vcenter{\hskip\unitindent
\setbox0=\hbox{Reduce-scatter the $ y^{(j)} $s so that $ y_i = \sum_{j=0}^{p-1}
y_i^{(j)} $ is on $ {\cal P}_i $ }
\dimen0=\wd0
\setbox1=\hbox{$ \lceil \log_2(p)\rceil \alpha + \frac{p-1}{p} n \beta
+ \frac{p-1}{p} n \gamma $ }
\dimen1=\wd1
<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
Step </td><td> Cost (lower bound) \ \whline
Locally compute $ y^{(j)} = A_j x_j $ </td><td>
$ \approx 2 \frac{n^2}{p} \gamma $ </td></tr>
<tr><td>
Reduce-scatter the $ y^{(j)} $s so that $ y_i = \sum_{j=0}^{p-1}
y_i^{(j)} $ is on $ {\cal P}_i $ </td><td>
$ \lceil \log_2(p)\rceil \alpha + \frac{p-1}{p} n \beta
+ \frac{p-1}{p} n \gamma $</td></tr>
<tr><td>
</td><td> $ \approx \log_2(p) \alpha + n ( \beta + \gamma ) $ </td></tr>
<tr><td>
</td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
}
\]
</p>

<h5><a id="Costanalysis">6.2.2.2.1</a> Cost analysis</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Matrix-vectorproduct,partitioningbycolumns">Matrix-vector product, partitioning by columns</a> > <a href="parallellinear.html#Costanalysis">Cost analysis</a>
</p>
<p name="switchToTextMode">

The total cost of the algorithm is given by, approximately,
\[
T_p^{\mbox{1D-col}}(n) =
2 \frac{n^2}{p} \gamma +
\begin{array}[t]{c}
\underbrace{\log_2(p) \alpha + n ( \beta + \gamma ).}
\\
\mbox{Overhead}
\end{array}
\]
Notice that this is identical to the cost
$ T_p^{\mbox{1D-row}}(n)  $, except with $ \beta $ replaced by $ (\beta + \gamma )$.  It is not hard to see that the conclusions about scalability
are the same.
</p>

<p name="switchToTextMode">

<h4><a id="Two-dimensionalpartitioning">6.2.2.3</a> Two-dimensional partitioning</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Two-dimensionalpartitioning">Two-dimensional partitioning</a>
</p>

</p>

<p name="switchToTextMode">
Next,
partition
\[
A \rightarrow \defmatrix{A}{p}{p}
\quad
x \rightarrow \defcolvector{x}{p} ,
\quad
\mbox{and}
\quad
y \rightarrow \defcolvector{y}{p} ,
\]
where $ A_{ij} \in \bbR^{n_i \times n_j} $ and $ x_i, y_i \in \bbR^{n_i} $ with
$ \sum_{i=0}^{p-1} n_i = N $ and $ n_i \approx N / \sqrt P $.
</p>

<p name="switchToTextMode">
We will view the nodes as an $ r \times c $ mesh, with $ P = r c $,
and index them as $p_{ij}$,
with $ i=0, &hellip;, r-1 $ and $ j = 0,&hellip;, c-1 $.
Figure&nbsp;
6.4
, for a $12\times12$ matrix
on a $3\times4$ processor grid,
illustrates the assignment of data to nodes, where the $ i,j$
`cell' shows the matrix and vector elements owned by  $ p_{ij} $.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
{\footnotesize
\[
\begin{array}{| c | c | c | c |} \hline
\begin{array}{c c c c}
x_0\\
a_{00} & a_{01} &a_{02} & y_0\\
a_{10} & a_{11} &a_{12} & \\
a_{20} & a_{21} &a_{22} & \\
a_{30} & a_{31} &a_{32} & \\
\end{array}
&
\begin{array}{c c c c}
x_3\\
a_{03} & a_{04} &a_{05} & \\
a_{13} & a_{14} &a_{15} & y_1\\
a_{23} & a_{24} &a_{25} & \\
a_{33} & a_{34} &a_{35} & \\
\end{array}
&
\begin{array}{c c c c}
x_6\\
a_{06} & a_{07} &a_{08} & \\
a_{16} & a_{17} &a_{18} & \\
a_{26} & a_{27} &a_{28} & y_2 \\
a_{37} & a_{37} &a_{38} & \\
\end{array}
&
\begin{array}{c c c c}
x_9\\
a_{09} & a_{0,10} & a_{0,11} & \\
a_{19} & a_{1,10} & a_{1,11} & \\
a_{29} & a_{2,10} & a_{2,11} & \\
a_{39} & a_{3,10} & a_{3,11} & y_3 \\
\end{array}
\\ \hline
\begin{array}{c c c c}
& x_1\\
a_{40} & a_{41} &a_{42} & y_4\\
a_{50} & a_{51} &a_{52} & \\
a_{60} & a_{61} &a_{62} & \\
a_{70} & a_{71} &a_{72} & \\
\end{array}
&
\begin{array}{c c c c}
& x_4\\
a_{43} & a_{44} &a_{45} & \\
a_{53} & a_{54} &a_{55} & y_5\\
a_{63} & a_{64} &a_{65} & \\
a_{73} & a_{74} &a_{75} & \\
\end{array}
&
\begin{array}{c c c c}
& x_7\\
a_{46} & a_{47} &a_{48} & \\
a_{56} & a_{57} &a_{58} & \\
a_{66} & a_{67} &a_{68} & y_6 \\
a_{77} & a_{77} &a_{78} & \\
\end{array}
&
\begin{array}{c c c c}
& x_{10}\\
a_{49} & a_{4,10} & a_{4,11} & \\
a_{59} & a_{5,10} & a_{5,11} & \\
a_{69} & a_{6,10} & a_{6,11} & \\
a_{79} & a_{7,10} & a_{7,11} & y_7 \\
\end{array}
\\ \hline
\begin{array}{c c c c}
&&x_2\\
a_{80} &  a_{81} &  a_{82} & y_8\\
a_{90} &  a_{91}   &a_{92} & \\
a_{10,0} &a_{10,1} &a_{10,2} & \\
a_{11,0} &a_{11,1} &a_{11,2} & \\
\end{array}
&
\begin{array}{c c c c}
&&x_5\\
a_{83} &   a_{84} &  a_{85} & \\
a_{93} &   a_{94} &  a_{95} & y_9\\
a_{10,3} & a_{10,4} &a_{10,5} & \\
a_{11,3} & a_{11,4} &a_{11,5} & \\
\end{array}
&
\begin{array}{c c c c}
&&x_8\\
a_{86} &   a_{87} &  a_{88} & \\
a_{96} &   a_{97} &  a_{98} & \\
a_{10,6} & a_{10,7} &a_{10,8} & y_{10} \\
a_{11,7} & a_{11,7} &a_{11,8} & \\
\end{array}
&
\begin{array}{c c c c}
&&x_{11}\\
a_{89} &   a_{8,10} &  a_{8,11} & \\
a_{99} &   a_{9,10} &  a_{9,11} & \\
a_{10,9} & a_{10,10} & a_{10,11} & \\
a_{11,9} & a_{11,10} & a_{11,11} & y_{11} \\
\end{array}
\\ \hline
\end{array}
\]
}
<p name="caption">
FIGURE 6.4: Distribution of matrix and vector elements for a problem of size 12 on a $4\times 3$ processor grid
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

In other words, $ p_{ij} $ owns the matrix block $A_{ij}$
and parts of $x$ and&nbsp;$y$. This makes possible the following algorithm
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
{This figure shows a partitioning of the matrix into contiguous blocks,
and the vector distribution seem to be what is necessary to work with this
matrix distribution. You could also look at this story the other way:
start with a distribution of input and output vector, and then decide what
that implies for the matrix distribution. For instance, if you distributed
$x$ and&nbsp;$y$ the same way, you would arrive at a different matrix distribution,
but otherwise the product algorithm would be much the same;
see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Flame:PBMD-report">[Flame:PBMD-report]</a>
.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Since $x_j$ is distributed over the $j$th column, the algorithm starts
  by collecting $x_j$ on each processor $p_{ij}$ by an
<i>allgather</i>
 inside the processor columns.
<li>
Each processor $p_{ij}$ then computes $y_{ij} = A_{ij}x_j$. This
  involves no further communication.
<li>
The result $y_i$ is then collected by gathering together the
  pieces $y_{ij}$ in each processor row to form&nbsp;$y_i$, and this is then
  distributed over the processor row. These two operations are in fact
  combined to form a 
<i>reduce-scatter</i>
.
<li>
If $r=c$, we can transpose the $y$ data over the processors, so
  that it can function as the input for a subsequent matrix-vector
  product. If, on the other hand, we are computing $A^tAx$, then $y$
  is now correctly distributed for the $A^t$ product.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

</p>

<h5><a id="Algorithm">6.2.2.3.1</a> Algorithm</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Two-dimensionalpartitioning">Two-dimensional partitioning</a> > <a href="parallellinear.html#Algorithm">Algorithm</a>
</p>
<p name="switchToTextMode">

The algorithm with cost analysis is
\[
 \vcenter{\hskip\unitindent
\setbox0=\hbox{Perform local matrix-vector multiply }
\dimen0=\wd0
\setbox1=\hbox{$ \lceil \log_2(c)\rceil \alpha + \frac{c-1}{p} n \beta +
\frac{c-1}{p} n \gamma $ }
\dimen1=\wd1
<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
Step </td><td> Cost (lower bound) \ \whline
Allgather $ x_i $'s  within columns </td><td>
$ \lceil \log_2(r)\rceil \alpha + \frac{r-1}{p} n \beta$</td></tr>
<tr><td>
</td><td> $ \approx \log_2(r) \alpha + \frac{n}{c} \beta $ </td></tr>
<tr><td>
Perform local matrix-vector multiply </td><td>
$ \approx 2 \frac{n^2}{p} \gamma $ </td></tr>
<tr><td>
Reduce-scatter $ y_i $'s  within rows </td><td>
$ \lceil \log_2(c)\rceil \alpha + \frac{c-1}{p} n \beta +
\frac{c-1}{p} n \gamma $</td></tr>
<tr><td>
</td><td> $ \approx \log_2(c) \alpha + \frac{n}{r} \beta + \frac{n}{r} \gamma
$ </td></tr>
<tr><td>
</td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">
}
\]
</p>

<h5><a id="Costanalysis">6.2.2.3.2</a> Cost analysis</h5>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Paralleldensematrix-vectorproduct">Parallel dense matrix-vector product</a> > <a href="parallellinear.html#Scalabilityofthedensematrix-vectorproduct">Scalability of the dense matrix-vector product</a> > <a href="parallellinear.html#Two-dimensionalpartitioning">Two-dimensional partitioning</a> > <a href="parallellinear.html#Costanalysis">Cost analysis</a>
</p>
<p name="switchToTextMode">

The total cost of the algorithm is given by, approximately,
\[
T_p^{r \times c}(n) = T_p^{r \times c}( n) =
2 \frac{n^2}{p} \gamma +
\begin{array}[t]{c}
\underbrace{\log_2(p) \alpha + \left( \frac{n}{c} + \frac{n}{r} \right) \beta + \frac{n}{r} \gamma.}
\\
\mbox{Overhead}
\end{array}
\]
We will now make the simplification that $ r = c = \sqrt{p} $ so that
\[
T_p^{\sqrt{p} \times \sqrt{p}}(n) = T_p^{\sqrt{p} \times \sqrt{p}}( n) =
2 \frac{n^2}{p} \gamma +
\begin{array}[t]{c}
\underbrace{\log_2(p) \alpha + \frac{n}{\sqrt{p}}\left( 2 \beta+ \gamma \right)
}
\\
\mbox{Overhead}
\end{array}
\]
</p>

<p name="switchToTextMode">
Since the sequential cost is $ T_1(n) = 2 n^2 \gamma $, the speedup is given by
\[
S_p^{\sqrt{p} \times \sqrt{p}}(n) =
\frac{T_1(n)}
{T_p^{\sqrt{p} \times \sqrt{p}}(n)} =
\frac{2 n^2 \gamma}
{ 2 \frac{n^2}{p} \gamma + \frac{n}{\sqrt{p}}
\left( 2 \beta + \gamma \right)}
=
\frac{p}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{\sqrt{p}}{2n}\frac{
\left( 2 \beta + \gamma \right)}{\gamma}}
\]
and the parallel efficiency by
\[
E_p^{\sqrt{p} \times \sqrt{p}}(n) =
\frac{1}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{\sqrt{p}}{2n}\frac{
\left( 2 \beta + \gamma \right)}{\gamma}}
\]
</p>

<p name="switchToTextMode">
We again ask the question what the parallel
efficiency for the largest problem that can be stored on $ p $ nodes is.
\begin{eqnarray*}
E_p^{\sqrt{p} \times \sqrt{p}}(n_{\max}(p)) &=&
\frac{1}
{ 1 + \frac{p \log_2(p)}{2 n^2} \frac{\alpha}{\gamma}
+ \frac{\sqrt{p}}{2n}\frac{
\left( 2 \beta + \gamma \right)}{\gamma}}
\\
&=&
\frac{1}
{ 1 + \frac{\log_2(p)}{2 M} \frac{\alpha}{\gamma}
+ \frac{1}{2\sqrt{M}}\frac{
\left( 2 \beta + \gamma \right)}{\gamma}}
\end{eqnarray*}
so that still
\begin{eqnarray*}
\lim_{p \rightarrow \infty}
E_p^{\sqrt{p} \times \sqrt{p}}(n_{\max}(p)) &=&
\lim_{p \rightarrow \infty}
\frac{1}
{ 1 + \frac{\log_2(p)}{2 M} \frac{\alpha}{\gamma}
+ \frac{1}{2\sqrt{M}}\frac{
\left( 2 \beta + \gamma \right)}{\gamma}}
=
0.
\end{eqnarray*}
However, $ \log_2{p} $ grows very slowly with $ p $ and is therefore
considered to act much like a constant.  In this case
$ E_p^{\sqrt{p}\times \sqrt{p}}( n_{\rm max}(p) ) $ decreases very slowly and the algorithm is considered to be scalable for practical purposes.
</p>

<p name="switchToTextMode">

Note that when $ r = p $ the 2D algorithm becomes the "partitioned by
rows" algorithm and when $ c = p $ it becomes the "partitioned by
columns" algorithm.  It is not hard to show that the 2D algorithm is
scalable in the sense of the above analysis
when $ r = c $, as long as $r/c$ is kept constant.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Compute the iso-efficiency curve for this operation.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h2><a id="LUfactorizationinparallel">6.3</a> LU factorization in parallel</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#LUfactorizationinparallel">LU factorization in parallel</a>
</p>
<p name="switchToTextMode">

The matrix-vector and matrix-product are easy to parallelize in one sense.
The elements of the output can all be computed independently and in any order,
so we have many degrees of freedom in parallelizing the algorithm.
This does not hold for computing an LU factorization, or solving a linear system
with the factored matrix.
</p>

<h3><a id="Solvingatriangularsystem">6.3.1</a> Solving a triangular system</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#LUfactorizationinparallel">LU factorization in parallel</a> > <a href="parallellinear.html#Solvingatriangularsystem">Solving a triangular system</a>
</p>
<!-- index -->
<p name="switchToTextMode">

The solution of a triangular system $y=L\inv x$ (with $L$&nbsp;is lower triangular)
is a matrix-vector operation, so
it has its $O(N^2)$ complexity in common with the matrix-vector product.
However, unlike the product operation, this solution process contains a recurrence
relation between the output elements:
\[
 y_{i} = \ell_{ii}\inv ( x_i-\sum_{j&lt;i} \ell_{ij}x_j ). 
\]
This means that parallelization is not trivial.
In the case of a sparse matrix special strategies may be possible; see section&nbsp;
6.9
.
Here we will make a few remarks about general, dense case.
</p>

<p name="switchToTextMode">
Let us assume for simplicity that communication takes no time, and
that all arithmetic operations take the same unit time.
First we consider the matrix distribution by rows,
meaning that processor&nbsp;$p$ stores the elements&nbsp;$\ell_{p*}$.
With this we can implement the triangular solution as:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Processor 1 solves $y_1=\ell_{11}\inv x_1$ and sends its value to the next processor.
<li>
In general, processor $p$ gets the values $y_1,&hellip;,y_{p-1}$
  from processor&nbsp;$p-1$, and computes&nbsp;$y_p$;
<li>
Each processor $p$ then sends $y_1,&hellip;,y_p$ to $p+1$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that this algorithm takes time $2N^2$, just like the sequential algorithm.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

This algorithm has each processor passing all computed $y_i$ values to its successor, in a pipeline fashion.
However, this means that processor&nbsp;$p$ receives&nbsp;$y_1$ only at the last moment, whereas that value was computed
already in the first step. We can formulate the solution algorithm in such a way that computed elements
are made available as soon as possible:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Processor&nbsp;1 solve $y_1$, and sends it to all later processors.
<li>
In general, processor&nbsp;$p$ waits for individual messages with values $y_q$ for&nbsp;$q&lt;p$.
<li>
Processor $p$ then computes $y_p$ and sends it to processors&nbsp;$q$ with $q&gt;p$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Under the assumption that communication time is negligible, this algorithm can be much faster.
For instance, all processors&nbsp;$p&gt;1$ receive&nbsp;$y_1$ simultaneously, and can compute $\ell_{p1}y_1$
simultaneously.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that this algorithm variant takes a time $O(N)$,
  if we ignore communication.
  What is the cost if we incorporate communication in the cost?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Now consider the matrix distribution by columns: processor&nbsp;$p$ stores&nbsp;$\ell_{*p}$.
  Outline the triangular solution algorithm with this distribution, and show that the
  parallel solve time is&nbsp;$O(N)$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h3><a id="Factorization,densecase">6.3.2</a> Factorization, dense case</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#LUfactorizationinparallel">LU factorization in parallel</a> > <a href="parallellinear.html#Factorization,densecase">Factorization, dense case</a>
</p>
<!-- index -->

<p name="switchToTextMode">

A full analysis of the scalability of
<i>parallel dense LU factorization</i>
is quite
involved, so we will state without further proof that as in the matrix-vector case a
two-dimensional distribution is needed. However, we can identify a
further complication. Since factorizations of any
type
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {Gaussian elimination can be performed in right-looking,
    left-looking and Crout variants; see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#TSoPMC">[TSoPMC]</a>
.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
progress through a matrix, processors will be inactive for part of the
time.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the regular right-looking Gaussian elimination
<!-- environment: verbatim start embedded generator -->
</p>
for k=1..n
  p = 1/a(k,k)
  for i=k+1,n
    for j=k+1,n
      a(i,j) = a(i,j)-a(i,k)*p*a(k,j)
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
  Analyze the running time, speedup, and efficiency as a function
  of&nbsp;$N$, if we assume a one-dimensional distribution, and enough
  processors to store one column per processor. Show that speedup is
  limited.
</p>

<p name="switchToTextMode">
  Also perform this analysis for a two-dimensional decomposition where
  each processor stores one element.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">
For this reason, an 
<i>overdecomposition</i>
 is used, where
the matrix is divided in more blocks than there are processors,
and each processor stores several, non-contiguous, sub-matrices.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/cyclic-1.jpg" width=800></img>
<p name="switchToTextMode">
  \caption{One-dimensional cyclic distribution: assignment of four
    matrix columns to two processors, and the resulting mapping of
    storage to matrix columns}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
We illustrate this in figure&nbsp;
6.5
 where we divide four
block columns of a matrix to two processors: each processor stores in
a contiguous block of memory two non-contiguous matrix columns.
</p>

<p name="switchToTextMode">
Next, we illustrate in figure&nbsp;
6.6
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/cyclic-1-mvp.jpg" width=800></img>
<p name="caption">
FIGURE 6.6: Matrix-vector multiplication with a cyclicly distributed matrix
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
that a matrix-vector product with such a matrix can be performed
without knowing that the processors store non-contiguous parts of the
matrix. All that is needed is that the input vector is also cyclicly
distributed.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Now consider a $4\times4$ matrix and a $2\times2$ processor
  grid. Distribute the matrix cyclicly both by rows and columns. Show
  how the matrix-vector product can again be performed using the
  contiguous matrix storage, as long as the input is distributed
  correctly. How is the output distributed? Show that more
  communication is needed than the reduction of the one-dimensional
  example.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Specifically,
with $P&lt;N$ processors, and assuming for simplicity $N=cP$, we let
processor&nbsp;0 store rows $0,c,2c,3c,&hellip;$; processor&nbsp;1 stores rows
$1,c+1,2c+1,&hellip;$, et cetera. This scheme can be generalized two a
two-dimensional distribution, if $N=c_1P_1=c_2P_2$
and&nbsp;$P=P_1P_2$. This is called a 2D 
  distribution}. This scheme can be further extended by considering
block rows and columns (with a small block size), and assigning to
processor&nbsp;0 the 
<i>block</i>
 rows $0,c,2c,&hellip;$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider a square $n\times n$ matrix, and a square $p\times p$
  processor grid, where $p$ divides&nbsp;$n$ without remainder. Consider
  the overdecomposition outlined above, and make a sketch of matrix
  element assignment for the specific case $n=6,p=2$. That is, draw an
  $n\times n$ table where location $(i,j)$ contains the processor
  number that stores the corresponding matrix element. Also make a
  table for each of the processors describing the local to global mapping,
  that is, giving the global $(i,j)$ coordinates of the elements in
  the local matrix. (You will find this task facilitated by using
  zero-based numbering.)
</p>

<p name="switchToTextMode">
  Now write functions $P,Q,I,J$ of $i,j$ that describe the global to
  local mapping, that is, matrix element $a_{ij}$ is stored in
  location $(I(i,j),J(i,j))$ on processor $(P(i,j),Q(i,j))$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Factorization,sparsecase">6.3.3</a> Factorization, sparse case</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#LUfactorizationinparallel">LU factorization in parallel</a> > <a href="parallellinear.html#Factorization,sparsecase">Factorization, sparse case</a>
</p>
<!-- index -->
</p>

<i>Sparse matrix LU factorization in parallel</i>
<p name="switchToTextMode">
difficult topic. Any type of factorization involves a sequential
component, making it non-trivial to begin with. To see the problem
with the sparse case in particular, suppose you process
the matrix rows sequentially. The dense case has enough elements per
row to derive parallelism there, but in the sparse case that number
may be very low.
</p>

<p name="switchToTextMode">
The way out of this problem is to realize that we are not interested
in the factorization as such, but rather that we can use it to solve a
linear system. Since permuting the matrix gives the same solution,
maybe itself permuted, we can explore permutations that have a higher
degree of parallelism.
</p>

<p name="switchToTextMode">
The topic of matrix orderings already came up in
section&nbsp;
5.4.3.5
, motivated by fill-in reduction.
We will consider orderings with favorable parallelism properties
below: nested dissection in section&nbsp;
6.8.1
, and
multi-color orderings in section&nbsp;
6.8.2
.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Matrix-matrixproduct">6.4</a> Matrix-matrix product</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Matrix-matrixproduct">Matrix-matrix product</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
The 
<i>matrix-matrix product</i>
 $C\leftarrow A\cdot B$
(or $C\leftarrow A\cdot B+\gamma C$, as it is used in the 
<span title="acronym" ><i>BLAS</i></span>
)
has a simple parallel structure. Assuming all square matrices of size
$N\times N$
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
the $N^3$ products $a_{ik}b_{kj}$ can be computed independently,
  after which
<li>
the $N^2$ elements $c_{ij}$ are formed by independent sum
  reductions, each of which take a time&nbsp;$\log_2 N$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
However, considerably more interesting are the questions of data
movement:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
With $N^3$ operations on $O(N^2)$ elements, there is
  considerable opportunity for
  
<i>data reuse</i>
%
<!-- index -->
<!-- index -->
  (section&nbsp;
1.6.1
). We will explore the `Goto algorithm' in
  section&nbsp;
6.4.1
.
<li>
Depending on the way the matrices are traversed, 
<span title="acronym" ><i>TLB</i></span>
 reuse
  also needs to be considered (section&nbsp;
1.3.8.2
).
<li>
The distributed memory version of the matrix-matrix product is
  especially tricky. Assuming $A,B,C$ are all distributed over a
  processor grid, elements of $A$ have to travel through a processor
  row, and elements of $B$&nbsp;through a processor columns. We discuss
  `Cannon's algorithm' and the outer-product method below.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- TranslatingLineGenerator file ['file'] -->
</p>

<h3><a id="Gotomatrix-matrixproduct">6.4.1</a> Goto matrix-matrix product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Matrix-matrixproduct">Matrix-matrix product</a> > <a href="parallellinear.html#Gotomatrix-matrixproduct">Goto matrix-matrix product</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In section~
1.6.1
 we argued that the
<i>matrix matrix product</i>
(or 
<tt>dgemm</tt>
 in 
<i>BLAS</i>
 terms) has a large amount of
possible data reuse: there are $O(n^3)$ operations on $O(n^2)$ data.
We will now consider an implementation, due
to 
<i>Kazushige Goto</i>
~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#GotoGeijn:2008:Anatomy">[GotoGeijn:2008:Anatomy]</a>
, that
indeed achieves close to peak performance.
</p>

<p name="switchToTextMode">
The matrix-matrix algorithm has three loops, each of which we can block,
giving a six-way nested loop.
Since there are no recurrences on the output elements, all resulting
loop exchanges are legal. Combine this with the fact that the loop blocking
introduces three blocking parameters, and you'll see that the number of
potential implementations is enormous. Here we present the global reasoning
that underlies the Goto implementation; for a detailed discussion see the paper cited.
</p>

<p name="switchToTextMode">
We start by writing the product $C\leftarrow A\cdot B$ (or
  $C\leftarrow C+AB$ according to the Blas standard) as a sequence of
  low-rank updates:
\[
 C_{**} = \sum_k  A_{*k}B_{k*} 
\]
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/gotoblas1.jpeg" width=800></img>
<p name="caption">
FIGURE 6.7: Matrix-matrix multiplication as a sequence of low-rank updates
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
See figure~
6.7
. Next we derive the `block-panel' multiplication
by multiplying a block of~$A$ by a `sliver' of~$B$; see figure~
6.8
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/gotoblas2.jpeg" width=800></img>
<p name="caption">
FIGURE 6.8: The block-panel multiplication in the matrix-matrix algorithm
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Finally, the inner algorithm accumulates a small row $C_{i,*}$,
typically of small size such as~4, by accumulating:
<!-- environment: verbatim start embedded generator -->
</p>
// compute C[i,*] :
for k:
   C[i,*] = A[i,k] * B[k,*]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
See figure&nbsp;
6.9
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/gotoblas3.jpeg" width=800></img>
<p name="caption">
FIGURE 6.9: The register-resident kernel of the matrix-matrix multiply
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Now this algorithm is tuned.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
We need enough registers for 
 <tt>C[i,*]</tt> , 
 <tt>A[i,k]</tt>  and
 <tt>B[k,*]</tt> . On current processors that means that we accumulate
  four elements of&nbsp;$C$.
<li>
Those elements of&nbsp;$C$ are accumulated, so they stay in register
  and the only data transfer is loading of the elements of $A$
  and&nbsp;$B$; there are no stores!
<li>
The elements 
 <tt>A[i,k]</tt>  and 
 <tt>B[k,*]</tt>  stream from L1.
<li>
Since the same block of&nbsp;$A$ is used for many successive slivers
  of&nbsp;$B$, we want it to stay resident; we choose the blocksize of $A$
  to let it stay in L2 cache.
<li>
In order to prevent TLB problems, $A$&nbsp;is stored by rows. If we
  start out with a matrix in (Fortran) 
<i>column-major</i>
  storage, this means we have to make a copy. Since copying is of a
  lower order of complexity, this cost is amortizable.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

This algorithm be implemented in specialized hardware, as in the
<i>Google TPU</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#GoogleTPUpage">[GoogleTPUpage]</a>
, giving great energy efficiency.
</p>

<!-- environment: comment start embedded generator -->

</comment>
<!-- environment: comment end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">

</p>

<h3><a id="Cannon'salgorithmforthedistributedmemorymatrix-matrixproduct">6.4.2</a> Cannon's algorithm for the distributed memory matrix-matrix product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Matrix-matrixproduct">Matrix-matrix product</a> > <a href="parallellinear.html#Cannon'salgorithmforthedistributedmemorymatrix-matrixproduct">Cannon's algorithm for the distributed memory matrix-matrix product</a>
</p>
<!-- index -->
<p name="switchToTextMode">

<!-- environment: wrapfigure start embedded generator -->
</p>
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<img src="graphics/cannon1.jpeg" width=800></img>
<p name="caption">
WRAPFIGURE 6.10: Cannon's algorithm for matrix-matrix multiplication: (a) initial rotation of matrix rows and columns, (b)&nbsp;resulting position in which processor&nbsp;$(i,j)$ can start accumulating&nbsp;$\sum_kA_{ik}B_{kj}$, (c)&nbsp;subsequent rotation of $A,B$ for the next term.
</p>

</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{4in}
In section&nbsp;
6.4.1
 we considered the high performance
implementation of the single-processor 
  product}.  We will now briefly consider the distributed memory
version of this operation.
(It is maybe interesting to note that this
is not a generalization based on the matrix-vector product algorithm
of section&nbsp;
6.2
.)
</p>

<p name="switchToTextMode">
One algorithm for this operation is known as
<i>Cannon's algorithm</i>
<!-- index -->
.
It assumes a square processor grid where processor $(i,j)$ gradually
accumulates the $(i,j)$ block $C_{i,j}=\sum_kA_{i,k}B_{k,j}$;
see figure&nbsp;
6.10
.
</p>

<p name="switchToTextMode">
If you start top-left, you see that processor $(0,0)$ has both
$A_{00}$ and&nbsp;$B_{00}$, so it can immediately start doing a local multiplication.
Processor $(0,1)$ has $A_{01}$ and&nbsp;$B_{01}$, which are
not needed together, but if we rotate the second column of&nbsp;$B$ up
by one position, processor&nbsp;$(0,1)$ will have $A_{01},B_{11}$ and those
two do need to be multiplied. Similarly, we rotate the third column of&nbsp;$B$
up by two places so that $(0,2)$ contains $A_{02},B_{22}$.
</p>

<p name="switchToTextMode">
How does this story go in the second row? Processor&nbsp;$(1,0)$ has
$A_{10},B_{10}$ which are not needed together. If we rotate
the second row of&nbsp;$A$ one position to the left, it contains
$A_{11},B_{10}$ which are needed for a partial product.
And now processor&nbsp;$(1,1)$ has $A_{11},B_{11}$.
</p>

<p name="switchToTextMode">
If we continue this story, we start with a matrix&nbsp;$A$ of which the
rows have been rotated left, and $B$ of which the columns have been rotated up.
In this setup, processor&nbsp;$(i,j)$ contains $A_{i,i+j}$ and&nbsp;$B_{i+j,j}$,
where the addition is done modulo the matrix size. This means that
each processor can start by doing a local product.
</p>

<p name="switchToTextMode">
Now we observe that $C_{ij}=\sum_kA_{ik}B_{kj}$ implies that
the next partial product term comes from increasing&nbsp;$k$ by&nbsp;$1$.
The corresponding elements of $A,B$ can be moved into the processor
by rotating the rows and columns by another location.
</p>

<p name="switchToTextMode">
\Level 1 {The outer-product method for the distributed matrix-matrix
  product}
</p>

<p name="switchToTextMode">
Cannon's algorithm suffered from the requirement of needing a square
processors grid.
The 
<i>outer-product method</i>
%
<!-- index -->
is more general. It is based the following arrangement of the
matrix-matrix product calculation:
<!-- environment: verbatim start embedded generator -->
</p>
for ( k )
  for ( i )
    for ( j )
      c[i,j] += a[i,k] * b[k,j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
That is, for each $k$ we take a column of&nbsp;$A$ and a row of&nbsp;$B$, and computing the
rank-1 matrix (or `outer product') $A_{*,k}\cdot B_{k,*}$. We then sum
over&nbsp;$k$.
</p>

<p name="switchToTextMode">
Looking at the structure of this algorithm, we notice that in
step&nbsp;$k$, each column $j$ receives $A_{*,k}$, and each row $i$
receives&nbsp;$B_{k,*}$. In other words, elements of $A_{*,k}$ are
broadcast through their row, and elements of $B_{k,*}$ are broadcast
through their row.
</p>

<p name="switchToTextMode">
Using the MPI library, these simultaneous broadcasts are realized by
having a subcommunicator for each row and each column.
</p>

<!-- index -->
<!-- index -->
<p name="switchToTextMode">

<h2><a id="Sparsematrix-vectorproduct">6.5</a> Sparse matrix-vector product</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
In linear system solving through iterative methods (see
section&nbsp;
5.5
) the matrix-vector product is
computationally an important kernel, since it is executed in each of
potentially hundreds of iterations. In this section we look at
performance aspects of the matrix-vector product on a single
processor first; the multi-processor case will get our attention in
section&nbsp;
6.5
.
</p>

<h3><a id="Thesingle-processorsparsematrix-vectorproduct">6.5.1</a> The single-processor sparse matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Thesingle-processorsparsematrix-vectorproduct">The single-processor sparse matrix-vector product</a>
</p>

<p name="switchToTextMode">

We are not much worried about the dense matrix-vector product in the
context of iterative methods, since one typically does not iterate on
dense matrices. In the case that we are dealing with block matrices,
refer to section&nbsp;
1.7.11
 for an analysis of the dense
product. The sparse product is a lot trickier, since most of that
analysis does not apply.
</p>

<p name="switchToTextMode">

<b>Data reuse in the sparse matrix-vector product</b><br>

</p>

<p name="switchToTextMode">
There are some similarities between the dense matrix-vector product,
executed by rows, and the 
<span title="acronym" ><i>CRS</i></span>
 sparse product
(section&nbsp;
5.4.1.4
). In both cases all matrix elements are
used sequentially, so any cache line loaded is utilized
fully. However, the 
<span title="acronym" ><i>CRS</i></span>
 product is worse at least the following
ways:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The indirect addressing requires loading the elements of an
  integer vector. This implies that the sparse product has more memory
  traffic for the same number of operations.
<li>
The elements of the source vector are not loaded sequentially,
  in fact they can be loaded in effectively a random order. This means
  that a cacheline contains a source element will likely not be fully
  utilized. Also, the prefetch logic of the memory subsystem
  (section&nbsp;
1.3.5
) cannot assist here.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For these reasons, an application that is computationally dominated by
the sparse matrix-vector product can very well be run at $\approx5\%$
of the peak performance of the processor.
</p>

<p name="switchToTextMode">
It may be possible to improve this performance if the structure of the
matrix is regular in some sense. One such case is where we are dealing
with a 
<i>block matrix</i>
 consisting completely of small dense
blocks. This leads at least to a reduction in the amount of indexing
information: if the matrix consists of $2\times2$ blocks we get a
$4\times$ reduction in the amount of integer data transferred.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Give two more reasons why this strategy is likely to improve
  performance. Hint: cachelines, and reuse.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">
Such a 
<i>matrix tessellation</i>
 may give a factor of&nbsp;2 in
performance improvement. Assuming such an improvement, we may adopt
this strategy even if the matrix is not a perfect block matrix: if
every $2\times2$ block will contain one zero element we may still get
a factor of&nbsp;$1.5$ performance
improvement&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#vuduc:thesis,ButtEijkLang:spmvp">[vuduc:thesis,ButtEijkLang:spmvp]</a>
.
</p>

<p name="switchToTextMode">

<b>Vectorization in the sparse product</b><br>

</p>

<p name="switchToTextMode">
In other circumstances bandwidth and reuse are not the dominant concerns:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
On old vector computers, such as old 
<i>Cray</i>
 machines,
  memory was fast enough for the processor, but vectorization was
  paramount. This is a problem for sparse matrices, since the number
  of zeros in a matrix row, and therefore the vector length, is
  typically low.
<li>
On 
<span title="acronym" ><i>GPUs</i></span>
 memory bandwidth is fairly high, but it is
  necessary to find large numbers of identical operations. Matrix can
  be treated independently, but since the rows are likely of unequal
  length this is not an appropriate source of parallelism.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For these reasons, a variation on the 
<i>diagonal storage</i>
scheme for sparse matrices has seen a revival
recently. The observation here is that
if you sort the matrix rows by the number of rows you get a small
number of blocks of rows; each block will be fairly large, and in each
block the rows have the same number of elements.
</p>

<p name="switchToTextMode">
A matrix with such a structure is good for vector
architectures&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#DAzevedo2005:vector-mvp">[DAzevedo2005:vector-mvp]</a>
. In this case the product
is computed by diagonals.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Write pseudo-code for this case. How did the sorting of the rows
  improve the situation?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

This sorted storage scheme also solves the problem we noted on
<span title="acronym" ><i>GPUs</i></span>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Bolz:GPUsparse">[Bolz:GPUsparse]</a>
. In this case we the traditional
<span title="acronym" ><i>CRS</i></span>
 product algorithm, and we have an amount of parallelism equal
to the number of rows in a block.
</p>

<p name="switchToTextMode">
Of course there is the complication that we have permuted the matrix:
the input and output vectors will need to be permuted accordingly. If
the product operation is part of an iterative method, doing this
permutation back and forth in each iteration will probably negate any
performance gain. Instead we could permute the whole linear system and
iterate on the permuted system.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you think of reasons why this would work? Reasons why it wouldn't?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Theparallelsparsematrix-vectorproduct">6.5.2</a> The parallel sparse matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Theparallelsparsematrix-vectorproduct">The parallel sparse matrix-vector product</a>
</p>
<!-- index -->
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<p name="switchToTextMode">
In section~
5.4
 you saw a first discussion of sparse
matrices, limited to use on a single processor. We will now go into
parallelism aspects.
</p>

<p name="switchToTextMode">
The dense matrix-vector product, as you saw above,
required each processor to communicate with every other, and to have a
local buffer of essentially the size of the global vector. In the
sparse case, considerably less buffer space, as well as less
communication, is needed. Let us analyze this case. We will assume
that the matrix is distributed by block rows, where processor $p$ owns
the matrix rows with indices in some set~$I_p$.
</p>

<p name="switchToTextMode">
The line $y_i=y_i+a_{ij}x_j$ now has to take into account that $a_{ij}$
can be zero. In particular, we need to consider that, for some pairs
$i\in I_p, j\not\in I_p$ no communication will be needed. Declaring
for each $i\in I_p$ a~sparsity pattern set
\[
 S_{p;i}=\{j\colon j\not\in I_p, a_{ij}\not=0\} 
\]
our multiplication instruction becomes
\[
 y_i\mathop{+=}a_{ij}x_j\qquad\hbox{if $j\in S_{p;i}$}. 
\]
If we want to avoid, as above, a flood of small messages, we combine
all communication into a single message per processor. Defining
\[
 S_p = \cup_{i\in I_p} S_{p;i}, 
\]
the algorithm now becomes:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Collect all necessary off-processor elements $x_j$ with $j\in
  S_p$ into one buffer;
<li>
Perform the matrix-vector product, reading all elements of~$x$
  from local storage.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

This whole analysis of course also applies to dense matrices. This
becomes different if we consider where sparse matrices come from.  Let
us start with a simple case.
</p>

<p name="switchToTextMode">
Recall figure~
4.1
, which illustrated a discretized
boundary value problem on the simplest domain, a square, and let us
now parallelize it. We do this by partitioning the domain; each
processor gets the matrix rows corresponding to its subdomain.
Figure~
6.11
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/laplaceparallel.jpeg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{A difference stencil applied to a two-dimensional square
    domain, distributed over processors. A cross-processor connection
    is indicated.}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
shows how this gives rise to connections between processors: the
elements $a_{ij}$ with $i\in I_p,j\not\in I_p$ are now the `legs' of
the stencil that reach beyond a processor boundary. The set of all
such~$j$, formally defined as
\[
 G = \{ j\not\in I_p \colon
    \exists_{i\in I_p}\colon a_{ij}\not=0 \}
\]
is known as the 
<i>ghost region</i>
 of a processor; see
figure~
6.12
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/laplaceghost.jpeg" width=800></img>
<p name="caption">
FIGURE 6.12: The ghost region of a processor, induced by a stencil
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that a one-dimensional partitioning of the domain leads to a
  partitioning of the matrix into block rows, but a two-dimensional
  partitioning of the domain does not. You can do this in the
  abstract, or you can illustrate it: take a $4\times4$ domain (giving
  a matrix of size~16), and partition it over 4 processors. The
  one-dimensional domain partitioning corresponds to giving each
  processor one line out of the domain, while the two-dimensional
  partitioning gives each processor a $2\times2$ subdomain. Draw the
  matrices for these two cases.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bandmat.jpeg" width=800></img>
<p name="caption">
FIGURE 6.13: Band matrix with halfbandwidth $\sqrt N$
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Figure~
6.13
 depicts a sparse matrix of size~$N$ with a
  halfbandwidth~$n=\sqrt N$. That is,
\[
 |i-j|>n \Rightarrow a_{ij}=0. 
\]
  We make a one-dimensional distribution of this matrix over $p$
  processors, where $p=n=\sqrt N$.
</p>

<p name="switchToTextMode">
  Show that the matrix-vector product using
  this scheme is weakly scalable by computing the efficiency
  as function of the number of processors
\[
 E_p = \frac{T_1}{pT_p}. 
\]
</p>

<p name="switchToTextMode">
  Why is this scheme not really weak scaling, as it is commonly defined?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

One crucial observation about the parallel sparse matrix-vector
product
is that, for each processor, the number of
other processors it is involved with is strictly limited. This has
implications for the efficiency of the operation.
</p>

<h3><a id="Parallelefficiencyofthesparsematrix-vectorproduct">6.5.3</a> Parallel efficiency of the sparse matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Parallelefficiencyofthesparsematrix-vectorproduct">Parallel efficiency of the sparse matrix-vector product</a>
</p>

<p name="switchToTextMode">

In the case of the dense matrix-vector product
(section&nbsp;
6.2.2
), partitioning the matrix over the
processors by (block) rows did not lead to a scalable algorithm. Part
of the reason was the increase in the number of neighbors that each
processors needs to communicate with. Figure&nbsp;
6.11
shows that, for the matrix of a 5-five point stencil, this number is
limited to four.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Take a square domain and a partitioning of the variables of the
  processors as in figure&nbsp;
6.11
.
  What is the maximum number of neighbors a
  processor needs to communication with for the box stencil in
  figure&nbsp;
4.3
? In three space dimensions, what is the
  number of neighbors if a 7-point central difference stencil is
  used?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The observation that each processor communicates with just a few
neighbors stays intact if we go beyond square domains to more
complicated physical objects. If a processor receives a more or less
contiguous subdomain, the number of its neighbors will be
limited. This implies that even in complicated problems each processor
will only communicate with a small number of other processors. Compare
this to the dense case where each processor had to receive data from
<i>every</i>
far more friendly to the interconnection network. (The fact that it
also more common for large systems may influence the choice of network
to install if you are about to buy a new parallel computer.)
</p>

<p name="switchToTextMode">
For square domains we can make this argument formal.
Let the
unit domain $[0,1]^2$ be partitioned over $P$ processors in a $\sqrt
P\times \sqrt P$ grid. From figure&nbsp;
6.11
 we see
that every processor communicates with at most four neighbors. Let
the amount of work per processor be&nbsp;$w$ and the communication time
with each neighbor&nbsp;$c$. Then the time to perform the total work on a
single processor is $T_1=Pw$, and the parallel time is $T_P=w+4c$,
giving a speed up of
\[
 S_P=Pw/(w+4c)=P/(1+4c/w)\approx P(1-4c/w). 
\]
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Express $c$ and $w$ as functions of $N$ and&nbsp;$P$, and show that the
  speedup is asymptotically optimal, under weak scaling of the problem.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  In this exercise you will analyze the parallel sparse matrix-vector
  product for a hypothetical, but realistic, parallel machine.
  Let the machine parameters be characterized by (see
  section&nbsp;
1.3.2
):
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Network 
<i> Latency:</i>
 $\alpha=1\mu s=10^{-6}s$.
<li>
Network 
<i> Bandwidth:</i>
 $1Gb/s$ corresponds to $\beta=10^{-9}$.
<li>

<i> Computation rate:</i>
 A per-core flops rate of $1G$flops
    means $\gamma=10^9$. This number may seem low, but note that the
    matrix-vector product has less reuse than the matrix-matrix
    product, which can achieve close to peak performance,
    and that the sparse matrix-vector product is even more
    bandwidth-bound.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  We perform a combination of asymptotic analysis and deriving specific numbers.
  We assume a cluster of
  $10^4$ single-core processors\footnote
  {Introducing multicore processors would complicate the story, but
    since the number of cores is $O(1)$, and the only way to grow a
    cluster is by adding more networked nodes, it does not change the
    asymptotic analysis.}. We apply this to a five-point
  stencil matrix of size $N=25\cdot 10^{10}$. This means each
  processor stores $5\cdot 8\cdot N/p=10^9$ bytes. If the matrix comes
  from a problem on a square domain, this means the domain was size
  $n\times n$ where $n=\sqrt N=5\cdot 10^5$.
</p>

<p name="switchToTextMode">
  Case 1. Rather than dividing the matrix, we divide the domain, and
  we do this first by horizontal slabs of size $n\times (n/p)$. Argue
  that the communication complexity is $2(\alpha+n\beta)$ and
  computation complexity is $10\cdot n\cdot (n/p)$. Show that the
  resulting computation outweighs the communication by a factor&nbsp;250.
</p>

<p name="switchToTextMode">
  Case 2. We divide the domain into patches of size
  $(n/\sqrt p)\times (n/\sqrt p)$.
  The memory and computation time are the same as
  before. Derive the communication time and show that it is better by
  a factor of&nbsp;50.
</p>

<p name="switchToTextMode">
  Argue that the first case does not weakly scale: under the
  assumption that $N/p$ is constant the efficiency will go down.
  (Show that speedup still goes up asymptotically as&nbsp;$\sqrt p$.)
  Argue that the second case does scale weakly.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The argument that a processor will only connect with a few neighbors
is based on the nature of the scientific computations. It is true for
<span title="acronym" ><i>FDM</i></span>
 and 
<span title="acronym" ><i>FEM</i></span>
 methods.
In the case
of the 
<i>BEM</i>
, any subdomain needs to communicate with
everything in a radius&nbsp;$r$ around it. As the number of processors goes
up, the number of neighbors per processor will also go up.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Give a formal analysis of the speedup and efficiency of the 
<span title="acronym" ><i>BEM</i></span>
  algorithm. Assume again a unit amount of work&nbsp;$w$ per processor and
  a time of communication&nbsp;$c$ per neighbor. Since the notion of
  neighbor is now based on physical distance, not on graph
  properties, the number of neighbors will go up. Give
  $T_1,T_p,S_p,E_p$ for this case.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

There are also cases where a sparse matrix needs to be handled
similarly to a dense matrix. For instance, 
<i>Google</i>
's
<i>PageRank</i>
 algorithm (see section&nbsp;
9.5
) has at its
heart the repeated operation $x\leftarrow Ax$ where $A$&nbsp;is a sparse
matrix with $A_{ij}\not=0$ if web page&nbsp;$j$ links to page&nbsp;$i$; see
section&nbsp;
9.5
. This makes $A$ a very sparse matrix, with
no obvious structure, so every processor will most likely communicate
with almost every other.
</p>

<p name="switchToTextMode">

<h3><a id="Memorybehaviorofthesparsematrix-vectorproduct">6.5.4</a> Memory behavior of the sparse matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Memorybehaviorofthesparsematrix-vectorproduct">Memory behavior of the sparse matrix-vector product</a>
</p>
</p>

<p name="switchToTextMode">
In section&nbsp;
1.7.11
 you saw an analysis of the sparse
matrix-vector product in the dense case, on a single processor. Some
of the analysis carries over immediately to the sparse case, such as
the fact that each matrix element is used only once and that the
performance is bound by the bandwidth between processor and memory.
</p>

<p name="switchToTextMode">
Regarding reuse of the input and the output vector, if the matrix is
stored by rows, such as in 
<i>CRS</i>
 format (section&nbsp;
5.4.1.3
),
access to the output vector will be limited to one write per matrix
row.
On the other hand, the loop unrolling
trick for getting reuse of the input vector can not be applied
here. Code that combines two iterations is as follows:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;M; i+=2) {
  s1 = s2 = 0;
  for (j) {
    s1 = s1 + a[i][j] * x[j];
    s2 = s2 + a[i+1][j] * x[j];
  }
  y[i] = s1; y[i+1] = s2;
}
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
The problem here is that if $a_{ij}$ is nonzero, it is not guaranteed
that $a_{i+1,j}$ is nonzero. The irregularity of the sparsity pattern
makes optimizing the matrix-vector product hard. Modest improvements
are possible by identifying parts of the matrix that are small dense
blocks&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#ButtEijkLang:spmvp,DemEtAl:ieeeproc2004,oski">[ButtEijkLang:spmvp,DemEtAl:ieeeproc2004,oski]</a>
.
</p>

<p name="switchToTextMode">
On a 
<i>GPU</i>
 the sparse matrix-vector product is also limited by
memory bandwidth. Programming is now harder because the 
<span title="acronym" ><i>GPU</i></span>
 has
to work in data parallel mode, with many active threads.
</p>

<p name="switchToTextMode">
An
interesting optimization becomes possible if we consider the context
in which the sparse matrix-vector product typically appears. The most
common use of this operation is in iterative solution methods for
linear systems (section&nbsp;
5.5
), where it is applied with
the same matrix in possibly hundreds of iterations. Thus we could
consider leaving the matrix stored on the GPU and only copying the
input and output vectors for each product operation.
</p>

<p name="switchToTextMode">

<h3><a id="Thetransposeproduct">6.5.5</a> The transpose product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Thetransposeproduct">The transpose product</a>
</p>

</p>

<p name="switchToTextMode">
In section&nbsp;
5.4.1.3
 you saw that the code for both the regular
and the transpose matrix-vector product are limited to loop orderings
where rows of the matrix are traversed. (In section&nbsp;
1.6.2
you saw a discussion of computational effects of changes in loop
order; in this case we are limited to row traversal by the storage
format.)
</p>

<p name="switchToTextMode">
In this section we will briefly look at the parallel transpose
product. Equivalently to partitioning the matrix by rows and
performing the transpose product, we look at a matrix stored and
partitioned by columns and perform the regular product.
</p>

<p name="switchToTextMode">
The  algorithm for the product by columns can be given as:
<!-- environment: quote start embedded generator -->
</p>
<!-- TranslatingLineGenerator quote ['quote'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
      $y\leftarrow 0$<br>
      for $j$:<br>
      \&gt; for $i$:<br>
      \&gt;\&gt; $y_i\leftarrow y_i+a_{ij}x_j$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
Both in shared and distributed memory we distribute outer iterations
over processors. The problem is then that each outer iteration updates
the whole output vector. This is a problem: with shared memory it
leads to multiple writes to locations in the output and in distributed
memory it requires communication that is as yet unclear.
</p>

<p name="switchToTextMode">
One way to solve this would be to allocate a private output
vector&nbsp;$y^{(p)}$ for each process:
<!-- environment: quote start embedded generator -->
</p>
<!-- TranslatingLineGenerator quote ['quote'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
      $y^{(p)}\leftarrow 0$<br>
      for $j\in$ the rows of processor $p$<br>
      \&gt; for all $i$:<br>
      \&gt;\&gt; $y^{(p)}_i\leftarrow y^{(p)}_i+a_{ji}x_j$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
after which we sum $y\leftarrow\sum_py^{(p)}$.
</p>

<h3><a id="Setupofthesparsematrix-vectorproduct">6.5.6</a> Setup of the sparse matrix-vector product</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Sparsematrix-vectorproduct">Sparse matrix-vector product</a> > <a href="parallellinear.html#Setupofthesparsematrix-vectorproduct">Setup of the sparse matrix-vector product</a>
</p>

<!-- index -->
<p name="switchToTextMode">

While the dense matrix-vector product relies on collectives
(see section&nbsp;
6.2
),
the sparse case uses point-to-point communication, that is,
every processor sends to just a few neighbors, and receives
from just a few.
This makes sense for the type of sparse matrices
that come from 
<span title="acronym" ><i>PDEs</i></span>
which have a clear structure, as you saw
in section&nbsp;
4.2.3
. However, there are
sparse matrices that are so random that you essentially
have to use dense techniques; see
section&nbsp;
9.6
.
</p>

<p name="switchToTextMode">
However, there is an asymmetry between sending and receiving.
It is fairly easy for a processor to find out what other processors
it will be receiving from.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Assume that the matrix is divided over the processors by block rows;
  see figure&nbsp;
6.2
 for an illustration. Also assume that each processor
  knows which rows each other processor stores. (How would you implement that knowledge?)
</p>

<p name="switchToTextMode">
  Sketch the algorithm by which a processor can find out who it will
  be receiving from; this algorithm should not involve any
  communication itself.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Discovering who to send 
<i>to</i>
 is harder.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Argue that this is easy in the case of a
<i>structurally symmetric matrix</i>
:
  $a_{ij}\not=0\Leftrightarrow a_{ji}\not=0$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In the general case, a&nbsp;processor can in principle be asked to send to any other,
so the simple algorithm is as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Each processor makes an inventory of what non-local indices it
  needs. Under the above assumption that it knows what range of indices each
  other processor owns, it then decides which indices to get from what
  neighbors.
<li>
Each processor sends a list of indices to each of its
  neighbors; this list will be empty for most of the neighbors, but
  we can not omit sending it.
<li>
Each processor then receives these lists from all others, and
  draws up lists of which indices to send.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
You will note that, even though the communication during the
matrix-vector product involves only a few neighbors for each
processor, giving a cost that is $O(1)$ in the number of processors,
the setup involves all-to-all communications, which
have time complexity&nbsp;$O(\alpha P)$
</p>

<p name="switchToTextMode">
If a processor has only a few neighbors, the above algorithm is wasteful.
Ideally, you would want space and running time proportional to the number of neighbors.
We could bring the receive time in the setup if we knew how many messages
were to be expected. That number can be found:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Each processor makes an array 
<tt>need</tt>
 of length&nbsp;$P$, where 
<tt>need[i]</tt>

  is&nbsp;1 if the processor needs any data from processor&nbsp;$i$, and zero otherwise.
<li>
A 
<i>reduce-scatter</i>
 collective on this array, with a
  sum operator, then leaves on each processor a number indicating how
  many processors need data from it.
<li>
The processor can execute that many receive calls.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
The reduce-scatter call has time complexity $\alpha\log P+\beta P$,
which is of the same order as the previous algorithm, but
probably with a lower proportionality constant.
</p>

<p name="switchToTextMode">
The time and space needed for the setup can be reduced to $O(\log P)$ with some
sophisticated trickery&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Falgout:scalable-hypre,Hoefler:2010:SCP">[Falgout:scalable-hypre,Hoefler:2010:SCP]</a>
.
</p>

<!-- index -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h2><a id="Computationalaspectsofiterativemethods">6.6</a> Computational aspects of iterative methods</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a>
</p>

<p name="switchToTextMode">

All iterative methods feature the following operations:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
A matrix-vector product; this was discussed for the sequential
  case in section&nbsp;
5.4
 and for the parallel case in
  section&nbsp;
6.5
.
  In the parallel case,
  construction of 
<span title="acronym" ><i>FEM</i></span>
 matrices has a complication that we will
  discuss in section&nbsp;
6.6.2
.
<li>
The construction of the preconditioner matrix&nbsp;$K\approx A$, and
  the solution of systems $Kx=y$. This was discussed in the sequential
  case in section&nbsp;
5.5.6
. Below
  we will go into parallelism aspects in section&nbsp;
6.7
.
<li>
Some vector operations (including inner products, in
  general). These will be discussed next.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Vectoroperations">6.6.1</a> Vector operations</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a> > <a href="parallellinear.html#Vectoroperations">Vector operations</a>
</p>
</p>

<p name="switchToTextMode">
There are two types of vector operations in a typical iterative method:
vector additions and inner products.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the 
<span title="acronym" ><i>CG</i></span>
 method of section&nbsp;
5.5.11
,
  figure&nbsp;
5.10
, applied to the
  matrix from a 2D 
<span title="acronym" ><i>BVP</i></span>
; equation&nbsp;\eqref{eq:5starmatrix}, First
  consider the unpreconditioned case $M=I$. Show that there is a
  roughly equal number of floating point
  operations are performed in the matrix-vector product and
  in the vector operations. Express everything in the matrix size&nbsp;$N$ and
  ignore lower order terms. How would this balance be if the matrix
  had 20 nonzeros per row?
</p>

<p name="switchToTextMode">
  Next, investigate this balance between vector and matrix operations
  for the 
<span title="acronym" ><i>FOM</i></span>
 scheme in section&nbsp;
5.5.9
. Since the number
  of vector operations depends on the iteration, consider the first 50
  iterations and count how many floating point operations are done in
  the vector updates and inner product versus the matrix-vector
  product. How many nonzeros does the matrix need to have for these
  quantities to be equal?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Flop counting is not the whole truth. What can you say about the
  efficiency of the vector and matrix operations in an iterative
  method, executed on a single processor?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Vectoradditions">6.6.1.1</a> Vector additions</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a> > <a href="parallellinear.html#Vectoroperations">Vector operations</a> > <a href="parallellinear.html#Vectoradditions">Vector additions</a>
</p>
</p>

<p name="switchToTextMode">
The vector additions are
typically of the form $x\leftarrow x+\alpha y$ or $x\leftarrow \alpha x+y$.
If we assume that all vectors are distributed the same way, this
operation is fully parallel.
</p>

<h4><a id="Innerproducts">6.6.1.2</a> Inner products</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a> > <a href="parallellinear.html#Vectoroperations">Vector operations</a> > <a href="parallellinear.html#Innerproducts">Inner products</a>
</p>
<!-- index -->
<p name="switchToTextMode">

Inner products are vector operations, but they are computationally
more interesting than updates, since they involve communication.
</p>

<p name="switchToTextMode">
When we compute an inner product, most likely
every processor needs to receive the computed value. We
use the following algorithm:
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>\For {processor $p$} {    compute $a\_p\leftarrow x\_p^ty\_p$ where $x\_p,y\_p$ are the part of    $x,y$ stored on processor&nbsp;$p$ }    do a global reduction to compute $a=\sum\_p a\_p$ \</p>
broadcast the result  \caption{Compute $a\leftarrow x^ty$ where $x,y$ are distributed vectors}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

The reduction and broadcast (which can be joined into an {\tt
  Allreduce}) combine data over all processors, so they have a
communication time that increases with the number of processors. This
makes the inner product potentially an expensive operation, and people
have suggested a number of ways to reducing their impact on the
performance of iterative methods.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Iterative methods are typically used for sparse matrices. In that
  context, you can argue that the communication involved in an inner product
  can have a larger influence on overall performance than the
  communication in the matrix-vector product. What is the
  complexity of the matrix-vector product and the inner product as a
  function of the number of processors?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Here are some of the approaches that have been taken.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The 
<span title="acronym" ><i>CG</i></span>
 method has two inner products per iteration that are
  inter-dependent. It is possible to rewrite the method so that it
  computes the same iterates (in exact arithmetic, at least) but so
  that the two inner products per iteration can be
  combined. See&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#ChGe:sstep,DAzEijRo:ppscicomp,Me:multicg,YandBrent:bicgstab">[ChGe:sstep,DAzEijRo:ppscicomp,Me:multicg,YandBrent:bicgstab]</a>
.
<li>
It may be possible to overlap the inner product calculation with
  other, parallel, calculations&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#dehevo92:acta">[dehevo92:acta]</a>
.
<li>
In the 
<span title="acronym" ><i>GMRES</i></span>
 method, use of the classical 
<span title="acronym" ><i>GS</i></span>
  method takes far fewer independent inner product than the modified
<span title="acronym" ><i>GS</i></span>
 method, but it is less stable. People have investigated strategies for deciding
  when it is allowed to use the classic 
<span title="acronym" ><i>GS</i></span>
 method&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Langou:thesis">[Langou:thesis]</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Since computer arithmetic is not associative, inner products are a prime
source of results that differ when the same calculation is executed of
two different processor configurations. In section&nbsp;
3.5.5
we sketched a solution.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Finiteelementmatrixconstruction">6.6.2</a> Finite element matrix construction</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a> > <a href="parallellinear.html#Finiteelementmatrixconstruction">Finite element matrix construction</a>
</p>

</p>

<p name="switchToTextMode">
The 
<i>FEM</i>
 leads to an interesting issue in parallel
computing. For this we need to sketch the basic outline of how this
method works. The 
<span title="acronym" ><i>FEM</i></span>
 derives its name from the fact that the
physical objects modeled are divided into small two or three
dimensional shapes, the elements, such as triangles and squares in 2D,
or pyramids and bricks in&nbsp;3D. On each of these, the function we are
modeling is then assumed to polynomial, often of a low degree, such as
linear or bilinear.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/fem.jpg" width=800></img>
<p name="switchToTextMode">
  \caption{A finite element domain, parallelization of the matrix
    construction, and parallelization of matrix element storage}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The crucial fact is that a matrix element&nbsp;$a_{ij}$ is then the sum of
computations, specifically certain integrals, over all elements that
contain both variables $i$ and&nbsp;$j$:
\[
 a_{ij}=\sum_{e\colon i,j\in e} a^{(e)}_{ij}. 
\]
The computations in each element share many common parts, so it is
natural to assign each element&nbsp;$e$ uniquely to a processor&nbsp;$P_e$,
which then computes all contributions&nbsp;$a^{(e)}_{ij}$. In
figure&nbsp;
6.14
 element&nbsp;2 is assigned to processor&nbsp;0
and element&nbsp;4 to processor&nbsp;1.
</p>

<p name="switchToTextMode">
Now consider variables $i$ and&nbsp;$j$ and the matrix
element&nbsp;$a_{ij}$.  It is constructed as the sum of computations over
domain elements 2 and&nbsp;4, which have been assigned to different processors.
Therefore, no matter what processor row $i$ is assigned to, at least
one processor will have to communicate its contribution to matrix
element&nbsp;$a_{ij}$.
</p>

<p name="switchToTextMode">
Clearly it is not possibly to make assignments $P_e$ of elements and
$P_i$ of variables such that $P_e$ computes in full the coefficients
$a_{ij}$ for all $i\in e$. In other words, if we compute the
contributions locally, there needs to be some amount of communication to
assemble certain matrix elements.
For this reason, modern linear algebra libraries such as PETSc
tut:petsc
)
allow any processor to set any
matrix element.
</p>

<h3><a id="Asimplemodelforiterativemethodperformance">6.6.3</a> A simple model for iterative method performance</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Computationalaspectsofiterativemethods">Computational aspects of iterative methods</a> > <a href="parallellinear.html#Asimplemodelforiterativemethodperformance">A simple model for iterative method performance</a>
</p>
<p name="switchToTextMode">

Above, we have already remarked that iterative methods have very little
opportunity for data reuse, and they are therefore characterized as
<i>bandwidth-bound</i>
 algorithms. This allows us to make a simple
prediction as to the
<i>floating point performance</i>
<!-- index -->
of iterative methods.
Since the number of iterations of an iterative method is hard to predict,
by performance here we mean the performance of a single iteration.
Unlike with 
<i>direct methods for linear systems</i>
(see for instance section&nbsp;
6.8.1
),
the number of flops to solution is very hard to establish in advance.
</p>

<p name="switchToTextMode">
First we argue that we can restrict ourselves to the 
<i>performance of the sparse   matrix vector product</i>

<!-- index -->
:
the time spent in this operation is considerably more than in the vector operations.
Additionally, most preconditioners have a computational structure that is
quite similar to the matrix-vector product.
</p>

<p name="switchToTextMode">
Let us then consider the 
<i>performance of the CRS matrix-vector   product</i>

  matrix-vector product}.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
First we observe that the arrays of matrix elements and column indices have no reuse,
  so the performance of loading them is completely determined by the available bandwidth.
  Caches and prefect streams only hide the latency, but do not improve the bandwidth.
  For
  each multiplication of matrix element times input vector element
  we load one floating point number and one integer. Depending one whether the
  indices are 32-bit or 64-bit, this means 12 or 16&nbsp;bytes loaded for each multiplication.
<li>
The demands for storing the result vector are less important: an output vector
  element is written only once for each matrix row.
<li>
The input vector can also be ignored in the bandwidth calculation.
  At first sight you might think that indirect indexing
  into the input vector is more or less random and therefore expensive.
  However, let's take the operator view of a matrix-vector product
  and consider the space domain of the 
<span title="acronym" ><i>PDE</i></span>
 from which the matrix stems;
  see section&nbsp;
4.2.3
. We now see that the seemingly random indexing
  is in fact into vector elements that are grouped closely together.
  This means that these vector elements will likely reside in L3 cache,
  and therefore accessible with a higher bandwidth (say, by a factor of at least&nbsp;5)
  than data from main memory.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
For the parallel performance of the sparse matrix-vector product we
consider that in a 
<span title="acronym" ><i>PDE</i></span>
 context each processor communicates only
with a few neighbors. Furthermore, a surface-to-volume argument
shows that the message volume is of a lower order than the on-node
computation.
</p>

<p name="switchToTextMode">
In sum, we conclude that a very simple model for the sparse matrix vector product,
and thereby for the whole iterative solver, consists of measuring
the effective bandwidth and computing the performance as one
addition and one multiplication operation per 12 or 16&nbsp;bytes loaded.
</p>

<h2><a id="Parallelpreconditioners">6.7</a> Parallel preconditioners</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a>
</p>

<p name="switchToTextMode">

Above (section&nbsp;
5.5.6
 and in particular&nbsp;
5.5.6.1
) we saw a
couple of different choices of&nbsp;$K$. In this section we will begin the
discussion of parallelization strategies. The discussion is continued
in detail in the next sections.
</p>

<h3><a id="Jacobipreconditioning">6.7.1</a> Jacobi preconditioning</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a> > <a href="parallellinear.html#Jacobipreconditioning">Jacobi preconditioning</a>
</p>
<p name="switchToTextMode">

The Jacobi method (section&nbsp;
5.5.3
) uses the diagonal
of&nbsp;$A$ as preconditioner. Applying this is as parallel as is
possible: the statement $y\leftarrow K\inv x$ scales every element of
the input vector independently. Unfortunately the improvement in the
number of iterations with a Jacobi preconditioner is rather
limited. Therefore we need to consider more sophisticated methods such
<span title="acronym" ><i>ILU</i></span>
. Unlike with the Jacobi preconditioner, parallelism is then
not trivial.
</p>

<h3><a id="ThetroublewithILUinparallel">6.7.2</a> The trouble with ILU in parallel</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a> > <a href="parallellinear.html#ThetroublewithILUinparallel">The trouble with ILU in parallel</a>
</p>
<p name="switchToTextMode">

Above we saw that, in a flop counting sense, applying an ILU
preconditioner (section&nbsp;
5.5.6.1
) is about as expensive as doing
a matrix-vector product. This is no longer true if we run our
iterative methods on a parallel computer.
</p>

<p name="switchToTextMode">
At first glance the operations are similar. A matrix-vector product
$y=Ax$ looks like
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  y[i] = sum over j=1..n a[i,j]*x[j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
In parallel this would look like
<!-- environment: verbatim start embedded generator -->
</p>
for i=myfirstrow..mylastrow
  y[i] = sum over j=1..n a[i,j]*x[j]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
Suppose that a processor has local copies of all the elements of $A$
and&nbsp;$x$ that it will need, then this operation is fully parallel: each
processor can immediately start working, and if the work load is
roughly equal, they will all finish at the same time. The total time
for the matrix-vector product is then divided by the number of
processors, making the speedup more or less perfect.
</p>

<p name="switchToTextMode">
Consider now the forward solve $Lx=y$, for instance in the context of
an 
<span title="acronym" ><i>ILU</i></span>
 preconditioner:
<!-- environment: verbatim start embedded generator -->
</p>
for i=1..n
  x[i] = (y[i] - sum over j=1..i-1 ell[i,j]*x[j]) / a[i,i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
We can simply write the parallel code:
<!-- environment: verbatim start embedded generator -->
</p>
for i=myfirstrow..mylastrow
  x[i] = (y[i] - sum over j=1..i-1 ell[i,j]*x[j]) / a[i,i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
but now there is a problem. We can no longer say `suppose a processor
has local copies of everything in the right hand side', since the
vector&nbsp;$x$ appears both in the left and right hand side. While the
matrix-vector product is in principle fully parallel over the matrix
rows, this triangular solve code is recursive, hence sequential.
</p>

<p name="switchToTextMode">
In a parallel computing context this means that, for the second
processor to start, it needs to wait for certain components of&nbsp;$x$
that the first processor computes. Apparently, the second processor
can not start until the first one is finished, the third processor has
to wait for the second, and so on. The disappointing conclusion is
that in parallel only one processor will be active at any time, and
the total time is the same as for the sequential algorithm. This is
actually not a big problem in the dense matrix case, since parallelism
can be found in the operations for handling a single row (see
section&nbsp;
6.11
), but in the sparse case it means we
can not use incomplete factorizations without some redesign.
</p>

<p name="switchToTextMode">
In the next few subsections we will see different strategies for
finding preconditioners that perform efficiently in parallel.
</p>

<h3><a id="BlockJacobimethods">6.7.3</a> Block Jacobi methods</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a> > <a href="parallellinear.html#BlockJacobimethods">Block Jacobi methods</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Various approaches have been suggested to remedy this sequentiality
the triangular solve. For instance, we could simply let the processors
ignore the components of&nbsp;$x$ that should come from other processors:
<!-- environment: verbatim start embedded generator -->
</p>
for i=myfirstrow..mylastrow
  x[i] = (y[i] - sum over j=myfirstrow..i-1 ell[i,j]*x[j])
         / a[i,i]
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
This is not mathematically equivalent to the sequential algorithm
(technically, it is called a 
<i>block Jacobi</i>
 method with
<span title="acronym" ><i>ILU</i></span>
 as the 
<i>local solve</i>
), but
since we're only looking for an approximation $K\approx A$, this is
simply a slightly cruder approximation.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Take the Gauss-Seidel code you wrote above, and simulate a parallel
  run. What is the effect of increasing the (simulated) number of processors?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

The idea behind block methods can be appreciated pictorially;
see figure&nbsp;
6.15
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/block-jacobi.jpeg" width=800></img>
<p name="switchToTextMode">
  \caption{Sparsity pattern corresponding to a block Jacobi
    preconditioner}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
In effect, we make an 
<span title="acronym" ><i>ILU</i></span>
 of the matrix that we get by ignoring
all connections between processors. Since in a 
<span title="acronym" ><i>BVP</i></span>
 all points
influence each other (see section&nbsp;
4.2.1
), using a
less connected preconditioner will increase the number of iterations
if executed on a sequential computer. However, block methods are
parallel and, as we observed above, a sequential preconditioner is
very inefficient in a parallel context, so we put up with this
increase in iterations.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="ParallelILU">6.7.4</a> Parallel ILU</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelpreconditioners">Parallel preconditioners</a> > <a href="parallellinear.html#ParallelILU">Parallel ILU</a>
</p>

</p>

<p name="switchToTextMode">
The Block Jacobi preconditioner operates by decoupling domain
parts. While this may give a method that is highly parallel, it may
give a higher number of iterations than a true 
<span title="acronym" ><i>ILU</i></span>
preconditioner. (A&nbsp;theoretical argument can be made that this
decoupling decreases the efficiency of the iterative method; see
section&nbsp;
4.2.1
.) Fortunately it is possible to
have a parallel 
<span title="acronym" ><i>ILU</i></span>
 method. Since we need the upcoming
material on re-ordering of variables, we postpone this discussion
until section&nbsp;
6.8.2.3
.
</p>

<h2><a id="Orderingstrategiesandparallelism">6.8</a> Ordering strategies and parallelism</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a>
</p>

<p name="switchToTextMode">

In the foregoing we have remarked on the fact that solving
a linear system of equations is inherently a recursive activity.
For dense systems, the number of operations is large enough
compared to the recursion length that finding parallelism
is fairly straightforward. Sparse systems, on the other hand,
take more sophistication. In this section we will look at
a number of strategies for reordering the equations (or, equivalently,
permuting the matrix) that will increase the available parallelism.
</p>

<p name="switchToTextMode">
These strategies can all be considered as variants of Gaussian elimination.
By making incomplete variants of them (see section&nbsp;
5.5.6.1
),
all these strategies also apply to constructing preconditioners
for iterative solution methods.
</p>

<h3><a id="Nesteddissection">6.8.1</a> Nested dissection</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Nesteddissection">Nested dissection</a>
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
</p>

<p name="switchToTextMode">
Above, you have seen several examples of ordering the variables in the
domain other than with the 
<i>lexicographic ordering</i>
. In this
section you will see the 
<i>nested dissection ordering</i>
, which was
initially designed as a way to reduce fill-in. However, it is also
advantageous in a parallel computing context.
</p>

<p name="switchToTextMode">
Nested dissection is a recursive process for determining a nontrivial
ordering of the unknowns in a domain. In the first step, the
computational domain is split in two parts, with a dividing strip
between them; see figure~
6.16
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/domdecomp.jpg" width=800></img>
<p name="caption">
FIGURE 6.16: Domain dissection into two unconnected subdomains and a separator
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
\newcommand\Add{A^{\mathrm{DD}}}
To be precise, the 
<i>separator</i>
 is wide enough that there are
no connections between the left and right 
<i>subdomain</i>
. The resulting
matrix $\Add$ has a $3\times3$ structure, corresponding to the three
divisions of the domain. Since the subdomains $\Omega_1$
and~$\Omega_2$ are not connected, the submatrices $\Add_{12}$
and~$\Add_{21}$ are zero.
\[
  \Add=
\begin{pmatrix}
    A_{11}&\emptyset&A_{13}\\
    \emptyset&A_{22}&A_{23}\\
    A_{31}&A_{32}&A_{33}
\end{pmatrix}
=
\left(
\begin{array}{ccccc|ccccc|c}
  \star&\star &      &      &      &&&&&&0\\
  \star&\star &\star &      &      &&&&&&\vdots\\
       &\ddots&\ddots&\ddots&      &&&\emptyset&&&\vdots\\
       &      &\star &\star &\star &&&&&&0\\
       &      &      &\star &\star &&&&&&\star\\ \hline
  &&&&&\star&\star &      &      &      &0\\
  &&&&&\star&\star &\star &      &      &\vdots\\
  &&\emptyset&&&     &\ddots&\ddots&\ddots&      &\vdots\\
  &&&&&     &      &\star &\star &\star &0\\
  &&&&&     &      &      &\star &\star &\star\\ \hline
  0&\cdots&\cdots&0&\star&0&\cdots&\cdots&0&\star&\star
\end{array}
\right)
\begin{array}{c}
  \left.
\begin{array}{c}
    \phantom{0}\\ \phantom{\vdots}\\ \phantom{\vdots}\\ \phantom{0}\\
    \phantom{\star}\\
\end{array}
\right\} \\
  \left.
\begin{array}{c}
    \phantom{0}\\ \phantom{\vdots}\\ \phantom{\vdots}\\ \phantom{0}\\
    \phantom{\star}\\
\end{array}
\right\} \\
  \left.
\begin{array}{c}
    \phantom{0}\\
\end{array}
\right\}
\end{array}
\begin{array}{c}
  \phantom{0}\\ \phantom{\vdots}\\ (n^2-n)/2\\ \phantom{0}\\ \phantom{\star}\\
  \phantom{0}\\ \phantom{\vdots}\\ (n^2-n)/2\\ \phantom{0}\\ \phantom{\star}\\
  n
\end{array}
\]
This process of dividing the domain by a separator is also called
<i>domain decomposition</i>
 or 
<i>substructuring</i>
,
although this name is also associated with the mathematical analysis
of the resulting matrices~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#BGSm:96">[BGSm:96]</a>
. In this example of a
rectangular domain it is of course trivial to find a
separator. However, for the type of equations we get from 
<span title="acronym" ><i>BVPs</i></span>
 it
is usually feasible to find a separator
efficiently for any domain~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#LiTa:separator">[LiTa:separator]</a>
;
see also section~
19.6.2
.
</p>

<p name="switchToTextMode">
Let us now consider the $LU$ factorization of this matrix. If we
factor it in terms of the $3\times 3$ block structure, we get
\[
  \Add=LU=
\begin{pmatrix}
    I\\
    \emptyset&I\\
    A_{31}A_{11}\inv&A_{32}A_{22}\inv&I
\end{pmatrix}
\begin{pmatrix}
    A_{11}&\emptyset&A_{13}\\
         &A_{22}&A_{23}\\
    &&S_{33}
\end{pmatrix}
\]
where 
\[
 S_{33}=A_{33}-A_{31}A_{11}\inv A_{13}-A_{32}A_{22}\inv A_{23}. 
\]
The important fact here is that
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
the contributions $A_{31}A_{11}\inv A_{13}$ and
  $A_{32}A_{22}\inv A_{23}$ can be computed simultaneously, so the
  factorization is largely parallel; and
<li>
both in the forward and backward solve, components 1 and~2 of
  the solution can be computed simultaneously, so the solution process
  is also largely parallel.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
The third block can not trivially be handled in parallel, so this
introduces a sequential component in the algorithm. We also need to
take a closer look at the structure of~$S_{33}$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  In section~
5.4.3.1
 you saw the connection between LU
  factorization and graph theory: eliminating a node leads to a graph
  with that node removed, but with certain new connections added.
  Show that, after eliminating the
  first two sets of variables, the graph of the remaining matrix on
  the separator will be fully connected.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The upshot is that after eliminating all the variables in blocks 1
and&nbsp;2 we are left with a matrix&nbsp;$S_{33}$ that is fully
dense of size $n\times n$.
</p>

<p name="switchToTextMode">
The introduction of a separator gave us a factorization that was
two-way parallel. Now we iterate this process: we put a
separator inside blocks 1 and&nbsp;2 (see figure&nbsp;
6.17
),
which gives the following matrix structure:
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/domdecomp2.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 6.17: A four-way domain decomposition
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
\[
  \Add=
  \left(
\begin{array}{cccc|cc|c}
    A_{11}&     &     &     &A_{15}&     &A_{17}\\
         &A_{22}&     &     &A_{25}&     &A_{27}\\
         &     &A_{33}&     &     &A_{36}&A_{37}\\
         &     &     &A_{44}&     &A_{46}&A_{47}\\ \hline
    A_{51}&A_{52}&    &     &A_{55}&      &A_{57}\\
         &      &A_{63}&A_{64}&    &A_{66}&A_{67}\\ \hline
    A_{71}&A_{72}&A_{73}&A_{74}&A_{75}&A_{76}&A_{77}
\end{array}
\right)
\]
(Note the similarities with the `arrow' matrix in
section&nbsp;
5.4.3.4
, and recall the argument that this led
to lower fill-in.)
The LU factorization of this is:
\[
  \left(
\begin{array}{cccc|cc|c}
        I&     &     &     &      &     &\\
         &I    &     &     &      &     &\\
         &     &I    &     &      &     &\\
         &     &     &I    &      &     &\\ \hline
    A_{51}A_{11}\inv&A_{52}A_{22}\inv&    &     &I    &      &\\
         &      &A_{63}A_{33}\inv&A_{64}A_{44}\inv&   &I     &\\ \hline
    A_{71}A_{11}\inv&A_{72}A_{22}\inv&A_{73}A_{33}\inv&A_{74}A_{44}\inv&
    A_{75}S_5\inv&A_{76}S_6\inv&I
\end{array}
\right) \cdot
\]
\[
  \kern 3\unitindent
  \left(
\begin{array}{cccc|cc|c}
    A_{11}&     &     &     &A_{15}&     &A_{17}\\
         &A_{22}&     &     &A_{25}&     &A_{27}\\
         &     &A_{33}&     &     &A_{36}&A_{37}\\
         &     &     &A_{44}&     &A_{46}&A_{47}\\ \hline
         &     &     &      &S_{5}&      &A_{57}\\
         &     &     &      &     &S_{6} &A_{67}\\ \hline
         &     &     &      &     &      &S_{7}
\end{array}
\right)
\]
where
\[
\begin{array}{l}
S_5=A_{55}-A_{51}A_{11}\inv A_{15}-A_{52}A_{22}\inv A_{25},\quad
   S_6=A_{66}-A_{63}A_{33}\inv A_{36}-A_{64}A_{44}\inv A_{46}\\
   S_7=A_{77}-\sum_{i=1,2,3,4}A_{7i}A_{ii}\inv A_{i7}
   - \sum_{i=5,6} A_{7i}S_i\inv A_{17}.
\end{array}
\]
Constructing the factorization now goes as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Blocks $A_{ii}$ are factored in parallel for $i=1,2,3,4$; similarly
   the contributions $A_{5i}A_{ii}\inv A_{i5}$ for $i=1,2$,
   $A_{6i}A_{ii}\inv A_{i6}$ for $i=3,4$, and $A_{7i}A_{ii}\inv
   A_{i7}$ for $i=1,2,3,4$  can be constructed in parallel.
<li>
The Schur complement matrices $S_5,S_6$ are formed and
  subsequently factored in parallel, and the contributions
  $A_{7i}S_i\inv A_{17}$ for $i=5,6$ are constructed in parallel.
<li>
The Schur complement $S_7$ is formed and factored.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Analogous to the above reasoning, we
conclude that after eliminating blocks 1,2,3,4 the updated matrices
$S_5,S_6$ are dense of size&nbsp;$n/2$, and after eliminating blocks 5,6
the Schur complement $S_7$ is dense of size&nbsp;$n$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that solving a system with $\Add$ has a similar parallelism to
  constructing the factorization as described above.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

For future reference, we will call the sets 1 and&nbsp;2 each others'
siblings, and similarly for 3 and&nbsp;4. The set 5 is the parent of 1
and&nbsp;2, 6&nbsp;is the parent of 3 and&nbsp;4; 5&nbsp;and&nbsp;6 are siblings and 7&nbsp;is the
parent of 5 and&nbsp;6.
</p>

<h4><a id="Domaindecomposition">6.8.1.1</a> Domain decomposition</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Nesteddissection">Nested dissection</a> > <a href="parallellinear.html#Domaindecomposition">Domain decomposition</a>
</p>
<p name="switchToTextMode">

In figure 
6.17
 we divided the domain four ways by a
recursive process. This leads up to our discussion of nested
dissection. It is also possible to immediately split a domain in any
number of strips, or in a grid of subdomains. As long as the
separators are wide enough, this will give a matrix structure with
many independent subdomains.
As in the above discussion, an LU factorization will be characterized
by
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
parallel processing of the subdomains, both in the factorization
  and $L,U$ solves, and
<li>
a system to be solved on the separator structure.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/domdecomp4.jpeg" width=800></img>
<p name="caption">
FIGURE 6.18: One-way domain decomposition
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The matrix from a two-dimensional 
<span title="acronym" ><i>BVP</i></span>
 has a block tridiagonal
  structure. Divide the domain in four strips, that is, using three
  separators (see figure&nbsp;
6.18
). Note that the
  separators are uncoupled in the original matrix.
</p>

<p name="switchToTextMode">
  Now sketch the sparsity structure of the resulting system on the
  separators are elimination of the subdomains. Show that the system
  is block tridiagonal.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In all the domain splitting schemes we have discussed so far we have
used domains that were rectangular, or `brick' shaped, in more than
two dimensions. All of these arguments are applicable to more general
domains in two or three dimensions, but things like finding a
separator become much harder&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#LiRoTa:dissection">[LiRoTa:dissection]</a>
, and that holds
even more for the parallel case. See section&nbsp;
19.6.2
for some introduction to this topic.
</p>

<h4><a id="Complexity">6.8.1.2</a> Complexity</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Nesteddissection">Nested dissection</a> > <a href="parallellinear.html#Complexity">Complexity</a>
</p>
<p name="switchToTextMode">

The nested dissection method repeats the above process until the
subdomains are very small.  For a theoretical analysis, we keep
dividing until we have subdomains of size&nbsp;$1\times1$, but in practice
one could stop at sizes such as&nbsp;$32$, and use an efficient dense
solver to factor and invert the blocks.
</p>

<p name="switchToTextMode">
To derive the complexity of the algorithm, we take another look at
figure&nbsp;
6.17
, and see that
complexity argument, the total space a full recursive nested
dissection factorization needs is the sum of
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
one dense matrix on a separator of size&nbsp;$n$, plus
<li>
two dense matrices on separators of size&nbsp;$n/2$,
<li>
taking together $3/2\,n^2$ space and $5/12\,n^3$ time;
<li>
the two terms above then get repeated on four subdomains of
  size $(n/2)\times(n/2)$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
With the observation that $n=\sqrt N$, this sums to
\[
\begin{array}{r@{{}={}}l}
\mathrm{space}&3/2n^2+4\cdot 3/2(n/2)^2+\cdots\\
   & N(3/2+3/2+\cdots)\quad\hbox{$\log n$ terms}\\
   & O(N\log N)
\end{array}
\]
\[
\begin{array}{r@{{}={}}l}
\mathrm{time}&5/12n^3/3+4\cdot 5/12(n/2)^3/3+\cdots\\
   & 5/12 N^{3/2}(1+1/4+1/16+\cdots)\\
   & O(N^{3/2})
\end{array}
\]
Apparently, we now have a factorization that is parallel to a large
extent, and that is done in $O(N\log N)$ space, rather than&nbsp;$O(N^{3/2})$
(see section&nbsp;
5.4.3.3
). The factorization time has also gone
down from $O(N^2)$ to&nbsp;$O(N^{3/2})$.
</p>

<p name="switchToTextMode">
Unfortunately, this space savings only happens in two dimensions: in three
dimensions we need
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
one separator of size $n\times n$, taking $(n\times
  n)^2=N^{4/3}$ space and $1/3\cdot (n\times n)^3=1/3\cdot N^2$ time,
<li>
two separators of size $n\times n/2$, taking $N^{3/2}/2$ space and
  $1/3\cdot N^2/4$ time,
<li>
four separators of size $n/2\times n/2$, taking $N^{3/2}/4$ space
  and $1/3\cdot N^2/16$ time,
<li>
adding up to $7/4 N^{3/2}$ space and $21/16 N^2/3$ time;
<li>
on the next level there are 8 subdomains that contribute these
  terms with $n\rightarrow n/2$ and therefore $N\rightarrow N/8$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
This makes the total space
\[
 \frac{7}{4}N^{3/2}(1+(1/8)^{4/3}+\cdots)=O(N^{3/2}) 
\]
and the total time
\[
 \frac{21}{16}N^2(1+1/16+\cdots)/3=O(N^2). 
\]
We no longer have the tremendous savings of the 2D case.
A&nbsp;much more complicated analysis shows that the
order improvement holds for general problems in&nbsp;2D, and that 3D in
general has a higher
complexity&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#LiRoTa:dissection">[LiRoTa:dissection]</a>
.
</p>

<h4><a id="Parallelism">6.8.1.3</a> Parallelism</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Nesteddissection">Nested dissection</a> > <a href="parallellinear.html#Parallelism">Parallelism</a>
</p>

<p name="switchToTextMode">

The nested dissection method clearly introduces a lot of parallelism,
and we can characterize it as task parallelism
(section&nbsp;
2.5.3
): associated with each separator is a
task of factoring its matrix, and later one of solving a linear system
on its variables.  However, the tasks are not independent: in
figure&nbsp;
6.17
 the factorization on domain&nbsp;7 has to wait
for 5 and&nbsp;6, and they have to wait for 1,2,3,4.
Thus, we have tasks with dependencies in the form of a tree: each
separator matrix can be factored only when its children have been
factored.
</p>

<p name="switchToTextMode">
Mapping these tasks to processors is not trivial. First of all, if we
are dealing with shared memory we can use a simple task queue:
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>\label{fig:taskqueue}  $\mbox{Queue}\leftarrow\{\}$\</p>
\For{all bottom level subdomains $d$}{add $d$ to the Queue}  \While{Queue is not empty}        {\If{a processor is idle}{assign a queued task to it}          \If{a task is finished AND its sibling is finished}{add its            parent to the queue}        }
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

The main problem here is that at some point we will have more
processors than tasks, thus causing load unbalance. This problem is
made more severe by the fact that the last tasks are also the most
substantial, since the separators double in size from level to
level. (Recall that the work of factoring a dense matrix goes up with the
third power of the size!) Thus, for the larger separators we have to
switch from task parallelism to medium-grained parallelism, where
processors collaborate on factoring a block.
</p>

<p name="switchToTextMode">
With distributed memory, we can now solve the parallelism problem with
a simple task queue, since it would involve moving large amounts of
data. (But recall that work is a higher power of the matrix size,
which this time works in our favor, making communication relatively
cheap.) The solution is then to use some form of domain
decomposition. In figure&nbsp;
6.17
 we could have four
processors, associated with block 1,2,3,4. Processors 1 and&nbsp;2 would
then negotiate which one factors block&nbsp;5 (and similarly processors 3
and&nbsp;4 and block&nbsp;6), or they could both do it redundantly.
</p>

<h4><a id="Preconditioning">6.8.1.4</a> Preconditioning</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Nesteddissection">Nested dissection</a> > <a href="parallellinear.html#Preconditioning">Preconditioning</a>
</p>
<p name="switchToTextMode">

As with all factorizations, it is possible to turn the nested
dissection method into a preconditioner by making the factorization
incomplete. (For the basic idea of incomplete factorizations, see
section&nbsp;
5.5.6.1
). However, here the factorization is formulated
completely in terms of 
<i>block matrices</i>

<!-- index -->
, and
the division by the pivot element becomes an inversion or system
solution with the pivot block matrix. We will not go into this
further; for details see the
literature&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#AxPo:dd2,Eij:general,Me:dd">[AxPo:dd2,Eij:general,Me:dd]</a>
.
</p>

<!-- index -->
<p name="switchToTextMode">

</p>

<h3><a id="Variablereorderingandcoloring:independentsets">6.8.2</a> Variable reordering and coloring: independent sets</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Variablereorderingandcoloring:independentsets">Variable reordering and coloring: independent sets</a>
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
</p>

<p name="switchToTextMode">
Parallelism can be achieved in sparse matrices by using graph coloring
(section~
19.3
). Since a `color' is defined as points
that are only connected to other colors, they are by definition
independent of each other, and therefore can be processed in
parallel. This leads us to the following strategy:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Decompose the adjacency graph of the problem into a small number
  of independent sets, called `colors';
<li>
Solve the problem in a number of sequential steps equal to the
  number of colors; in each step there will be large number of
  independently processable points.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Red-blackcoloring">6.8.2.1</a> Red-black coloring</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Variablereorderingandcoloring:independentsets">Variable reordering and coloring: independent sets</a> > <a href="parallellinear.html#Red-blackcoloring">Red-black coloring</a>
</p>
</p>

<p name="switchToTextMode">
We start with a simple example, where we consider a tridiagonal
matrix~$A$. The equation $Ax=b$ looks like
\[
\begin{pmatrix}
  a_{11}&a_{12}&&&&\emptyset\\ a_{21}&a_{22}&a_{23}\\
  &a_{32}&a_{33}&a_{34}\\ \emptyset&&\ddots&\ddots&\ddots
\end{pmatrix}
\begin{pmatrix}
  x_1\\ x_2\\ x_3\\ \vdots
\end{pmatrix}
 =
\begin{pmatrix}
  y_1\\ y_2\\ y_3\\ \vdots
\end{pmatrix}
\]
We observe that $x_i$ directly depends on $x_{i-1}$ and~$x_{i+1}$, but
not $x_{i-2}$ or~$x_{i+1}$. Thus, let us see what happens if we
permute the indices to group every other component together.
</p>

<p name="switchToTextMode">
Pictorially, we take the points $1,\ldots,n$ and color them red and
black (figure~
6.19
), then we permute them to first
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/red-black-1d.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="caption">
FIGURE 6.19: Red-black ordering of a the points on a line
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
take all red points, and subsequently all black ones.
The correspondingly permuted matrix looks as follows:
\[
\begin{pmatrix}
  a_{11}&&&&a_{12}\\ &a_{33}&&&a_{32}&a_{34}\\ &&a_{55}&&&\ddots&\ddots\\
  &&&\ddots\\
  a_{21}&a_{23}&&&a_{22}\\ &a_{43}&a_{45}&&&a_{44}\\ &&\ddots&\ddots&&&\ddots
\end{pmatrix}
\begin{pmatrix}
  x_1\\ x_3\\ x_5\\ \vdots\\ x_2\\ x_4\\ \vdots
\end{pmatrix}
 =
\begin{pmatrix}
  y_1\\ y_3\\ y_5\\ \vdots\\ y_2\\ y_4\\ \vdots
\end{pmatrix}
\]
With this permuted $A$, the Gauss-Seidel matrix $D_A+L_A$ looks like
\[
\begin{pmatrix}
  a_{11}&&&&\emptyset\\ &a_{33}\\ &&a_{55}\\
  &&&\ddots\\
  a_{21}&a_{23}&&&a_{22}\\ &a_{43}&a_{45}&&&a_{44}\\ &&\ddots&\ddots&&&\ddots
\end{pmatrix}
\]
What does this buy us? Well, let's spell out the solution of a system
$Lx=y$.
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>\For{$i=1,3,5,\ldots$}{solve $x\_i\leftarrow y\_i/a\_{ii}$}  \For{$i=2,4,6,\ldots$}{compute $t=a\_{ii-1}x\_{i-1}+a\_{ii+1}x\_{i+1}$    \</p>
solve $x\_i\leftarrow (y\_i-t)/a\_{ii}$  }
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

Apparently the algorithm has three stages that are each parallel over
half the domain points. This is illustrated in
figure~
6.20
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/red-black-1d-solve.jpg" width=800></img>
<p name="caption">
FIGURE 6.20: Red-black solution on a 1d domain
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Theoretically we could accommodate a number of processors that is half
the number of the domain points, but in practice each processor will
have a subdomain. Now you can see in figure~
6.21
how this causes a very modest amount of communication: each processor
sends at most the data of two red points to its neighbors.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/red-black-1d-solve-par.jpg" width=800></img>
<p name="caption">
FIGURE 6.21: Parallel red-black solution on a 1d domain
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Argue that the adjacency graph here is a 
<i>bipartite graph</i>
.
  We see that such graphs (and in general, colored graphs) are associated with parallelism.
  Can you also point out performance benefits for non-parallel processors?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Red-black ordering can be applied to two-dimensional problems too.
Let us apply a red-black ordering to the points $(i,j)$ where $1\leq
i,j\leq n$.
Here we first apply a successive numbering to the odd
points on the first line $(1,1),(3,1),(5,1),&hellip;$, then the even
points of the second line $(2,2),(4,2),(6,2),&hellip;$, the odd points
on the third line, et cetera. Having thus numbered half the points in
the domain, we continue with the even points in the first line, the
odd points in the second, et cetera.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/redblack.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{Red-black ordering of the variables of a two-dimensional
    domain}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
As you can see in figure&nbsp;
6.22
, now the red points are
only connected to black points, and the other way around. In graph
theoretical terms, you have found a 
<i>coloring</i>
(see appendix&nbsp;
app:graph
 for the definition
of this concept) of the matrix graph with two colors.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Apply the red-black ordering to the 2D 
<span title="acronym" ><i>BVP</i></span>
 \eqref{eq:laplace}.
  Sketch the resulting matrix structure.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The red-black ordering is a simple
example of 
<i>graph coloring</i>
 (sometimes called
<i>multi-coloring</i>
). In
simple cases, such as the unit square domain we considered in
section&nbsp;
4.2.3
 or its extension to 3D,
the 
<i>color number</i>
 of the adjacency graph is readily determined;
in less regular cases it is harder.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  You saw that a red-black ordering of unknowns
  coupled with the regular five-point star stencil give two subsets of
  variables that are not connected among themselves, that is,
  they form a two-coloring of the matrix graph.
  Can you find a coloring if nodes are connected by the second stencil in
  figure&nbsp;
4.3
?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h4><a id="Generalcoloring">6.8.2.2</a> General coloring</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Variablereorderingandcoloring:independentsets">Variable reordering and coloring: independent sets</a> > <a href="parallellinear.html#Generalcoloring">General coloring</a>
</p>
<p name="switchToTextMode">

There is a simple bound for the number of colors needed for the graph
of a sparse matrix: the number of colors is at most $d+1$ where
$d$&nbsp;is the degree of the graph. To see that we can color a graph with
degree&nbsp;$d$ using $d+1$ colors, consider a node with
degree&nbsp;$d$. No matter how its neighbors are colored, there is always
an unused color among the $d+1$ available ones.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider a sparse matrix, where the graph can be colored with $d$
  colors. Permute the matrix by first enumerating the unknowns of the
  first color,
  then the second color, et cetera. What can you say about the
  sparsity pattern of the resulting permuted matrix?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

If you
are looking for a direct solution of the linear system you can repeat
the process of coloring and permuting on the matrix that remains
after you have eliminated one color. In the case of a tridiagonal
matrix you saw that this remaining matrix was again tridiagonal, so
it is clear how to continue the process. This is called
<i>recursive doubling</i>
. If the matrix is not tridiagonal but
<i>block tridiagonal</i>
, this operation can be performed on
blocks.
</p>

<h4><a id="Multi-colorparallelILU">6.8.2.3</a> Multi-color parallel ILU</h4>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Variablereorderingandcoloring:independentsets">Variable reordering and coloring: independent sets</a> > <a href="parallellinear.html#Multi-colorparallelILU">Multi-color parallel ILU</a>
</p>

<!-- index -->
<p name="switchToTextMode">

In section&nbsp;
6.8.2
 you saw the combination of
<i>graph coloring</i>
 and permutation. Let $P$ be the
permutation that groups like-colored variables together, then $\tilde
A=P^tAP$ is a matrix with the following structure:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$\tilde A$ has a block structure with the number of blocks equal
  to the number of colors in the adjacency graph of&nbsp;$A$; and
<li>
each diagonal block is a diagonal matrix.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Now, if you are performing an iterative system solution
and you are looking for a parallel preconditioner you can use this
permuted matrix. Consider solving $Ly=x$ with the permuted system. We
write the usual algorithm (section&nbsp;
5.3.5
) as
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for $c$ in the set of colors:<br>
    \&gt;for $i$ in the variables of color $c$:<br>
    \&gt;\&gt;$y_i\leftarrow x_i-\sum_{j&lt;i} \ell_{ij}y_j$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that the flop count of solving a system $LUx=y$ remains the
  same (in the highest order term) when you from an 
<span title="acronym" ><i>ILU</i></span>
  factorization in the natural ordering to one in the color-permuted
  ordering.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/pilu.jpeg" width=800></img>
<p name="caption">
FIGURE 6.23: A partitioned domain with colored nodes
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Where does all this coloring get us? Solving is still
sequential&hellip; Well, it is true that the outer loop over the colors
is sequential, but all the points of one color are independent of
each other, so they can be solved at the same time.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/pilu-solve.jpeg" width=800></img>
<p name="caption">
FIGURE 6.24: Solving a parallel multicolor ILU in four steps
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
So if we use an ordinary domain partitioning and combine that with a
multi-coloring (see figure&nbsp;
6.23
), the processors are all
active during all the color stages; see
figure&nbsp;
6.24
. Ok, if you take a close look at that
figure you'll see that one processor is not active in the last
color. With large numbers of nodes per processor this is unlikely to
happen, but there may be some load imbalance.
</p>

<p name="switchToTextMode">
One remaining problem is how to generate the multi-coloring in
parallel. Finding the optimal color number is NP-hard.
The thing that saves us is that we don't necessarily need the optimal
number because we are making in incomplete factorization
anyway. (There is even an argument that using a slightly larger number
of colors decreases the number of iterations.)
</p>

<p name="switchToTextMode">
An elegant algorithm for finding a multi-coloring in parallel was
found by&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#jopl94,Luby:parallel">[jopl94,Luby:parallel]</a>
:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Assign a random value to each variable.
<li>
Find the variables that have a higher random value than all their
  neighbors; that is color&nbsp;1.
<li>
Then find the variables that have a higher random value than all
  their neighbors that are not of color&nbsp;1. That is color&nbsp;2.
<li>
Iterate until all points are colored.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">

<h3><a id="Irregulariterationspaces">6.8.3</a> Irregular iteration spaces</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Irregulariterationspaces">Irregular iteration spaces</a>
</p>

</p>

<p name="switchToTextMode">
Applying a computational 
<i>stencil</i>
, in the context of
<i>explicit time-stepping</i>
or as a sparse matrix-vector product, is parallel. However, in
practice it may not be trivial to split the iteration space. If the
iteration space is a Cartesian brick, it is easy, even with nested
parallelism. However, in case of symmetry it becomes harder to make an
even
<i>load distribution</i>
<!-- index -->
.
A&nbsp;typical iteration space then looks like:
<!-- environment: verbatim start embedded generator -->
</p>
for (i=0; i&lt;N; i++)
  for (j=i; j&lt;N; j++)
    for (k=j; k&lt;N; k++)
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
In some cases (see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Briggs:CMB">[Briggs:CMB]</a>
) the bounds can be even more
complicated, such as 
\verb/j=i+i%2/ or 
\verb/k&lt;max(i+j,N)/.
</p>

<p name="switchToTextMode">
In such cases the following can be done:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
The loop is traversed to count the total number of inner
  iterations; this is then divide in as many parts as there processes.
<li>
The loop is traversed to find the starting and ending 
<tt>i,j,k</tt>

  values for each process.
<li>
The loop code is then rewritten so that it can run over such an
  
<tt>i,jk</tt>
 subrange.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Orderingforcacheefficiency">6.8.4</a> Ordering for cache efficiency</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Orderingforcacheefficiency">Ordering for cache efficiency</a>
</p>

</p>

<p name="switchToTextMode">
The 
<i>performance of stencil operations</i>
 is often quite
low. The operations have no obvious cache reuse; often they resemble
<i>stream operations</i>
 where long streams of data are retrieved
from memory and used only once. If only a single stencil evaluation
was done, that would be the end of the story. However, usually we do
many such updates, and we can apply techniques similar to
<i>loop tiling</i>
 described in section&nbsp;
1.7.8
.
</p>

<p name="switchToTextMode">
We can do better than with plain tiling if we take the shape of the
stencil into account.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/oblivious-stencil-1.jpeg" width=800></img>
<img src="graphics/oblivious-stencil-2}." width=800></img>
<p name="switchToTextMode">
  \caption{First and second step in cache-optimized iteration space
    traversal}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
6.25
 shows (left) how we first compute a
time-space trapezoid that is cache-contained. Then (right) we compute
another cache-contained trapezoid that builds on the
first&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Frigo:2007:oblivious-stencil">[Frigo:2007:oblivious-stencil]</a>
.
</p>

<p name="switchToTextMode">
\Level 0 {Parallelism in solving linear systems from 
<span title="acronym" ><i>PDEs</i></span>
}
</p>

<p name="switchToTextMode">
The numerical solution of 
<span title="acronym" ><i>PDEs</i></span>
 is an important activity,
and the precision required often makes it a prime candidate for
parallel treatment. If we wonder just how parallel we can be,
and in particular what sort of a speedup is attainable,
we need to distinguish between various aspects of the question.
</p>

<p name="switchToTextMode">
First of all we can ask if there is any intrinsic parallelism
in the problem. On a global level this will typically not be the case
(if parts of the problem were completely uncoupled, then they would be
separate problems, right?) but on a smaller level there may be
parallelism.
</p>

<p name="switchToTextMode">
For instance, looking at time-dependent problems, and referring to
section&nbsp;
4.2.1
, we can say that every next time
step is of course dependent on the previous one, but not every
individual point on the next time step is dependent on every point in
the previous step: there is a 
<i>region of influence</i>
.
Thus it may be possible to partition the problem domain and
obtain parallelism.
</p>

<h3><a id="Operatorsplitting">6.8.5</a> Operator splitting</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Orderingstrategiesandparallelism">Ordering strategies and parallelism</a> > <a href="parallellinear.html#Operatorsplitting">Operator splitting</a>
</p>
<p name="switchToTextMode">

In some contexts, it is necessary to perform implicit calculations
through all directions of a two or three-dimensional array. For
example, in section&nbsp;
4.3
 you saw how the implicit solution
of the heat equation
gave rise to repeated systems
<!-- environment: equation start embedded generator -->
</p>
  (\alpha I+\frac {d^2}{dx^2}+\frac{d^2}{dy^2})u^{(t+1)}=u^{(t)}
\label{eq:heat-recap}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
Without proof, we state that the time-dependent problem can also be solved by
<!-- environment: equation start embedded generator -->
</p>
  (\beta I+\frac {d^2}{dx^2})(\beta I+\frac{d^2}{dy^2})u^{(t+1)}=u^{(t)}
\label{eq:adi-recap}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
for suitable&nbsp;$\beta$. This scheme will not compute the same
values on each individual time step, but it will converge to the same
steady state. The scheme can also be used as a preconditioner in the
<span title="acronym" ><i>BVP</i></span>
 case.
</p>

<p name="switchToTextMode">
This approach has considerable advantages, mostly in terms of
operation counts: the original system has to be solved either making a
factorization of the matrix, which incurs 
<i>fill-in</i>
, or by
solving it iteratively.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze the relative merits of these approaches, giving rough
  operation counts. Consider both the case where $\alpha$ has
  dependence on&nbsp;$t$ and where it does not. Also discuss the expected
  speed of various operations.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

A further advantage appears when we consider the parallel solution
of&nbsp;\eqref{eq:adi-recap}. Note that we have a two-dimensional set of
variables&nbsp;$u_{ij}$, but the operator $I+d^2u/dx^2$ only connects
$u_{ij},u_{ij-1},u_{ij+1}$. That is, each line corresponding to an
$i$&nbsp;value can be processed independently. Thus, both operators can be
solved fully parallel using a one-dimensional partition on the domain.
The solution of a the system in&nbsp;\eqref{eq:heat-recap}, on the other
hand, has limited parallelism.
</p>

<p name="switchToTextMode">
Unfortunately, there is a serious complication: the operator in $x$
direction needs a partitioning of the domain in on direction, and the
operator in $y$ in the other. The solution usually taken is to
transpose the $u_{ij}$ value matrix in between the two solves, so that
the same processor decomposition can handle both. This transposition
can take a substantial amount of the processing time of each time step.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Discuss the merits of and problems with a two-dimensional
  decomposition of the domain, using a grid of $P=p\times p$
  processors. Can you suggest a way to ameliorate the problems?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

One way to speed up these calculations, is to replace the implicit
solve, by an explicit operation; see
section&nbsp;
6.9.3
.
</p>

<h2><a id="Parallelismandimplicitoperations">6.9</a> Parallelism and implicit operations</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelismandimplicitoperations">Parallelism and implicit operations</a>
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In the discussion of 
<span title="acronym" ><i>IBVPs</i></span>
 (section~
4.1.2.2
) you
saw that implicit operations can have great advantages from the point
of numerical stability. However, you also saw that they make the
difference between methods based on a simple operation such as the
matrix-vector product, and ones based on the more complicated linear
system solution. There are further problems with implicit methods when
you start computing in parallel.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Let $A$ be the matrix
<!-- environment: equation start embedded generator -->
</p>
\begin{pmatrix}
    a\_{11}&&\emptyset\\ a\_{21}&a\_{22}\\ &\ddots&\ddots\\
    \emptyset&&a\_{n,n-1}&a\_{nn}
\end{pmatrix}
.
\label{eq:ex:bidiagonal}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
 A=
  Show that the matrix vector product $y\leftarrow Ax$ and the system
  solution $x\leftarrow A\inv y$, obtained by solving the
    triangular system $Ax=y$, not by inverting&nbsp;$A$, have the same
  operation count.
</p>

<p name="switchToTextMode">
  Now consider parallelizing the product $y\leftarrow Ax$. Suppose we
  have $n$ processors, and each processor&nbsp;$i$ stores $x_i$ and the
  $i$-th row of&nbsp;$A$. Show that the product $Ax$ can be computed without
  idle time on any processor but the first.
</p>

<p name="switchToTextMode">
  Can the same be done for the solution of the triangular system
  $Ax=y$? Show that the straightforward implementation has every
  processor idle for an $(n-1)/n$ fraction of the computation.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

We will now see a number of ways of dealing with this inherently
sequential component.
</p>

<h3><a id="Wavefronts">6.9.1</a> Wavefronts</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelismandimplicitoperations">Parallelism and implicit operations</a> > <a href="parallellinear.html#Wavefronts">Wavefronts</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Above, you saw that solving a lower triangular system of size&nbsp;$N$ can
have sequential time complexity of $N$ steps. In practice, things are
often not quite that bad. Implicit algorithms such as solving a
triangular system are inherently sequential, but the number of
steps can be less than is apparent at first.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Take another look at the matrix from a two-dimensional 
<span title="acronym" ><i>BVP</i></span>
 on
  the unit square,
  discretized with central differences.
  Derive the matrix structure if we
  order the unknowns by diagonals. What can you say about the sizes of
  the blocks and the structure of the blocks themselves?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Let us take another look at
figure&nbsp;
4.1
 that describes the
<i>finite difference stencil</i>
of a two-dimensional 
<span title="acronym" ><i>BVP</i></span>
. The corresponding picture for the
<i>stencil of the lower triangular factor</i>
 is
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/laplacelower.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{The difference stencil of the $L$ factor of the matrix
    of a two-dimensional 
<span title="acronym" ><i>BVP</i></span>
}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
in figure&nbsp;
6.26
. This describes the sequentiality of
the lower triangular solve process $x\leftarrow L\inv y$:
\[
 x_k = y_k - \ell_{k,k-1}x_{k-1} - \ell_{k,k-n}x_{k-n} 
\]
In other words, the value at point&nbsp;$k$ can be found if its neighbors
to the left (that is, variable $k-1$) and below (variable $k-n$) are
known.
</p>

<p name="switchToTextMode">
Turning this around, we see that, if we know $x_1$, we can not only
find $x_2$, but also $x_{n+1}$. In the next step we can determine
$x_3$, $x_{n+2}$, and $x_{2n+1}$. Continuing this way, we can solve
$x$ by 
<i>wavefronts</i>
: the values of $x$ on each wavefront are
independent, so they can be solved in parallel in the same sequential
step.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Finish this argument. What is the maximum number of processors we
  can employ, and what is the number of sequential steps? What is the
  resulting efficiency?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Of course you don't have to use actual parallel processing
to exploit this parallelism. Instead you could use a
<i>vector processor</i>
, 
<i>vector instructions</i>
,
or a 
<i>GPU</i>
&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Liu:cudasw2009">[Liu:cudasw2009]</a>
.
</p>

<p name="switchToTextMode">
In section&nbsp;
5.4.3.5
 you saw the
<i>Cuthill-McKee ordering</i>
 for reducing the fill-in of a
matrix. We can modify this algorithm as follows to give wavefronts:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Take an arbitrary node, and call that `level zero'.
<li>
For level&nbsp;$n+1$, find points connected to
  level&nbsp;$n$, that are not themselves connected.
<li>
For the so-called `reverse Cuthill-McKee ordering', reverse the
  numbering of the levels.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  This algorithm is not entirely correct. What is the problem; how can
  you correct it? Show that the resulting permuted matrix is no
  longer tridiagonal, but will likely still have a band structure.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<!-- index -->
<p name="switchToTextMode">

<h3><a id="Recursivedoubling">6.9.2</a> Recursive doubling</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelismandimplicitoperations">Parallelism and implicit operations</a> > <a href="parallellinear.html#Recursivedoubling">Recursive doubling</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
Recursions $y_{i+1} = a_i  y_i + b_i $,
such as appear in solving a bilinear set of equations
(see exercise&nbsp;
4.2.2
),
seem intrinsically sequential.
However, you  already saw in
exercise&nbsp;
1.3
 how, at the cost of some preliminary
operations, the computation can be parallelized.
</p>

<p name="switchToTextMode">
We will now formalize this strategy, generally known as
<i>recursive doubling</i>
<i>bidiagonal matrix</i>
from&nbsp;\eqref{eq:ex:bidiagonal} and scale it to be of the normalized form
\[
\begin{pmatrix}
    1&&\emptyset\\ b_{21}&1\\ &\ddots&\ddots\\
    \emptyset&&b_{n,n-1}&1
\end{pmatrix}
\]
which we write as $A=I+B$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that the scaling to normalized form can be done by multiplying
  with a diagonal matrix.
  How does solving the system $(I+B)x=y$ help in solving $Ax=y$? What
  are the operation counts of solving the system in the two different ways?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Now we do something that looks like Gaussian elimination, except that
we do not start with the first row, but the second. (What would happen
if you did Gaussian elimination or LU decomposition on the matrix
$I+B$?) We use the second row to eliminate&nbsp;$b_{32}$:
\[
\begin{pmatrix}
    1&&&\emptyset\\ &1\\ &-b_{32}&1\\ &&&\ddots\\
    \emptyset&&&&1
\end{pmatrix}
\times
\begin{pmatrix}
    1&&&\emptyset\\ b_{21}&1\\ &b_{32}&1\\ &&\ddots&\ddots\\
    \emptyset&&&b_{n,n-1}&1
\end{pmatrix}
  =
\begin{pmatrix}
    1&&&\emptyset\\ b_{21}&1\\ -b_{32}b_{21}&0&1\\
    \emptyset&&b_{n,n-1}&1
\end{pmatrix}
\]
which we write as $L^{(2)}A=A^{(2)}$. We also compute
$L^{(2)}y=y^{(2)}$ so that $A^{(2)}x=y^{(2)}$ has the same solution as
$Ax=y$. Solving the transformed system gains us a little: after we
compute&nbsp;$x_1$, $x_2$&nbsp;and&nbsp;$x_3$ can be computed in parallel.
</p>

<p name="switchToTextMode">
Now we repeat this elimination process by using the fourth row to
eliminate&nbsp;$b_{54}$, the sixth row to eliminate&nbsp;$b_{76}$, et
cetera. The final result is, summarizing all&nbsp;$L^{(i)}$ matrices:
{\small
\[
\begin{pmatrix}
    1&&&&&&&\emptyset\\ 0&1\\ &-b_{32}&1\\ &&0&1\\
    &&&-b_{54}&1\\ &&&&0&1\\ &&&&&-b_{76}&1\\ &&&&&&\ddots&\ddots\\
\end{pmatrix}
\times (I+B) =
\begin{pmatrix}
    1&&&&&&&\emptyset\\ b_{21}&1\\ -b_{32}b_{21}&0&1\\
    &&b_{43}&1\\ &&-b_{54}b_{43}&0&1\\
    &&&&b_{65}&1\\ &&&&-b_{76}b_{65}&0&1\\ &&&&&\ddots&\ddots&\ddots
\end{pmatrix}
\]
}
which we write as $L(I+B)=C$, and solving $(I+B)x=y$ now becomes
$Cx=L\inv y$.
</p>

<p name="switchToTextMode">
This final result needs close investigation.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
First of all, computing $y'=L\inv y$ is simple. (Work out the
  details. How much parallelism is available?)
<li>
Solving $Cx=y'$ is still sequential, but it no longer takes $n$
  steps: from $x_1$ we can get $x_3$, from that we get&nbsp;$x_5$, et
  cetera. In other words, there is only a sequential relationship
  between the odd numbered components of&nbsp;$x$.
<li>
The even numbered components of&nbsp;$x$ do not depend on each other,
  but only on the odd components: $x_2$&nbsp;follows from&nbsp;$x_1$, $x_4$
  from&nbsp;$x_3$, et cetera. Once the odd components have been computed,
  admittedly sequentially, this step is fully parallel.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
We can describe the sequential solving of the odd components by
itself:
\[
\begin{pmatrix}
    1&&\emptyset\\ c_{21}&1\\ &\ddots&\ddots\\
    \emptyset&&c_{n,n-1}&1
\end{pmatrix}
\begin{pmatrix}
    x_1\\ x_3\\ \vdots\\ x_n
\end{pmatrix}
 =
\begin{pmatrix}
    y'_1\\ y'_3\\ \vdots\\ y'_n
\end{pmatrix}
\]
where $c_{i+1i}=-b_{2n+1,2n}b_{2n,2n-1}$. In other words, we have
reduced a size&nbsp;$n$ sequential problem to a sequential problem of the
size kind and a parallel problem, both of size&nbsp;$n/2$. Now we can
repeat this procedure recursively, reducing the original problem to a
sequence of parallel operations, each half the size of the former.
</p>

<p name="switchToTextMode">
The process of computing all partial sums through recursive doubling
is also referred to as a parallel
<i>prefix operation</i>
. Here we use a prefix sum, but in the abstract it
can be applied to any associative operator.
</p>

<!-- index -->
<p name="switchToTextMode">

<h3><a id="Approximatingimplicitbyexplicitoperations,seriesexpansion">6.9.3</a> Approximating implicit by explicit operations, series expansion</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Parallelismandimplicitoperations">Parallelism and implicit operations</a> > <a href="parallellinear.html#Approximatingimplicitbyexplicitoperations,seriesexpansion">Approximating implicit by explicit operations, series expansion</a>
</p>

</p>

<p name="switchToTextMode">
There are various reasons why it is sometimes allowed to replace an
implicit operation, which, as you saw above, can be problematic in
practice, by a different one that is practically more advantageous.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Using an explicit method for the heat equation
  (section&nbsp;
4.3
) instead of an implicit one is equally
  legitimate, as long as we observe 
<i>step size</i>
 restrictions
  on the explicit method.
<li>
Tinkering with the preconditioner
  (section&nbsp;
5.5.8
) in an iterative method is allowed,
  since it will only affect the speed of convergence, not the solution
  the method converges to. You already saw one example of this general
  idea in the 
<i>block Jacobi</i>
 method;
  section&nbsp;
6.7.3
. In the rest of this section you will
  see how recurrences in the preconditioner, which are implicit
  operations, can be replaced by explicit operations, giving various
  computational advantages.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Solving a linear system is a good example of an implicit operation,
and since this comes down to solving two triangular systems, let us
look at ways of finding a computational alternative to solving a lower
triangular system. If $U$ is upper triangular and nonsingular, we let
$D$ be the diagonal of&nbsp;$U$, and we write $U=D(I-B)$ where $B$ is an
upper triangular matrix with a zero diagonal, also called a
<i>strictly upper triangular matrix</i>
; we say that $I-B$ is
a 
<i>unit upper triangular matrix</i>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Let $A=LU$ be an LU factorization where $L$ has ones on the
  diagonal. Show how solving a system $Ax=b$ can be done, involving
  only the solution of unit upper and lower triangular systems. Show
  that no divisions are needed during the system solution.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Our operation of interest is now solving the system $(I-B)x=y$. We
observe that
<!-- environment: equation start embedded generator -->
</p>
  (I-B)\inv = I+B+B^2+\cdots
\label{eq:neuman-series}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
and $B^n=0$ where $n$ is the matrix size (check
this!), so we can solve $(I-B) x = y$ exactly by
\[
 x = \sum_{k=0}^{n-1}B^k y. 
\]
Of course, we want to avoid computing the powers&nbsp;$B^k$ explicitly, so
we observe that
<!-- environment: equation start embedded generator -->
</p>
 \sum\_{k=0}^1B^ky = (I+B)y,\quad \sum\_{k=0}^2B^ky =
 (I+B(I+B))y,\quad \sum\_{k=0}^3B^ky = (I+B(I+B((I+B))))y,
\label{eq:horner}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
et cetera.
The resulting algorithm for evaluating $\sum_{k=0}^{n-1}B^k y$ is
called 
<i>Horner's rule</i>
, and you see that it avoids computing
matrix powers&nbsp;$B^k$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Suppose that $I-B$ is bidiagonal.  Show that the above calculation
  takes $n(n+1)$ operations. What is the operation count for computing
  $(I-B)x=y$ by triangular solution?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

We have now turned an implicit operation into an explicit one, but
unfortunately one with a high operation count. In practical
circumstances, however, we can truncate the sum of matrix powers.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Let $A$ be the tridiagonal matrix
\[
 A=
\begin{pmatrix}
  2&-1&&&\emptyset\\ -1&2&-1\\ &\ddots&\ddots&\ddots\\
  &&&&-1\\ \emptyset&&&-1&2
\end{pmatrix}
\]
of the one-dimensional 
<span title="acronym" ><i>BVP</i></span>
 from section&nbsp;
4.2.2
.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Recall the definition of diagonal dominance in
  section&nbsp;
5.3.4
. Is this matrix diagonally dominant?
<li>
Show that the pivots in an LU factorization of this matrix
  (without pivoting) satisfy a recurrence.  Hint: show that after $n$
  elimination steps ($n\geq0$) the remaining matrix looks like
\[
 A^{(n)}=
\begin{pmatrix}
  d_n&-1&&&\emptyset\\ -1&2&-1\\ &\ddots&\ddots&\ddots\\
  &&&&-1\\ \emptyset&&&-1&2
\end{pmatrix}
\]
  and show the relation between $d_{n+1}$ and&nbsp;$d_n$.
<li>
Show that the sequence $n\mapsto d_n$ is descending, and derive
  the limit value.
<li>
Write out the $L$ and $U$ factors in terms of the $d_n$ pivots.
<li>
Are the $L$ and $U$ factors diagonally dominant?
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: answer start embedded generator -->
</p>

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The above exercise implies (note that we did not actually prove it!)
that for matrices from 
<span title="acronym" ><i>BVPs</i></span>
 we find that $B^k\downarrow0$, in
element size and in norm. This means that we can approximate the solution
of $(I-B)x=y$ by, for instance, $x=(I+B)y$ or $x=(I+B+B^2)y$.
Doing this still has a higher operation count than the direct
triangular solution, but it is computationally advantageous in at
least two ways:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The explicit algorithm has a better pipeline behavior.
<li>
The implicit algorithm has problems in parallel, as you have
  seen; the explicit algorithm is more easily parallelized.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Of course, this
approximation may have further implications for the stability of the
overall numerical algorithm.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Describe the parallelism aspects of Horner's rule;
  equation&nbsp;\eqref{eq:horner}.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">

<h2><a id="Gridupdates">6.10</a> Grid updates</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Gridupdates">Grid updates</a>
</p>
</p>

<p name="switchToTextMode">
One of the conclusions of chapter&nbsp;
Numerical treatment of differential equations
 was that explicit
methods for time-dependent problems are computationally easier than
implicit ones.  For instance, they typically involve a matrix-vector
product rather than a system solution, and parallelizing explicit
operations is fairly simple: each result value of the matrix-vector
product can be computed independently. That does not mean that there
are other computational aspects worth remarking on.
</p>

<p name="switchToTextMode">
Since we are dealing with sparse matrices, stemming from some
computational
<i>stencil</i>
,
we take the operator point of view. In figures 
6.11
and&nbsp;
6.12
 you saw how applying a stencil in each point of the domain
induces certain relations between processors: in order to evaluate the matrix-vector
product $y\leftarrow Ax$ on a processor, that processor needs to obtain the $x$-values
of its 
<i>ghost region</i>
. Under reasonable assumptions on the partitioning
of the domain over the processors, the number of messages involved will be fairly
small.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
Reason that, in a 
<span title="acronym" ><i>FEM</i></span>
 or 
<span title="acronym" ><i>FDM</i></span>
 context,
the number of messages is $O(1)$ as&nbsp;$h\downarrow 0$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In section&nbsp;
1.6.1
 you saw that the matrix-vector product has
little data reuse, though there is some locality to the computation;
in section&nbsp;
5.4.1.4
 it was pointed out that the locality of the
sparse matrix-vector product is even worse because of
indexing schemes that the sparsity necessitates. This means that the sparse
product is largely a 
<i>bandwidth-bound algorithm</i>
.
</p>

<p name="switchToTextMode">
Looking at just a
single product there is not much we can do about that.
However,
often we do a number of such products in a row, for instance as the steps
in a time-dependent process. In that case there may be rearrangements
of the operations that lessen the bandwidth demands. Consider as a simple example
<!-- environment: equation start embedded generator -->
</p>
\forall\_i\colon x^{(n+1)}\_i = f\bigl( x^{(n)}\_i, x^{(n)}\_{i-1}, x^{(n)}\_{i+1} \bigr)
\label{eq:3p-average}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">

and let's assume that the set $\{x^{(n)}_i\}_i$ is too large to fit
in cache.
This is a model for, for instance, the explicit scheme for the heat
equation in one space dimension; section&nbsp;
4.3.1.1
.
Schematically:
\[
\begin{array}{ccccc}
  x^{(n)}_0&x^{(n)}_1&x^{(n)}_2\\
  \downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow\\
  x^{(n+1)}_0&x^{(n+1)}_1&x^{(n+1)}_2\\
  \downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow\\
  x^{(n+2)}_0&x^{(n+2)}_1&x^{(n+2)}_2\\
\end{array}
\]
In the ordinary computation, where we first compute all&nbsp;$x^{(n+1)}_i$,
then all&nbsp;$x^{(n+2)}_i$, the intermediate values at level&nbsp;$n+1$
will be flushed from the cache
after they were generated, and then brought back into cache as input for the
level $n+2$ quantities.
</p>

<p name="switchToTextMode">
However,
if we compute not one, but two iterations, the intermediate values
may stay in cache.
Consider $x^{(n+2)}_0$: it requires $x^{(n+1)}_0,x^{(n+1)}_1$,
which in turn require $x^{(n)}_0,&hellip;,x^{(n)}_2$.
</p>

<p name="switchToTextMode">
Now suppose that we are not interested in the intermediate results, but
only the final iteration. Figure&nbsp;
6.27
 shows
a simple example.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/grid-update-overlap.jpeg" width=800></img>
<p name="caption">
FIGURE 6.27: Computation of blocks of grid points over multiple iterations
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
The first processor computes 4&nbsp;points on level $n+2$. For this it needs 5&nbsp;points
from level $n+1$, and these need to be computed too, from 6&nbsp;points on level&nbsp;$n$.
We see that a processor apparently needs to collect a 
<i>ghost region</i>
of width two, as opposed to just one for the regular single step update.
One of the points computed by the first processor is $x^{(n+2)}_3$,
which needs $x^{(n+1)}_4$. This point is also needed for the computation
of $x^{(n+2)}_4$, which belongs to the second processor.
</p>

<p name="switchToTextMode">
The easiest solution is to let this sort of point on the intermediate
level 
<i>redundantly computed</i>

<!-- index -->
, in
the computation of both blocks where it is needed, on two different processors.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you think of cases where a point would be redundantly computed by
  more than two processors?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

We can give several interpretations to this scheme of computing multiple
update steps by blocks.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
First of all, as we motivated above, doing this
on a single processor increases locality: if all points in a colored block
(see the figure) fit in cache, we get reuse of the intermediate points.
<li>
Secondly, if we consider this as a scheme for distributed memory computation,
it reduces message traffic. Normally, for every update step the processors
need to exchange their boundary data. If we accept some redundant duplication
of work, we can now eliminate the data exchange for the intermediate levels.
The decrease in communication will typically outweigh the increase in work.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Discuss the case of using this strategy for multicore computation.
  What are the savings? What are the potential pitfalls?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Analysis">6.10.1</a> Analysis</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Gridupdates">Grid updates</a> > <a href="parallellinear.html#Analysis">Analysis</a>
</p>
</p>

<p name="switchToTextMode">
Let's analyze the algorithm we have just sketched.  As in
equation&nbsp;\eqref{eq:3p-average} we limit ourselves to a 1D set of
points and a function of three points. The parameters describing the
problem are these:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$N$ is the number of points to be updated, and $M$&nbsp;denotes the
  number of update steps. Thus, we perform $MN$ function evaluations.
<li>
$\alpha,\beta,\gamma$ are the usual parameters describing
  latency, transmission time of a single point, and time for an
  operation (here taken to be an $f$ evaluation).
<li>
$b$ is the number of steps we block together.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
Each halo communication consists of $b$ points, and we do this $\sqrt
N/b$ many times.  The work performed consists of the $MN/p$ local
updates, plus the redundant work because of the halo. The latter term
consists of $b^2/2$ operations, performed both on the left and right
side of the processor domain.
</p>

<p name="switchToTextMode">
Adding all these terms together, we find a cost of
\[
 \frac Mb\alpha+M\beta+\left(\frac {MN}p+Mb\right)\gamma. 
\]
We observe that the overhead of $\alpha M/b+\gamma Mb$ is independent of&nbsp;$p$,
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Compute the optimal value of&nbsp;$b$, and remark that it only depends on
  the architectural parameters $\alpha,\beta,\gamma$ but not on the
  problem parameters.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Communicationandworkminimizingstrategy">6.10.2</a> Communication and work minimizing strategy</h3>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Gridupdates">Grid updates</a> > <a href="parallellinear.html#Communicationandworkminimizingstrategy">Communication and work minimizing strategy</a>
</p>
</p>

<p name="switchToTextMode">
We can make this algorithm more efficient by
<i>overlapping computation with communication</i>
.
As illustrated in
figure&nbsp;
6.28
, each processor start by
communicating its halo, and overlapping this communication with the
part of the communication that can be done locally. The values that
depend on the halo will then be computed last.
</p>

<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/grid-update-local.jpeg" width=800></img>
<p name="caption">
FIGURE 6.28: Computation of blocks of grid points over multiple iterations
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  What is a great practical problem with organizing your code (with
  the emphasis on `code'!) this way?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

If the number of points per processor is large enough, the amount of
communication is low relative to the computation, and you could take
$b$ fairly large. However, these grid updates are mostly used in
iterative methods such as the 
<i>CG</i>
 method
(section&nbsp;
5.5.11
), and in that case considerations of roundoff
prevent you from taking $b$ too large
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#ChGe:sstep">[ChGe:sstep]</a>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Go through the complexity analysis for the non-overlapping algorithm
  in case the points are organized in a 2D grid. Assume that each
  point update involves four neighbors, two in each coordinate
  direction.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

A further refinement of the above algorithm is possible.
Figure&nbsp;
6.29
 illustrates that it is possible
to use a halo region that uses different points from different time steps.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/grid-update-minimal.jpeg" width=800></img>
<p name="caption">
FIGURE 6.29: Computation of blocks of grid points over multiple iterations
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
This algorithm (see&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Demmel2008IEEE:avoiding">[Demmel2008IEEE:avoiding]</a>
) cuts down on the amount
of redundant computation. However, now the halo values that are communicated
first need to be computed, so this requires splitting the local communication
into two phases.
</p>

<h2><a id="Blockalgorithmsonmulticorearchitectures">6.11</a> Block algorithms on multicore architectures</h2>
<p name=crumbs>
crumb trail:  > <a href="parallellinear.html">parallellinear</a> > <a href="parallellinear.html#Blockalgorithmsonmulticorearchitectures">Block algorithms on multicore architectures</a>
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

In section~
5.3.7
 you saw that certain linear algebra
algorithms can be formulated in terms of submatrices. This point of
view can be beneficial for the efficient execution of linear algebra
operations on shared memory architectures such as current
<i>multicore</i>
 processors.
</p>

<p name="switchToTextMode">
\newcommand\chol{\mathop{\mathrm{Chol}}}
As an example, let us consider the 
<i>Cholesky factorization</i>
,
which computes $A=LL^t$ for a symmetric positive definite matrix~$A$;
see also section~
5.3.2
.
Recursively, we can describe the algorithm as follows:
\[
 \chol
\begin{pmatrix}
  A_{11}&A_{21}^t\\ A_{21}&A_{22}
\end{pmatrix}
 = LL^t\qquad\hbox{where}\quad L=
\begin{pmatrix}
  L_{11}&0\\ \tilde A_{21} &\chol(A_{22}-\tilde A_{21}\tilde A_{21}^t)
\end{pmatrix}
\]
and where $\tilde A_{21}=A_{21}L_{11}\invt,$ $A_{11}=L_{11}L_{11}^t$.
</p>

<p name="switchToTextMode">
In practice, the block implementation is applied to a partitioning
\[
\left(
\begin{array}{c|c|cc}
  &\multicolumn{2}{c}{\mathrm{finished}}\\ \hline
  \multirow{2}{*}{}&A_{kk}&A_{k,>k}&\\ \hline
  &A_{>k,k}&A_{>k,>k}
\end{array}
\right)
\]
where $k$ is the index of the current block row, and the
factorization is finished for all indices~$<k$.
The factorization is written as follows,
using Blas names for the operations:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for $k=1,\mathrm{nblocks}$:<br>
&nbsp;        <br>
<tt>Chol</tt><br>
: factor $L_kL_k^t\leftarrow A_{kk}$<br>
&nbsp;        <br>
<tt>Trsm</tt><br>
: solve $\tilde A_{>k,k} \leftarrow A_{>k,k}L_k\invt$<br>
&nbsp;        <br>
<tt>Gemm</tt><br>
: form the product $\tilde A_{>k,k}\tilde A_{>k,k}^t$<br>
&nbsp;        <br>
<tt>Syrk</tt><br>
: symmmetric rank-$k$ update<br>
    $A_{>k,>k}\leftarrow A_{>k,>k}-\tilde A_{>k,k}\tilde A_{>k,k}^t$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">
The key to parallel performance is to partition the indices~$>k$ and
write the algorithm in terms of these blocks:
\[
\left(
\begin{array}{c|c|cc}
  &\multicolumn{2}{c}{\mathrm{finished}}\\ \hline
  \multirow{2}{*}{}&A_{kk}&A_{k,k+1}&A_{k,k+2}\cdots\\ \hline
  &A_{k+1,k}&A_{k+1,k+1}&A_{k+1,k+2}\cdots\\
  &A_{k+2,k}&A_{k+2,k+2}\\
  &\vdots&\vdots
\end{array}
\right)
\]
The algorithm now gets an extra level of inner loops:
<!-- environment: quotation start embedded generator -->
</p>
<!-- TranslatingLineGenerator quotation ['quotation'] -->
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for $k=1,\mathrm{nblocks}$:<br>
&nbsp;        <br>
<tt>Chol</tt><br>
: factor $L_kL_k^t \leftarrow A_{kk}$<br>
&nbsp;        for $\ell>k$:<br>
&nbsp;&nbsp;        <br>
<tt>Trsm</tt><br>
: solve $\tilde A_{\ell,k} \leftarrow A_{\ell,k}L_k\invt$<br>
&nbsp;        for $\ell_1,\ell_2>k$:<br>
&nbsp;&nbsp;        <br>
<tt>Gemm</tt><br>
: form the product $\tilde A_{\ell_1,k}\tilde A_{\ell_2,k}^t$<br>
&nbsp;        for $\ell_1,\ell_2>k$, $\ell_1\leq\ell_2$:<br>
&nbsp;&nbsp;        <br>
<tt>Syrk</tt><br>
: symmmetric rank-$k$ update<br>
    $A_{\ell_1,\ell_2}\leftarrow A_{\ell_1,\ell_2}<br>
    -\tilde A_{\ell_1,k}\tilde A_{\ell_2,k}^t$<br>
</p>
<!-- environment: tabbing end embedded generator -->
</quotation>
<!-- environment: quotation end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/chol4dag.jpg" width=800></img>
<p name="switchToTextMode">
  \caption{Graph of task dependencies in a $4\times4$ Cholesky
    factorization}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Now it is clear that the algorithm has a good deal of parallelism: the
iterations in every $\ell$-loop can be processed independently.
However, these loops get shorter in every iteration of the outer
$k$-loop, so it is not immediate how many processors we can
accommodate. Moreover, it is not necessary to preserve the order of
operations of the algorithm above. For instance, after
\[
  L_1L_1^t=A_{11},\quad A_{21}\leftarrow A_{21}L_1\invt,\quad
  A_{22}\leftarrow A_{22}-A_{21}A_{21}^t
\]
the factorization $L_2L_2^t=A_{22}$ can start, even if the rest of the
$k=1$ iteration is still unfinished. Thus, there is probably a lot more
parallelism than we would get from just parallelizing the inner loops.
</p>

<p name="switchToTextMode">
The best way to approach parallelism in this case is to shift away
from a 
<i>control flow</i>
 view of the algorithm, where the
sequence of operations is prescribed, to a 
<i>data flow</i>
 view.
In the latter only data dependencies are indicated, and any ordering of
operations that obeys these dependencies is allowed. (Technically, we
abandon the program order of the tasks and replace it with a
<i>partial ordering</i>
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
{Let's write $a\leq b$ if $a$~is
  executed before~$b$, then the relation~$\cdot\leq\cdot$ is a partial
  order if $a\leq b\wedge b\leq a\Rightarrow a=b$ and $a\leq b\wedge
  b\leq c\Rightarrow a\leq c$. The difference with a total ordering,
  such as program ordering, is that it is not true that $a\leq b\vee
  b\leq a$: there can be pairs that are not ordered, meaning that
  their time ordering is not prescribed.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
  .)  The best way of
representing the data flow of an algorithm is by constructing a
<i>DAG</i>
 (see section~
app:graph
 for a brief tutorial on
graphs) of tasks.  We add an edge $(i,j)$ to the graph if task~$j$
uses the output of task~$i$.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  In section~
2.6.1.6
 you learned the concept of
<i>sequential consistency</i>
: a threaded parallel code program
  should give the same results when executed in parallel as when it's
  executed sequentially. We have just stated that 
<span title="acronym" ><i>DAG</i></span>
-based
  algorithms are free to execute tasks in any order that obeys the
  partial order of the graph nodes. Discuss whether
  sequential consistency is a problem in this context.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In our example, we construct a 
<span title="acronym" ><i>DAG</i></span>
 by making a vertex task for every
inner iteration.
Figure~
6.30
 shows the 
<span title="acronym" ><i>DAG</i></span>
 of all tasks
of matrix of $4\times4$ blocks. This graph is constructed by
simulating the Cholesky algorithm above,
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  What is the diameter of this graph? Identify the tasks that lie on
  the path that determines the diameter. What is the meaning of these
  tasks in the context of the algorithm? This path is called the
<i>critical path</i>
. Its length determines the execution time of the
  computation in parallel, even if an infinite number of processors is
  available.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Assume there are $T$ tasks that all take a unit time to execute, and
  assume we have $p$ processors. What is the
  theoretical minimum time to execute the algorithm? Now amend this
  formula to take into account the critical path; call its length&nbsp;$C$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

In the execution of the tasks a 
<span title="acronym" ><i>DAG</i></span>
, several observations
can be made.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
If more than one update is made to a block, it is probably
  advantageous to have these updates be computed by the same
  process. This simplifies maintaining 
<i>cache coherence</i>
.
<li>
If data is used and later modified, the use must be finished
  before the modification can start. This can even be true if the two
  actions are on different processors, since the memory subsystem
  typically maintains cache coherence, so the modifications can affect the
  process that is reading the data. This case can be remedied by
  having a copy of the data in main memory, giving a reading process
  data that is reserved (see section&nbsp;
1.4.1
).
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">

</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
