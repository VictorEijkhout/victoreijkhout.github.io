<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Numerical treatment of differential equations</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


4.1 : <a href="odepde.html#Initialvalueproblems">Initial value problems</a><br>
4.1.1 : <a href="odepde.html#Errorandstability">Error and stability</a><br>
4.1.2 : <a href="odepde.html#Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">Finite difference approximation: Euler explicit and implicit methods</a><br>
4.1.2.1 : <a href="odepde.html#StabilityoftheEulerexplicitmethod">Stability of the  Euler explicit method</a><br>
4.1.2.2 : <a href="odepde.html#TheEulerimplicitmethod">The Euler implicit method</a><br>
4.1.2.3 : <a href="odepde.html#StabilityoftheimplicitEulermethod">Stability of the implicit Euler method</a><br>
4.2 : <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a><br>
4.2.1 : <a href="odepde.html#GeneralPDEtheory">General PDE theory</a><br>
4.2.1.1 : <a href="odepde.html#Hyperbolicequations">Hyperbolic equations</a><br>
4.2.1.2 : <a href="odepde.html#Parabolicequations">Parabolic equations</a><br>
4.2.1.3 : <a href="odepde.html#Ellipticequations">Elliptic equations</a><br>
4.2.2 : <a href="odepde.html#ThePoissonequationinonespacedimension">The Poisson equation in one space dimension</a><br>
4.2.3 : <a href="odepde.html#ThePoissonequationintwospacedimensions">The Poisson equation in two space dimensions</a><br>
4.2.4 : <a href="odepde.html#Differencestencils">Difference stencils</a><br>
4.2.5 : <a href="odepde.html#Otherdiscretizationtechniques">Other discretization techniques</a><br>
4.3 : <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a><br>
4.3.1 : <a href="odepde.html#Discretization">Discretization</a><br>
4.3.1.1 : <a href="odepde.html#Explicitscheme">Explicit scheme</a><br>
4.3.1.2 : <a href="odepde.html#Implicitscheme">Implicit scheme</a><br>
4.3.2 : <a href="odepde.html#Stabilityanalysis">Stability analysis</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>4 Numerical treatment of differential equations</h1>
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<p name="switchToTextMode">
In this chapter we will look at the numerical solution of
<span title="acronym" ><i>ODEs</i></span>
 and 
<span title="acronym" ><i>PDEs</i></span>
.
These equations
are commonly used in physics to describe phenomena such as the
flow of air around an aircraft, or the bending of a bridge under
various stresses. While these equations are often fairly simple,
getting specific numbers out of them (`how much does this bridge sag
if there are a hundred cars on it') is more complicated, often taking large
computers to produce the desired results. Here we will describe the
techniques that turn 
<span title="acronym" ><i>ODEs</i></span>
 and 
<span title="acronym" ><i>PDEs</i></span>
 into computable problems.
</p>

<p name="switchToTextMode">
First of all, we will look at 
<span title="acronym" ><i>IVPs</i></span>
, which describes processes
that develop in time. Here we consider 
<span title="acronym" ><i>ODEs</i></span>
: scalar functions that
are only depend on time. The name derives from the fact that typically
the function is specified at an initial time point.
</p>

<p name="switchToTextMode">
Next, we will look at 
<span title="acronym" ><i>BVPs</i></span>
, describing processes in space. In
realistic situations, this will concern multiple space variables, so
we have a 
<span title="acronym" ><i>PDE</i></span>
.
The name 
<span title="acronym" ><i>BVP</i></span>
 is explained by the fact that the solution is
specified on the boundary of the domain of definition.
</p>

<p name="switchToTextMode">
Finally, we will consider the `heat equation', an 
<span title="acronym" ><i>IBVP</i></span>
which has
aspects of both 
<span title="acronym" ><i>IVPs</i></span>
 and 
<span title="acronym" ><i>BVPs</i></span>
: it describes heat spreading through a
physical object such as a rod. The initial value describes the initial
temperature, and the boundary values give prescribed temperatures at
the ends of the rod.
</p>

<p name="switchToTextMode">
Our aim in this chapter is to show the origins of an important class
of computational problems. Therefore we will not go into theoretical
matters of existence, uniqueness, or conditioning of solutions. For
this, see~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Heath:scicomp">[Heath:scicomp]</a>
 or any book that is specifically
dedicated to 
<span title="acronym" ><i>ODEs</i></span>
 or 
<span title="acronym" ><i>PDEs</i></span>
.
For ease of analysis we will also assume that all functions involved
have sufficiently many higher derivatives, and that each derivative is
sufficiently smooth.
</p>

<h2><a id="Initialvalueproblems">4.1</a> Initial value problems</h2>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a>
</p>

<!-- index -->
<!-- index -->
<p name="switchToTextMode">

Many physical phenomena change over time, and typically the laws of
physics give a description of the change, rather than of the quantity
of interest itself. For instance, Newton's second law
<!-- environment: equation start embedded generator -->
</p>
  F=ma
\label{eq:f=ma}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
is a
statement about the change in position of a point mass: expressed as
\[
 a(t)=\frac{d^2}{dt^2}x(t)=F/m 
\]
it states that acceleration depends linearly on the force exerted on
the mass. A&nbsp;closed form description $x(t)=&hellip;$ for the location of
the mass can sometimes be
derived analytically, but in many cases some form of approximation or
numerical computation is needed.
This is also known as `numerical integration'.
</p>

<p name="switchToTextMode">
Newton's second law \eqref{eq:f=ma} is an 
<span title="acronym" ><i>ODE</i></span>
 since it describes a function of one
variable, time. It is an 
<span title="acronym" ><i>IVP</i></span>
 since it describes the development
in time, starting with some initial conditions.  As an 
<span title="acronym" ><i>ODE</i></span>
, it is
`of second order' since it involves a second derivative, We can reduce
this to first order, involving only first derivatives, if we allow
vector quantities.
We introduce a two-component vector&nbsp;$u$ that combines the location&nbsp;$x$ and
velocity&nbsp;$x'$, where
we use the
  prime symbol to indicate differentiation in case of functions of a
  single variable:
\[
 u(t)=(x(t),x'(t))^t,
\]
Newton's equation expressed in $u$ then becomes:
\[
 u'=Au+B,\qquad A=
\begin{pmatrix}
  0&1\\ 0& 0
\end{pmatrix}
,\quad B=
\begin{pmatrix}
  0\\ F/a
\end{pmatrix}
\]
</p>

<p name="switchToTextMode">
For simplicity, in this course we will only consider scalar equations;
our reference equation is then
<!-- environment: equation start embedded generator -->
</p>
  u'(t)=f(t,u(t)),\qquad u(0)=u\_0,\qquad t&gt;0.
\label{eq:ode-nonauton}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
Equation&nbsp;\eqref{eq:ode-nonauton} allows for an explicit time dependence
of the process, but in general we only consider equations that
do not have this explicit dependence,
the  so-called `autonomous'  
<span title="acronym" ><i>ODEs</i></span>
 of the form
<!-- environment: equation start embedded generator -->
</p>
\label{eq:ode}
  u'(t)=f(u(t))
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
in which the right hand side does not explicitly depend
on&nbsp;$t$.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
Non-autonomous 
<span title="acronym" ><i>ODE</i></span>
s can be transformed to autonomous
  ones, so this is no limitation. If $u=u(t)$ is a scalar function and
  $f=f(t,u)$, we define $u_2(t)=t$ and consider the equivalent
  autonomous system
  $\big({u'\atop u'_2}\bigr)=\bigl({f(u_2,u)\atop 1}\bigr)$
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

Typically, the initial value in some starting point (often
chosen as $t=0$) is given: $u(0)=u_0$ for some value&nbsp;$u_0$, and we are
interested in the behavior of&nbsp;$u$ as $t\rightarrow\infty$. As an
example, $f(x)=x$ gives the equation $u'(t)=u(t)$. This is a
simple model for population growth: the equation states that the rate
of growth is equal to the size of the population.
The equation&nbsp;\eqref{eq:ode-nonauton} can be solved analytically for some
choices of&nbsp;$f$, but we will not consider this. Instead, we only
consider the numerical solution and the accuracy of this process.
</p>

<p name="switchToTextMode">
In a numerical method, we consider discrete size time steps to
approximate the solution of the continuous time-dependent
process. Since this introduces a certain amount of error, we will
analyze the error introduced in each time step, and how this adds up
to a global error. In some cases, the need to limit the global error
will impose restrictions on the numerical scheme.
</p>

<h3><a id="Errorandstability">4.1.1</a> Error and stability</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a> > <a href="odepde.html#Errorandstability">Error and stability</a>
</p>
<p name="switchToTextMode">

Since numerical computation will always involve the inaccuracies
stemming from the use of machine arithmetic, we want to avoid the
situation where a small perturbation in the initial value leads to
large perturbations in the solution. Therefore, we
will call a
differential equation `stable' if solutions corresponding to different
initial values&nbsp;$u_0$ converge to one another
as&nbsp;$t\rightarrow\infty$.
</p>

<p name="switchToTextMode">
A&nbsp;sufficient criterion for stability is:
\[
 \frac\partial{\partial u}f(u)=
\begin{cases}
    &gt;0&\mbox{unstable}\\ =0&\mbox{neutrally stable}\\ &lt;0&\mbox{stable}
\end{cases}
\]
Proof. If $u^*$ is a zero of&nbsp;$f$, meaning&nbsp;$f(u^*)=0$, then the
constant function $u(t)\equiv u^*$ is a solution of $u'=f(u)$,
a&nbsp;so-called `equilibrium' solution. We will now consider how small
perturbations from the equilibrium behave. Let $u$ be a solution of
the PDE, and write $u(t)=u^*+\eta(t)$, then we have
\[
\begin{array}{r@{{}={}}l}
  \eta'&u'=f(u)=f(u^*+\eta)=f(u^*)+\eta f'(u^*)+O(\eta^2)\\
     &\eta f'(u^*)+O(\eta^2)
\end{array}
\]
Ignoring the second order terms, this has the solution
\[
 \eta(t)=e^{f'(x^*)t} 
\]
which means that the perturbation will damp out if $f'(x^*)&lt;0$,
and amplify if&nbsp;$f'(x^*)&gt;0$.
</p>

<p name="switchToTextMode">
We will often refer to the simple example
$f(u)=-\lambda u$, with solution $u(t)=u_0e^{-\lambda t}$. This
problem is stable if&nbsp;$\lambda&gt;0$.
</p>

<h3><a id="Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">4.1.2</a> Finite difference approximation: Euler explicit and implicit methods</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a> > <a href="odepde.html#Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">Finite difference approximation: Euler explicit and implicit methods</a>
</p>

<!-- index -->
<!-- index -->
<p name="switchToTextMode">

In order to solve the problem numerically, we turn the continuous
problem into a discrete one by looking at finite time/space steps.
Assuming all functions are sufficiently smooth, a straightforward
<i>Taylor series</i>
 expansion
(see appendix&nbsp;
app:taylor
)
gives:
\[
 u(t+\Delta t)=u(t)+u'(t)\Delta t+u''(t)\frac{\Delta t^2}{2!}
+ u'''(t)\frac{\Delta t^3}{3!}+\cdots 
\]
This gives for $u'$:
<!-- environment: equation start embedded generator -->
</p>
\begin{array}{r@{{}={}}l}
  u'(t) & \frac{u(t+\Delta t)-u(t)}{\Delta t}+\frac1{\Delta t}
                \left(u''(t)\frac{\Delta t^2}{2!}
                + u'''(t)\frac{\Delta t^3}{3!}+\cdots\right)\\
        & \frac{u(t+\Delta t)-u(t)}{\Delta t}+\frac1{\Delta t}O(\Delta t^2)\\
        & \frac{u(t+\Delta t)-u(t)}{\Delta t}+O(\Delta t)
\end{array}
\label{eq:forwarddifference}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
We can approximate the infinite sum of higher derivatives by a single
$O(\Delta t^2)$ if all derivatives are bounded; alternatively,
you can show that this sum is equal to $\Delta
t^2u''(t+\alpha\Delta t)$ with $0&lt;\alpha&lt;1$.
</p>

<p name="switchToTextMode">
We see that we can approximate a differential operator by a
<i>finite difference</i>
, with an error that is known in its
order of magnitude as a function of the time step.
</p>

<p name="switchToTextMode">
Substituting this in $u'=f(t,u)$ gives
</p>

<p name="switchToTextMode">
\[
 \frac{u(t+\Delta t)-u(t)}{\Delta t} = f(t,u(t)) +O(\Delta t)
\]
or
<!-- environment: equation start embedded generator -->
</p>
  u(t+\Delta t) = u(t) + \Delta t\,f(t,u(t)) +O(\Delta t^2)
\label{eq:uplusdt-exact}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- TranslatingLineGenerator remark ['remark'] -->
  The preceding two equations are
  mathematical equalities, and should not be interpreted as a way
  of computing $u'$ for a given function&nbsp;$u$. Recalling the discussion
  in section&nbsp;
3.4.4
 you can see that such a formula would
  quickly lead to cancellation for small&nbsp;$\Delta t$. Further discussion
  of numerical differentiation is outside the scope of this book;
  please see any standard numerical analysis textbook.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

We now use equation&nbsp;\eqref{eq:uplusdt-exact} to derive a numerical scheme:
with $t_0=0$, $t_{k+1}=t_k+\Delta t=\cdots=(k+1)\Delta t$,
we get a difference equation
\[
 u_{k+1}=u_k+\Delta t\,f(t_k,u_k) 
\]
for $u_k$ quantities, and we hope that $u_k$ will be a good
approximation to&nbsp;$u(t_k)$.
This is known as the `Explicit Euler' or `Euler forward' method.
</p>

<p name="switchToTextMode">
The process of going from a differential equation to a difference
equation is often referred to as 
<i>discretization</i>
, since we
compute function values only in a discrete set of points. The values
computed themselves are still real valued. Another way of phrasing
this: the numerical solution is found in the finite dimensional
space&nbsp;$\bbR^k$ if we compute $k$ time steps. The solution to the original
problem is found in the space of functions $\bbR\rightarrow\bbR$.
</p>

<p name="switchToTextMode">
In \eqref{eq:forwarddifference} we approximated one operator by
another, and in doing so made a 
<i>truncation error</i>
 of order
$O(\Delta t)$ as $\Delta t\downarrow 0$ (see
appendix&nbsp;
app:complexity
 for a more formal introduction to this
notation for orders of magnitude.). This does 
<i>not</i>
 immediately
imply that the difference equation computes a solution that is close
to the true solution. For that some more analysis is needed.
</p>

<p name="switchToTextMode">
We start by analyzing the `local error': if we assume the computed
solution is exact at step&nbsp;$k$, that is, $u_k=u(t_k)$, how wrong will
we be at step&nbsp;$k+1$? We have
\[
\begin{array}{r@{{}={}}l}
    u(t_{k+1})&u(t_k)+u'(t_k)\Delta t+u''(t_k)\frac{\Delta t^2}{2!}+\cdots\\
    &u(t_k)+f(t_k,u(t_k))\Delta t+u''(t_k)\frac{\Delta
      t^2}{2!}+\cdots\\
\end{array}
\]
and
\[
    u_{k+1}=u_k+f(t_ku_k)\Delta t
\]
  So
\[
\begin{array}{r@{{}={}}l}
  L_{k+1}&u_{k+1}-u(t_{k+1})=u_k-u(t_k)+f(t_k,u_k)-f(t_k,u(t_k))
  -u''(t_k)\frac{\Delta t^2}{2!}+\cdots\\
  &-u''(t_k)\frac{\Delta t^2}{2!}+\cdots
\end{array}
\]
This shows that in each step we make an error of $O(\Delta t^2)$. If
we assume that these errors can be added, we find a global error of
\[
 E_k\approx\Sigma_k L_k=k\Delta t\frac{\Delta t^2}{2!}  =O(\Delta t)
\]
Since the global error is of first order in&nbsp;$\Delta t$, we call
this a `first order method'. Note that this error, which measures the
distance between the true and computed solutions, is of the same order
$O(\Delta t)$
as the truncation error, which is the error in approximating the
operator.
</p>

<h4><a id="StabilityoftheEulerexplicitmethod">4.1.2.1</a> Stability of the  Euler explicit method</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a> > <a href="odepde.html#Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">Finite difference approximation: Euler explicit and implicit methods</a> > <a href="odepde.html#StabilityoftheEulerexplicitmethod">Stability of the  Euler explicit method</a>
</p>
<p name="switchToTextMode">

Consider the 
<span title="acronym" ><i>IVP</i></span>
 $u'=f(t,u)$ for $t\geq0$,
where $f(t,u)=-\lambda u$ and an initial value $u(0)=u_0$ is given.
This has an exact solution
of $u(t)=u_0e^{-\lambda t}$. From the above discussion, we conclude
that this problem is stable, meaning that small perturbations in the
solution ultimately damp out, if&nbsp;$\lambda&gt;0$. We will now
investigate the question of whether the numerical solution behaves the
same way as the exact solution, that is,
whether numerical solutions also converge
to zero.
</p>

<p name="switchToTextMode">
The Euler forward, or explicit Euler, scheme for this problem is
\[
 u_{k+1}=u_k-\Delta t \lambda u_k=(1-\lambda \Delta t)u_k
  \Rightarrow
  u_k=(1-\lambda\Delta t)^ku_0.
\]
For stability, we require that
$u_k\rightarrow 0$ as $k\rightarrow\infty$. This is equivalent to
\begin{eqnarray*}
    u_k\downarrow 0
    &\Leftrightarrow&|1-\lambda \Delta t|&lt;1\\
    &\Leftrightarrow&-1&lt;1-\lambda\Delta t&lt;1\\
    &\Leftrightarrow&-2&lt;-\lambda\Delta t&lt;0\\
    &\Leftrightarrow&0&lt;\lambda\Delta t&lt;2\\
    &\Leftrightarrow&\Delta t&lt;2/\lambda
\end{eqnarray*}
We see that the stability of the numerical solution scheme depends on
the value of&nbsp;$\Delta t$: the scheme is only stable if $\Delta t$ is
small enough.  For this reason, we call the explicit Euler method
<i>conditionally stable</i>
. Note that the stability of the
differential equation and the stability of the numerical scheme are
two different questions. The continuous problem is stable if
$\lambda&gt;0$; the numerical problem has an additional condition that
depends on the discretization scheme used.
</p>

<p name="switchToTextMode">
Note that the stability analysis we just performed was specific to the
differential equation $u'=-\lambda u$. If you are dealing with a
different 
<span title="acronym" ><i>IVP</i></span>
 you have to perform a separate analysis. However,
you will find that explicit methods typically give conditional stability.
</p>

<!-- index -->
<p name="switchToTextMode">

<h4><a id="TheEulerimplicitmethod">4.1.2.2</a> The Euler implicit method</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a> > <a href="odepde.html#Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">Finite difference approximation: Euler explicit and implicit methods</a> > <a href="odepde.html#TheEulerimplicitmethod">The Euler implicit method</a>
</p>

<!-- index -->
<!-- index -->
</p>

<p name="switchToTextMode">
The explicit method you just saw was easy to compute, but the
conditional stability is a potential problem. For instance, it could
imply that the number of time steps would be a limiting factor.
There is an alternative to the explicit method that does not suffer
from the same objection.
</p>

<p name="switchToTextMode">
Instead of expanding $u(t+\Delta t)$, consider the following expansion
of $u(t-\Delta t)$:
\[
 u(t-\Delta t)=u(t)-u'(t)\Delta t+u''(t)\frac{\Delta t^2}{2!}+\cdots 
\]
which implies
\[
 u'(t)=\frac{u(t)-u(t-\Delta t)}{\Delta t}+u''(t)\Delta t/2+\cdots
\]
As before, we take the equation $u'(t)=f(t,u(t))$ and
approximate $u'(t)$ by a difference formula:
\[
 \frac{u(t)-u(t-\Delta t)}{\Delta t}=f(t,u(t)) +O(\Delta t)
   \Rightarrow u(t)=u(t-\Delta t)+\Delta t f(t,u(t))+O(\Delta t^2)
\]
Again we define fixed points $t_k=kt$,
and we define a numerical scheme:
\[
 u_{k+1}=u_k+\Delta tf(t_{k+1},u_{k+1}) 
\]
where $u_k$ is an approximation of&nbsp;$u(t_k)$.
</p>

<p name="switchToTextMode">
An important difference with the explicit scheme is that $u_{k+1}$ now
also appears on the right hand side of the equation. That is,
the computation of $u_{k+1}$ is now implicit.
For example, let $f(t,u)=-u^3$, then $u_{k+1}=u_k-\Delta t
u_{k+1}^3$. In other words, $u_{k+1}$&nbsp;is the solution for&nbsp;$x$ of the
equation $\Delta t x^3+x=u_k$. This is a nonlinear equation, which
typically can be solved using the Newton method.
</p>

<h4><a id="StabilityoftheimplicitEulermethod">4.1.2.3</a> Stability of the implicit Euler method</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialvalueproblems">Initial value problems</a> > <a href="odepde.html#Finitedifferenceapproximation:Eulerexplicitandimplicitmethods">Finite difference approximation: Euler explicit and implicit methods</a> > <a href="odepde.html#StabilityoftheimplicitEulermethod">Stability of the implicit Euler method</a>
</p>
<p name="switchToTextMode">

Let us take another look at the example $f(t,u)=-\lambda
u$. Formulating the implicit method gives
\[
    u_{k+1}=u_k-\lambda \Delta t u_{k+1} \Leftrightarrow
    (1+\lambda\Delta t)u_{k+1}=u_k
\]
so
\[
    u_{k+1}=\left (\frac1{1+\lambda\Delta t}\right)u_k \Rightarrow
    u_k=\left (\frac1{1+\lambda\Delta t}\right)^ku_0.
\]
If $\lambda&gt;0$, which is the condition for a stable equation, we find
that $u_k\rightarrow0$ for all values of $\lambda$ and&nbsp;$\Delta
t$. This method is called 
<i>unconditionally stable</i>
.  One
advantage of an implicit method over an explicit one is clearly
the stability: it is possible to take larger time steps without
worrying about unphysical behavior. Of course, large time steps can
make convergence to the 
<i>steady state</i>
 (see
Appendix&nbsp;
15.4
) slower, but at least there will be no
divergence.
</p>

<p name="switchToTextMode">
On the other hand, implicit methods are more complicated. As you saw
above, they can involve nonlinear systems to be solved in every time step.
In cases where $u$ is vector-valued, such as in the heat equation,
discussed below, you will see
that the implicit method requires the solution of a system of
equations.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze the accuracy and computational aspects of the following
scheme for the IVP $u'(x)=f(x)$:
\[
 u_{i+1}=u_i+h(f(x_i)+f(x_{i+1}))/2 
\]
 which corresponds to adding
the Euler explicit and implicit schemes together. You do not have
to analyze the stability of this scheme.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the initial value problem $y'(t)=y(t)(1-y(t))$.
Observe that $y\equiv 0$ and $y\equiv 1$ are solutions. These are called `equilibrium solutions'.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
A solution is stable, if perturbations `converge back to the
  solution', meaning that for $\epsilon$ small enough,
\[
 \hbox{if  $y(t)=\epsilon$ for some&nbsp;$t$, then
    $\lim_{t\rightarrow\infty}y(t)=0$} 
\]
  and
\[
 \hbox{if  $y(t)=1+\epsilon$ for some&nbsp;$t$, then
    $\lim_{t\rightarrow\infty}y(t)=1$} 
\]
  This requires for instance that 
\[
 y(t)=\epsilon\Rightarrow y'(t)&lt;0. 
\]
  Investigate this behavior. Is zero a stable solution? Is one?
<li>
Consider the explicit method
\[
 y_{k+1}=y_k+\Delta t y_k(1-y_k) 
\]
  for computing a numerical solution
  to the differential equation. Show that
\[
 y_k\in(0,1)\Rightarrow y_{k+1}&gt;y_k,\qquad
     y_k&gt;1\Rightarrow y_{k+1}&lt;y_k 
\]
<li>
Write a small program to investigate the behavior of the
  numerical solution under various choices for $\Delta t$. Include
  program listing and a couple of runs in your homework submission.
<li>
You see from running your program that the numerical solution
  can oscillate. Derive a condition on $\Delta t$ that makes the
  numerical solution monotone. It is enough to show that
  $y_k&lt;1\Rightarrow y_{k+1}&lt;1$, and $y_k&gt;1\Rightarrow y_{k+1}&gt;1$.
<li>
Now consider the implicit method
\[
 y_{k+1}-\Delta t y_{k+1}(1-y_{k+1})=y_k 
\]
  and show that $y_{k+1}$ can be computed from&nbsp;$y_k$. Write a program, and investigate the
  behavior of the numerical solution under various choices
  for&nbsp;$\Delta t$.
<li>
Show that the numerical solution of the implicit scheme is
  monotone for all choices of&nbsp;$\Delta t$.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Boundaryvalueproblems">4.2</a> Boundary value problems</h2>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a>
</p>

<!-- index -->
<!-- index -->
</p>

<p name="switchToTextMode">
In the previous section you saw initial value problems, which model
phenomena that evolve over time. We will now move on to`boundary value
problems', which are in general stationary in time, but which describe
a phenomenon that is location dependent. Examples would be the shape
of a bridge under a load, or the heat distribution in a window pane,
as the temperature outside differs from the one inside.
</p>

<p name="switchToTextMode">
The general form of a (second order, one-dimensional) 
<span title="acronym" ><i>BVP</i></span>
 is
\[
 \hbox{$u''(x)=f(x,u,u')$ for $x\in[a,b]$ where $u(a)=u_a$,
  $u(b)=u_b$} 
\]
but here we will only consider the simple form
<!-- environment: equation start embedded generator -->
</p>
 \hbox{$-u''(x)=f(x)$ for $x\in[0,1]$ with $u(0)=u\_0$, $u(1)=u\_1$.}
\label{eq:2nd-order-bvp}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
in one space dimension, or
<!-- environment: equation start embedded generator -->
</p>
 \hbox{$-u\_{xx}(\bar x)-u\_{yy}(\bar x)=f(\bar x)$ for
   $\bar x\in\Omega=[0,1]^2$
    with $u(\bar x)=u\_0$ on $\delta\Omega$.}
\label{eq:2nd-order-bvp-2D}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
in two space dimensions. Here, $\delta\Omega$ is the boundary of the
domain&nbsp;$\Omega$. Since we prescribe the value of $u$ on the boundary,
such a problem is called a 
<span title="acronym" ><i>BVP</i></span>
.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
  The boundary conditions can be more general, involving
  derivatives on the interval end points. Here we only look at
  
<i>Dirichlet boundary conditions</i>
%
<!-- index -->
  which prescribe function values on the boundary of the
  domain.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

</p>

<h3><a id="GeneralPDEtheory">4.2.1</a> General PDE theory</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#GeneralPDEtheory">General PDE theory</a>
</p>

<p name="switchToTextMode">

There are several types of 
<span title="acronym" ><i>PDE</i></span>
, each with distinct mathematical
properties. The most important property is that of 
  of influence}: if we tinker with the problem so that the solution
changes in one point, what other points will be affected.
</p>

<h4><a id="Hyperbolicequations">4.2.1.1</a> Hyperbolic equations</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#GeneralPDEtheory">General PDE theory</a> > <a href="odepde.html#Hyperbolicequations">Hyperbolic equations</a>
</p>
<p name="switchToTextMode">

<span title="acronym" ><i>PDEs</i></span>
<!-- index -->
 are of the form
\[
 Au_{xx}+Bu_{yy}+\hbox{lower order terms} =0 
\]
with $A,B$ of opposite sign. Such equations describe waves,
or more general convective phenomena, that are conservative,
and do not tend to a steady state.
</p>

<p name="switchToTextMode">
Intuitively, changing the solution of a wave equation at any point
will only change certain future points, since waves have a propagation
speed that makes it impossible for a point to influence points in the
near future that are too far away in space. This type of 
<span title="acronym" ><i>PDE</i></span>
 will
not be discussed in this book.
</p>

<h4><a id="Parabolicequations">4.2.1.2</a> Parabolic equations</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#GeneralPDEtheory">General PDE theory</a> > <a href="odepde.html#Parabolicequations">Parabolic equations</a>
</p>
<p name="switchToTextMode">

<span title="acronym" ><i>PDEs</i></span>
<!-- index -->
 are of the form
\[
 Au_x+Bu_{yy}+\hbox{no higher order terms in $x$}=0 
\]
and they describe diffusion-like phenomena; these often tend to a
<i>steady state</i>
. The best way to
characterize them is to consider that the solution in each point in
space and time is influenced by a certain finite region at each
previous point in space.
</p>

<!-- environment: remark start embedded generator -->
<!-- TranslatingLineGenerator remark ['remark'] -->
  This leads to a condition limiting
  the time step in 
    condition}&nbsp;
<a href=http://en.wikipedia.org/wiki/Courant-Friedrichs-Lewy_condition>http://en.wikipedia.org/wiki/Courant-Friedrichs-Lewy_condition</a>
.
  It describes the notion that in the exact problem $u(x,t)$ depends
  on a range of $u(x',t-\Delta t)$ values; the time step of the
  numerical method has to be small enough that the numerical solution
  takes all these points into account.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

The 
<i>heat equation</i>
(section&nbsp;
4.3
) is the standard example of the parabolic type.
</p>

<h4><a id="Ellipticequations">4.2.1.3</a> Elliptic equations</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#GeneralPDEtheory">General PDE theory</a> > <a href="odepde.html#Ellipticequations">Elliptic equations</a>
</p>
<p name="switchToTextMode">

<span title="acronym" ><i>PDEs</i></span>
<!-- index -->
 have the form
\[
 Au_{xx}+Bu_{yy}+\hbox{lower order terms} =0 
\]
where $A,B&gt;0$; they typically describe processes that have reached a
<i>steady state</i>
, for instance as $t\rightarrow\infty$ in a parabolic
problem. They are characterized by the fact that all points influence
each other. These equations often describe phenomena in structural
mechanics, such as a beam or a membrane. It is intuitively clear that
pressing down on any point of a membrane will change the elevation of
every other point, no matter how little. The 
  equation} (section&nbsp;
4.2.2
) is the standard example of
this type.
</p>

<p name="switchToTextMode">

<h3><a id="ThePoissonequationinonespacedimension">4.2.2</a> The Poisson equation in one space dimension</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#ThePoissonequationinonespacedimension">The Poisson equation in one space dimension</a>
</p>


</p>

<p name="switchToTextMode">
We call the operator $\Delta$, defined by
\[
 \Delta u = u_{xx}+u_{yy}, 
\]
a second order 
<i>differential operator</i>
, and equation
eq:2nd-order-bvp-2D
<span title="acronym" ><i>PDE</i></span>
. Specifically, the problem
<!-- environment: equation start embedded generator -->
</p>
 \hbox{$-\Delta u = -u\_{xx}(\bar x)-u\_{yy}(\bar x)=f(\bar x)$ for
   $\bar x\in\Omega=[0,1]^2$
    with $u(\bar x)=u\_0$ on $\delta\Omega$.}
\label{eq:2nd-order-bvp-poisson}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
is called the 
<i>Poisson equation</i>
, in this case defined on
the unit square.
Second order 
<span title="acronym" ><i>PDEs</i></span>
 are quite common, describing many phenomena in
fluid and heat flow and structural mechanics.
</p>

<p name="switchToTextMode">
At first, for simplicity, we consider the one-dimensional Poisson
equation
\[
 -u_{xx}=f(x).
\]
and we consider the two-dimensional case below; the extension to three
dimensions will then be clear.
</p>

<p name="switchToTextMode">
In order to find a numerical scheme we use Taylor series as before,
expressing $u(x+h)$ and $u(x-h)$ in terms of $u$ and its derivatives
at&nbsp;$x$. Let $h&gt;0$, then
\[
 u(x+h)=u(x)+u'(x)h+u''(x)\frac{h^2}{2!}+u'''(x)\frac{h^3}{3!}
+u^{(4)}(x)\frac{h^4}{4!}+u^{(5)}(x)\frac{h^5}{5!}+\cdots 
\]
and
\[
 u(x-h)=u(x)-u'(x)h+u''(x)\frac{h^2}{2!}-u'''(x)\frac{h^3}{3!}
+u^{(4)}(x)\frac{h^4}{4!}-u^{(5)}(x)\frac{h^5}{5!}+\cdots 
\]
Our aim is now to approximate&nbsp;$u''(x)$.
We see that the $u'$ terms in these equations would cancel out under
addition, leaving $2u(x)$:
\[
  u(x+h)+u(x-h)=2u(x)+u''(x)h^2+u^{(4)}(x)\frac{h^4}{12}+\cdots
\]
so
<!-- environment: equation start embedded generator -->
</p>
 -u''(x)=\frac{2u(x)-u(x+h)-u(x-h)}{h^2}+u^{(4)}(x)\frac{h^2}{12}+\cdots
\label{eq:2nd-order-num}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
The basis for a numerical scheme for&nbsp;\eqref{eq:2nd-order-bvp} is then
the observation
\[
 \frac{2u(x)-u(x+h)-u(x-h)}{h^2}=f(x,u(x),u'(x))+O(h^2), 
\]
which shows that we can approximate the differential operator by a
difference operator, with an $O(h^2)$ 
<i>truncation error</i>
as $h\downarrow0$.
</p>

<p name="switchToTextMode">
To derive a numerical method,
we divide the interval $[0,1]$ into equally spaced points: $x_k=kh$
where $h=1/(n+1)$ and $k=0&hellip; n+1$. With these, the 
<span title="acronym" ><i>FD</i></span>
formula&nbsp;\eqref{eq:2nd-order-num} leads to a numerical scheme that
forms a system of equations:
<!-- environment: equation start embedded generator -->
</p>
\label{eq:2nddiff-formula}
   -u\_{k+1}+2u\_k-u\_{k-1} = h^2 f(x\_k)
  \quad\hbox{for $k=1,&hellip;,n$}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
This process of using the 
<span title="acronym" ><i>FD</i></span>
 formula&nbsp;\eqref{eq:2nd-order-num}
for the approximate solution of a 
<span title="acronym" ><i>PDE</i></span>
 is known as the
<i>FDM</i>
.
</p>

<p name="switchToTextMode">
For most values of $k$ this equation relates $u_k$ unknown to the
unknowns $u_{k-1}$ and&nbsp;$u_{k+1}$. The exceptions are $k=1$ and
$k=n$. In that case we recall that $u_0$ and $u_{n+1}$ are known
boundary conditions,
and we write the equations with unknowns on the left and known
quantities on the right as
\[
\begin{cases}
  -u_{i-1} + 2u_i - u_{i+1} = h^2f(x_i)\\
  2u_1-u_2=h^2f(x_1)+u_0\\
  2u_n-u_{n-1}=h^2f(x_{n})+u_{n+1}.
\end{cases}
\]
We can now summarize these equations for $u_k,k=1&hellip; n-1$
as a matrix equation:
<!-- environment: equation start embedded generator -->
</p>
\begin{pmatrix}
      2&-1\\ -1&2&-1\\ &\ddots&\ddots&\ddots
\end{pmatrix}
\begin{pmatrix}
      u\_1\\ u\_2\\ \vdots
\end{pmatrix}
  =
\begin{pmatrix}
      h^2f\_1+u\_0\\ h^2f\_2\\ \vdots
\end{pmatrix}
\label{eq:1d2nd-matrix-vector}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
This has the form $Au=f$ with $A$ a fully known matrix, $f$&nbsp;a fully
known vector, and $u$ a vector of unknowns. Note that the right hand
side vector has the boundary values of the problem in the first and
last locations. This means that, if you want to solve the same
differential equation with different boundary conditions, only the
vector $f$ changes.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  A condition of the type $u(0)=u_0$ is called a 
    boundary condition}. Physically, this corresponds to, for
  instance, knowing the temperature at the end point of a rod. Other
  boundary conditions exist. Specifying a value for the derivative,
  $u'(0)=u'_0$, rather than for the function value,would be
  appropriate if we are modeling fluid flow and the outflow rate at
  $x=0$ is known. This is known as a 
    condition}.
</p>

<p name="switchToTextMode">
  A Neumann boundary condition $u'(0)=u'_0$ can be modeled by stating
\[
 \frac{u_0-u_1}h=u'_0. 
\]
  Show that, unlike in the case of the Dirichlet boundary condition,
  this affects the matrix of the linear system.
</p>

<p name="switchToTextMode">
  Show that having a
  Neumann boundary condition at both ends gives a singular
  matrix, and therefore no unique solution to the linear system.
  (Hint: guess the vector that has eigenvalue zero.)
</p>

<p name="switchToTextMode">
  Physically this makes sense. For instance, in an elasticity problem,
  Dirichlet boundary conditions state that the rod is clamped at a
  certain height; a Neumann boundary condition only states its angle
  at the end points, which leaves its height undetermined.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Let us
list some properties of&nbsp;$A$ that you will see later
are relevant for solving such systems
of equations:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The matrix is very 
<i>sparse</i>

<!-- index -->
: the
  percentage of elements that is nonzero is low. The nonzero elements
  are not randomly distributed but located in a band around the main
  diagonal. We call this a 
<i>banded matrix</i>
 in general, and a
<i>tridiagonal matrix</i>
 in this specific case.
</p>

<p name="switchToTextMode">
  The banded structure is typical for 
<span title="acronym" ><i>PDEs</i></span>
, but sparse matrices
  in different applications can be less regular.
<li>
The matrix is symmetric. This property does not hold for all
  matrices that come from discretizing 
<span title="acronym" ><i>BVP</i></span>
s, but it is true if
  there are no odd order (meaning first, third, fifth,&hellip;)
  derivatives, such as $u_x$, $u_{xxx}$,&nbsp;$u_{xy}$.
<li>
Matrix elements are constant in each diagonal, that is, in each
  set of points $\{(i,j)\colon i-j=c\}$ for some&nbsp;$c$. This is only
  true for very simple problems. It is no longer true if the
  differential equation has location dependent terms such as $\frac
  d{dx}(a(x)\frac d{dx}u(x))$. It is also no longer true if we make
  $h$ variable through the interval, for instance because we want to
  model behavior around the left end point in more detail.
<li>
Matrix elements conform to the following sign pattern: the
  diagonal elements are positive, and the off-diagonal elements are
  nonpositive. This property depends on the numerical scheme used, but
  it is often true. Together with the following property of
  definiteness, this is called an 
<i>M-matrix</i>
. There is a
  whole mathematical theory of these matrices&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#BePl:book">[BePl:book]</a>
.
<li>
The matrix is positive definite: $x^tAx&gt;0$ for all nonzero
  vectors&nbsp;$x$. This property is inherited from the original continuous
  problem, if the numerical scheme is carefully chosen. While the use
  of this may not seem clear at the moment, later you will see methods
  for solving the linear system that depend on it.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Strictly speaking the solution of equation
eq:1d2nd-matrix-vector
computing $A\inv$
is not the best way of finding&nbsp;$u$. As observed just now, the matrix
$A$ has only $3N$ nonzero elements to store. Its inverse, on the other
hand, does not have a single nonzero. Although we will not prove it,
this sort of statement holds for most sparse matrices. Therefore, we
want to solve $Au=f$ in a way that does not require $O(n^2)$ storage.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  How would you solve the tridiagonal
  system of equations?
  Show that the 
<i>LU factorization</i>
%
<!-- index -->
  of the coefficient matrix gives factors that are of
<i>bidiagonal matrix</i>
 form: they have a nonzero diagonal
  and exactly one nonzero sub or super diagonal.
</p>

<p name="switchToTextMode">
  What is the total number of operations of solving the tridiagonal
  system of equations? What is the operation count of multiplying a
  vector with
  such a matrix? This relation is not typical!
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="ThePoissonequationintwospacedimensions">4.2.3</a> The Poisson equation in two space dimensions</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#ThePoissonequationintwospacedimensions">The Poisson equation in two space dimensions</a>
</p>

</p>

<p name="switchToTextMode">
The one-dimensional 
<span title="acronym" ><i>BVP</i></span>
 above was atypical in a number of ways,
especially related to the resulting linear algebra problem. In this
section we will investigate the two-dimensional Poisson problem.
You'll see that it constitutes a non-trivial generalization of the one-dimensional problem.
The three-dimensional case is very much like the two-dimensional case,
so we will not discuss it. (For one essential difference, see the discussion
in section&nbsp;
6.8.1
.)
</p>

<p name="switchToTextMode">
The one-dimensional problem above had a function $u=u(x)$, which now becomes
$u=u(x,y)$ in two dimensions.
The two-dimensional problem we are interested is then
<!-- environment: equation start embedded generator -->
</p>
\label{eq:laplace}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
 -u_{xx}-u_{yy} = f,\quad (x,y)\in[0,1]^2,
where the values on the boundaries are given. We get our discrete
equation by applying equation&nbsp;\eqref{eq:2nd-order-num} in $x$ and&nbsp;$y$
directions:
\[
\begin{array}{l}
  -u_{xx}(x,y)=\frac{2u(x,y)-u(x+h,y)-u(x-h,y)}{h^2}+u^{(4)}(x,y)\frac{h^2}{12}+\cdots\\
  -u_{yy}(x,y)=\frac{2u(x,y)-u(x,y+h)-u(x,y-h)}{h^2}+u^{(4)}(x,y)\frac{h^2}{12}+\cdots
\end{array}
\]
or, taken together:
<!-- environment: equation start embedded generator -->
</p>
  4u(x,y)-u(x+h,y)-u(x-h,y)-u(x,y+h)-u(x,y-h)=1/h^2\,f(x,y)+O(h^2)
\label{eq:5-point-star}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">

Let again $h=1/(n+1)$ and
define $x_i=ih$ and $y_j=jh$; let $u_{ij}$ be the approximation to
$u(x_i,y_j)$, then
our discrete equation becomes
<!-- environment: equation start embedded generator -->
</p>
  4u\_{ij}-u\_{i+1,j}-u\_{i-1,j}-u\_{i,j+1}-u\_{i,j-1}=h^{-2}f\_{ij}.
\label{eq:5-point-star-ij}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
We now have $n\times n$ unknowns&nbsp;$u_{ij}$. To render this in a
<i>linear system</i>
<!-- index -->
as before we need to put them in a linear ordering, which we do
by defining $I=I_{ij}=j+i\times n$. This is called the
<i>lexicographic ordering</i>
 since it sorts the coordinates
$(i,j)$ as if they are strings.
</p>

<p name="switchToTextMode">
Using this ordering gives us $N=n^2$ equations
<!-- environment: equation start embedded generator -->
</p>
  4u\_I-u\_{I+1}-u\_{I-1}-u\_{I+n}-u\_{I-n}=h^{-2}f\_I,\qquad I=1,&hellip;,N
\label{eq:5-point-star-linear}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
and the linear system looks like
<!-- environment: equation start embedded generator -->
</p>
\label{eq:5starmatrix}
  A=
  \left(
\begin{array}{ccccc|ccccc|cc}
    4&-1&&&\emptyset&-1&&&&\emptyset&\\
    -1&4&-1&&&&-1&&&&\\
    &\ddots&\ddots&\ddots&&&&\ddots&&\\
    &&\ddots&\ddots&-1&&&&\ddots&\\
    \emptyset&&&-1&4&\emptyset&&&&-1&\\ \hline
    -1&&&&\emptyset&4&-1&&&&-1\\
    &-1      &      &&&-1      &4       &-1      &&&&-1\\
    &\uparrow&\ddots&&&\uparrow&\uparrow&\uparrow&&  &&\uparrow\\
    &k-n     &      &&&k-1     &k       &k+1     &&-1&&k+n\\
    &&&&-1&&&&-1&4&&\\ \hline
    &        &      &&&\ddots  &        &        &&  &\ddots\\
\end{array}
\right)
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
where the matrix is of size $N\times N$, with&nbsp;$N=n^2$.
As in the one-dimensional case, we see that a 
<span title="acronym" ><i>BVP</i></span>
gives rise to a 
<i>sparse matrix</i>

<!-- index -->
.
</p>

<p name="switchToTextMode">
It may seem the natural thing to consider
this system of linear equations in its matrix form.
However, it can be more insightful to render these equations in a way
that makes clear the two-dimensional connections of the unknowns. For this,
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: quote start embedded generator -->
<!-- TranslatingLineGenerator quote ['quote'] -->
<img src="graphics/laplacedomain.jpg" width=800></img>
</p name="quote">
</quote>
<!-- environment: quote end embedded generator -->
<p name="switchToTextMode">
  \caption{A difference stencil applied to a two-dimensional square
    domain}

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
figure&nbsp;
4.1
 pictures the variables in the domain,
and how
equation&nbsp;\eqref{eq:5-point-star-ij} relates them through a
<i>finite difference stencil</i>
.
From now on, when
making such a picture of the domain, we will
just use the indices of the variables, and omit the `$u$' identifier.
</p>

<p name="switchToTextMode">
The matrix of equation&nbsp;
eq:5starmatrix
 is banded as in the
one-dimensional case, but
unlike in the one-dimensional case, there are zeros inside the
band. (This has some important consequences when we try to solve the
linear system; see section&nbsp;
5.4.3
.) Because the matrix has five
nonzero diagonals, it is said to be of 
<i>penta-diagonal</i>
structure.
</p>

<p name="switchToTextMode">
You can also put a block structure on the matrix, by grouping the
unknowns together that are in one row of the domain. This is called a
<i>block matrix</i>
, and, on the block level, it
has a
<i>tridiagonal matrix</i>
 structure,
so we call this a
<i>block tridiagonal</i>
 matrix. Note that the diagonal blocks
themselves are tridiagonal; the off-diagonal blocks are
minus the identity matrix.
</p>

<p name="switchToTextMode">

This matrix, like the one-dimensional example above, has constant
diagonals, but this is again due to the simple nature of the
problem. In practical problems it will not be true. That said,
such `constant coefficient' problems occur, and when they are on
rectangular domains, there are very efficient methods for solving
the linear system with $N\log N$ time complexity.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/laplacetriangle.jpg" width=800></img>
<p name="caption">
FIGURE 4.2: A triangular domain of definition for the Poisson equation
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The block structure of the matrix, with all diagonal blocks having
  the same size, is due to the fact that we defined our 
<span title="acronym" ><i>BVP</i></span>
 on a
  square domain. Sketch the matrix structure that arises from
  discretizing equation&nbsp;\eqref{eq:laplace}, again with central
  differences, but this time defined on a triangular domain; see
  figure&nbsp;
4.2
. Show that, again, there is a block
  tridiagonal matrix structure, but that the blocks are now of varying
  sizes. Hint: start by sketching a small example. For $n=4$ you
  should get a $10\times 10$ matrix with a $4\times 4$ block structure.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">
For domains that are even more irregular, the matrix structure will
also be irregular.
</p>

<p name="switchToTextMode">
The regular block structure is also caused by our decision to order
the unknowns by rows and columns. This known as the 
  ordering} or 
<i>lexicographic ordering</i>
; various other orderings are
possible. One common way of ordering the unknowns is the
<i>red-black ordering</i>
 or 
  ordering} which has advantages for parallel computation. This will
be discussed in section&nbsp;
6.7
.
</p>

<p name="switchToTextMode">
There is more to say about analytical aspects of the 
<span title="acronym" ><i>BVP</i></span>
 (for
instance, how smooth is the solution and how does that depend on the
boundary conditions?) but those questions are outside the scope of
this course. Here we only focus on the numerical aspects of the
matrices. In the  chapter on linear algebra,
specifically sections 
5.4
 and&nbsp;
5.5
,
we will discuss solving the linear system from&nbsp;
<span title="acronym" ><i>BVPs</i></span>
.
</p>

<h3><a id="Differencestencils">4.2.4</a> Difference stencils</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#Differencestencils">Difference stencils</a>
</p>

<p name="switchToTextMode">

The discretization \eqref{eq:5-point-star} is often phrased as
applying the
<i>difference stencil</i>
<!-- index -->
<!-- index -->
\[
\begin{matrix}
  \cdot&-1&\cdot\\ -1&4&-1\\ \cdot&-1&\cdot
\end{matrix}
\]
to the function&nbsp;$u$. Given a physical domain, we apply the stencil to
each point in that domain to derive the equation for that
point. Figure&nbsp;
4.1
 illustrates that for a square domain
of $n\times n$ points.
Connecting this figure with equation&nbsp;\eqref{eq:5starmatrix}, you
see that the connections in the same line give rise to the main
diagonal and first upper and lower offdiagonal; the connections to the
next and previous lines become the nonzeros in the off-diagonal
blocks.
</p>

<p name="switchToTextMode">
This particular stencil is often referred to as
the `5-point star' or 
<i>five-point stencil</i>
.
There are other difference stencils; the structure
of some of them are depicted in figure&nbsp;
4.3
.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
  \centering
<img src="graphics/stencils.jpg" width=800></img>
<p name="caption">
FIGURE 4.3: The structure of some difference stencils in two dimensions
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
A stencil with only connections in horizontal or vertical direction is
called a `star stencil', while one that has cross connections (such as
the second in figure&nbsp;
4.3
) is called a `box
stencil'.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the third stencil in figure&nbsp;
4.3
, used for a
<span title="acronym" ><i>BVP</i></span>
 on a square domain.
  What does the sparsity structure of the resulting matrix look like,
  if we again order the variables by rows and columns?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Other stencils than the 5-point star can be used to attain higher
accuracy, for instance giving a truncation error of&nbsp;$O(h^4)$. They can
also be used for other differential equations than the one discussed
above. For instance, it is not hard to show that for the equation
$u_{xxxx}+u_{yyyy}=f$ we need a stencil that contains both $x,y\pm h$
and $x,y\pm 2h$ connections, such as the third stencil in the
figure. Conversely, using the 5-point stencil no values of the
coefficients give a discretization of the fourth order problem with
less than&nbsp;$O(1)$ truncation error.
</p>

<p name="switchToTextMode">
While the discussion so far has been about two-dimensional
problems, it can be generalized to higher dimensions for such
equations as $-u_{xx}-u_{yy}-u_{zz}=f$. The straightforward
generalization of the 5-point stencil, for instance, becomes a
<i>7-point stencil</i>
in three dimensions.
</p>

<h3><a id="Otherdiscretizationtechniques">4.2.5</a> Other discretization techniques</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Boundaryvalueproblems">Boundary value problems</a> > <a href="odepde.html#Otherdiscretizationtechniques">Other discretization techniques</a>
</p>

<p name="switchToTextMode">

In the above, we used the 
<span title="acronym" ><i>FDM</i></span>
 to find a numerical solution
to a differential equation. There are various other techniques, and in
fact, in the case of boundary value problems, they are usually
preferred over finite differences. The most popular methods are the
<i>FEM</i>
 and the 
  method}. Especially the finite element method is attractive, since
it can handle irregular shapes more readily than finite differences,
and it is more amenable to
approximation error analysis. However, on the simple problems discussed here
it gives similar or even the same linear systems as 
<span title="acronym" ><i>FD</i></span>
 methods,
so we limit the discussion to Finite Differences, since we are mostly
concerned with the computational aspects of the linear systems.
</p>

<p name="switchToTextMode">
There will be a brief discussion of finite element matrices in
section&nbsp;
6.6.2
.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Initialboundaryvalueproblem">4.3</a> Initial boundary value problem</h2>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
We will now go on to discuss an 
<span title="acronym" ><i>IBVP</i></span>
, which, as you may deduce
from the name, combines aspects of 
<span title="acronym" ><i>IVP</i></span>
s and 
<span title="acronym" ><i>BVP</i></span>
s. Here we
will limit ourselves to one space dimension.
</p>

<p name="switchToTextMode">
The problem we are considering is that of  heat conduction in a rod, where
$T(x,t)$ describes the temperature in location&nbsp;$x$ at time&nbsp;$t$, for
$x\in[a,b]$, $t&gt;0$. The so-called 
<i>heat equation</i>
 (see
Appendix&nbsp;
app:pde
 for a quick introduction to 
<span title="acronym" ><i>PDEs</i></span>
 in
general and the heat equation in particular) is:
\[
  \frac\partial{\partial t}T(x,t)-\alpha\frac{\partial^2}{\partial x^2}T(x,t)
  =q(x,t)
\]
where
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The initial condition $T(x,0)=T_0(x)$ describes the initial
  temperature distribution.
<li>
The boundary conditions $T(a,t)=T_a(t)$, $T(b,t)=T_b(t)$
  describe the ends of the rod, which can for instance be fixed to an
  object of a known temperature.
<li>
The material the rod is made of is modeled by a single parameter
  $\alpha&gt;0$, the thermal diffusivity, which describes how fast heat
  diffuses through the material.
<li>
The forcing function $q(x,t)$ describes externally applied
  heating, as a function of both time and place.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
There is a simple connection between the 
<span title="acronym" ><i>IBVP</i></span>
 and the 
<span title="acronym" ><i>BVP</i></span>
:
if the boundary functions $T_a$ and $T_b$ are constant, and $q$&nbsp;does
not depend on time, only on location, then intuitively $T$&nbsp;will
converge to a 
<i>steady state</i>
.
The equation for this is $-\alpha u''(x)=q$.
</p>

<h3><a id="Discretization">4.3.1</a> Discretization</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a> > <a href="odepde.html#Discretization">Discretization</a>
</p>
<p name="switchToTextMode">

We now discretize both space and time, by $x_{j+1}=x_j+\Delta x$ and
$t_{k+1}=t_k+\Delta t$, with boundary conditions $x_0=a$, $x_n=b$,
and $t_0=0$. We write $T^k_j$ for the numerical solution at $x=x_j,t=t_k$;
with a little luck, this will approximate the exact
solution $T(x_j,t_k)$.
</p>

<p name="switchToTextMode">
For the space discretization we use the central difference formula
eq:2nddiff-formula
\[
  \left.\frac{\partial^2}{\partial x^2}T(x,t_k)\right|_{x=x_j}
  \Rightarrow
  \frac{T_{j-1}^k-2T_j^k+T_{j+1}^k}{\Delta x^2}.
\]
For the time discretization we can use any of the schemes in
section&nbsp;
4.1.2
. We will investigate again the explicit and
implicit schemes, with similar conclusions about the
resulting stability.
</p>

<h4><a id="Explicitscheme">4.3.1.1</a> Explicit scheme</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a> > <a href="odepde.html#Discretization">Discretization</a> > <a href="odepde.html#Explicitscheme">Explicit scheme</a>
</p>

<p name="switchToTextMode">

\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
</p>

<!-- environment: wrapfigure start embedded generator -->
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<p name="switchToTextMode">
  \leavevmode\kern\unitindent
<img src="graphics/euler-forward.jpg" width=800></img>
  \caption{The difference stencil of the Euler forward method for the
    heat equation}

.
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2.5in}
With explicit time stepping we approximate the time derivative as
<!-- environment: equation start embedded generator -->
</p>
  \left.\frac\partial{\partial t}T(x\_j,t)\right|\_{t=t\_k}
  \Rightarrow
  \frac{T\_j^{k+1}-T\_j^k}{\Delta t}.
\label{eq:disc-time-explicit}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
Taking this together with the central differences in space, we now have
\[
  \frac{T_j^{k+1}-T_j^k}{\Delta t}-\alpha
  \frac{T_{j-1}^k-2T_j^k+T_{j+1}^k}{\Delta x^2}=q_j^k
\]
which we rewrite as
<!-- environment: equation start embedded generator -->
</p>
\label{eq:bivp-explicit}
  T\_j^{k+1}=T\_j^k+\dtdxx
  (T\_{j-1}^k-2T\_j^k+T\_{j+1}^k)+\Delta t q\_j^k
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
Pictorially, we render this as a difference stencil in
figure&nbsp;
4.4
.
This expresses that the function value in each point is determined by
a combination of points on the previous time level.
</p>

<p name="switchToTextMode">
It is convenient to summarize the set of equations
eq:bivp-explicit
in vector form as
<!-- environment: equation start embedded generator -->
</p>
\label{eq:bivp-explicit-vector}
   T^{k+1}=\left(I-\dtdxx K\right)
   T^k+\Delta t q^k
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
where
\[
  K=
\begin{pmatrix}
    2&-1\\ -1&2&-1\\ &\ddots&\ddots&\ddots
\end{pmatrix}
,\qquad
  T^k=
\begin{pmatrix}
    T^k_1\\ \vdots \\ T^k_n
\end{pmatrix}
.
\]
The important observation here is that the dominant computation for
deriving the vector $T^{k+1}$
from $ T^k$ is a simple matrix-vector multiplication:
\[
 T^{k+1}\leftarrow AT^k+\Delta tq^k 
\]
where $A=I-\dtdxx K$. This is a first indication that the sparse
matrix-vector product is an important operation; see sections
5.4
 and&nbsp;
6.5
.
Actual
computer programs using an explicit method often do not form the
matrix, but evaluate the equation&nbsp;\eqref{eq:bivp-explicit}. However,
the linear algebra formulation&nbsp;\eqref{eq:bivp-explicit-vector}
is more insightful for purposes of analysis.
</p>

<p name="switchToTextMode">
In later chapters we will consider parallel execution of operations.
For now, we note that the explicit scheme is trivially parallel:
each point can be updated with just the information of a few surrounding
points.
</p>

<h4><a id="Implicitscheme">4.3.1.2</a> Implicit scheme</h4>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a> > <a href="odepde.html#Discretization">Discretization</a> > <a href="odepde.html#Implicitscheme">Implicit scheme</a>
</p>
<p name="switchToTextMode">

In equation \eqref{eq:disc-time-explicit} we let $T^{k+1}$ be defined
from&nbsp;$T^k$. We can turn this around by defining $T^k$ from&nbsp;$T^{k-1}$,
as we did for the 
<span title="acronym" ><i>IVP</i></span>
 in section&nbsp;
4.1.2.2
. For
the time discretization this gives
<!-- environment: equation start embedded generator -->
</p>
  \left.\frac\partial{\partial t}T(x,t)\right|\_{t=t\_k}
  \Rightarrow
  \frac{T\_j^k-T\_j^{k-1}}{\Delta t}
  \qquad\hbox{or}\qquad
  \left.\frac\partial{\partial t}T(x,t)\right|\_{t=t\_{k+1}}
  \Rightarrow
  \frac{T\_j^{k+1}-T\_j^k}{\Delta t}.
\label{eq:heat-time-explicit}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">

<!-- environment: wrapfigure start embedded generator -->
</p>
<!-- TranslatingLineGenerator wrapfigure ['wrapfigure'] -->
<p name="switchToTextMode">
  \leavevmode\kern\unitindent
<img src="graphics/euler-backward.jpg" width=800></img>
  \caption{The difference stencil of the Euler backward method for the
    heat equation}

.
</wrapfigure>
<!-- environment: wrapfigure end embedded generator -->
<p name="switchToTextMode">
{r}{2.5in}
The implicit time step discretization of the whole heat equation,
evaluated in&nbsp;$t_{k+1}$, now becomes:
\[
  \frac{T_j^{k+1}-T_j^k}{\Delta t}-\alpha
  \frac{T_{j-1}^{k+1}-2T_j^{k+1}+T_{j+1}^{k+1}}{\Delta x^2}=q_j^{k+1}
\]
or
<!-- environment: equation start embedded generator -->
</p>
\label{eq:bivp-implicit}
  T\_j^{k+1}-\dtdxx
  (T\_{j-1}^{k+1}-2T\_j^{k+1}+T\_{j+1}^{k+1})=T\_j^k+\Delta t q\_j^{k+1}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
4.5
 renders this as a stencil;
this expresses that each point on the current time level influences a
combination of points on the next level.
Again we write this in vector form:
<!-- environment: equation start embedded generator -->
</p>
\label{eq:bivp-implicit-vector}
  \left(I+\dtdxx K\right) T^{k+1}=
   T^k+\Delta t q^{k+1}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
As opposed to the explicit method, where a matrix-vector
multiplication sufficed, the derivation of the vector $ T^{k+1}$
from $ T^k$ now involves a
<i>linear system solution</i>
<!-- index -->
:
\[
 T^{k+1}\leftarrow A\inv (T^k+\Delta tq^{k+1}) 
\]
where $A=I+\dtdxx K$, a harder operation than the matrix-vector
multiplication.  In this case, it is not possible, as above, to
evaluate the equation&nbsp;\eqref{eq:bivp-implicit} directly. Codes using an
implicit method actually form the coefficient matrix, and solve the
system&nbsp;\eqref{eq:bivp-implicit-vector} as such. Solving linear systems
will be the focus of much of chapters 
Numerical linear algebra
and&nbsp;
High performance linear algebra
.
</p>

<p name="switchToTextMode">
In contrast to the explicit scheme, we now have no
obvious parallelization strategy. The parallel solution of linear
systems will occupy us in sections 
6.6
and on.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that the flop count for a time step of the implicit method is
  of the same order as of a time step of the explicit method. (This
  only holds for a problem with one space dimension.) Give at least
  one argument why we consider the implicit method as computationally
  `harder'.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The numerical scheme that we used here is of first order in time and
second order in space: the truncation error (section&nbsp;
4.1.2
)
is $O(\Delta t+\Delta x^2)$. It would be possible to use a scheme that
is second order in time by using central differences in time
too. Alternatively, see exercise&nbsp;
4.3.2
.
</p>

<h3><a id="Stabilityanalysis">4.3.2</a> Stability analysis</h3>
<p name=crumbs>
crumb trail:  > <a href="odepde.html">odepde</a> > <a href="odepde.html#Initialboundaryvalueproblem">Initial boundary value problem</a> > <a href="odepde.html#Stabilityanalysis">Stability analysis</a>
</p>
<p name="switchToTextMode">

We now analyze the stability of the explicit and implicit
schemes in a simple case. Let $q\equiv0$, and assume
$T_j^k=\beta^ke^{i\ell x_j}$ for some&nbsp;$\ell$
<!-- environment: footnoteenv start embedded generator -->
</p>
<!-- TranslatingLineGenerator footnoteenv ['footnoteenv'] -->
  {Actually,
  $\beta$ is also dependent on&nbsp;$\ell$, but we will save ourselves a
  bunch of subscripts, since different $\beta$ values never appear
  together in one formula.}
</p name="footnoteenv">
)
</footnoteenv>
<!-- environment: footnoteenv end embedded generator -->
<p name="switchToTextMode">
. This assumption is intuitively
defensible: since the differential equation does not `mix' the $x$ and
$t$ coordinates, we surmise that the solution will be a product
$T(x,t)=v(x)\cdot w(t)$ of the
separate solutions of
\[
\begin{cases}
  v_{xx}=c_1 v,&v(0)=0,\,v(1)=0\\
  w_t=c_2 w & w(0)=1\\
\end{cases}
\]
The only meaningful solution occurs with $c_1,c_2&lt;0$, in which case we
find:
\[
 v_{xx}=-c^2v \Rightarrow v(x)=e^{-icx}=e^{-i\ell\pi x} 
\]
where we substitute $c=\ell\pi$ to take boundary conditions into
account,
and
\[
 w(t) = e^{-ct} = e^{-ck\Delta t} = \beta^k,\quad \beta=e^{-ck}. 
\]
If the assumption on this form of the solution holds up, we need
$|\beta|&lt;1$ for stability.
</p>

<p name="switchToTextMode">
Substituting the surmised form for $T_j^k$ into the  explicit scheme gives
\begin{eqnarray*}
    T_j^{k+1}&=&T_j^k+\dtdxx(T_{j-1}^k-2T_j^k+T_{j+1}^k)\\
    \Rightarrow \beta^{k+1}e^{i\ell x_j}&=&\beta^ke^{i\ell x_j}
    +\dtdxx (\beta^ke^{i\ell x_{j-1}}-2\beta^ke^{i\ell x_j}+\beta^ke^{i\ell x_{j+1}})\\
    &=&\beta^ke^{i\ell x_j}\left[1+\dtdxx\left[e^{-i\ell\Delta x}-2+e^{i\ell\Delta x}\right]\right]\\
    \Rightarrow \beta&=&
    1+2\dtdxx[\frac12(e^{i\ell\Delta x}+e^{-\ell\Delta x})-1]\\
    &=&1+2\dtdxx(\cos(\ell\Delta x)-1)
\end{eqnarray*}
</p>

<p name="switchToTextMode">
For stability we need $|\beta|&lt;1$:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
$\beta&lt;1\Leftrightarrow 2\dtdxx(\cos(\ell\Delta x)-1)&lt;0$: this is
    true for any $\ell$ and any choice of $\Delta x,\Delta t$.
<li>
$\beta&gt;-1\Leftrightarrow 2\dtdxx(\cos(\ell\Delta x)-1)&gt;-2$: this
    is true for all&nbsp;$\ell$ only if $2\dtdxx&lt;1$, that is
\[
 \Delta t&lt;\frac{\Delta x^2}{2\alpha} 
\]
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
The latter condition poses a big restriction on the allowable size of
the time steps: time steps have to be small enough for the method to
be stable. This is similar to the stability analysis of the explicit
method for the 
<span title="acronym" ><i>IVP</i></span>
; however, now the time step is also related to
the space discretization. This implies that, if we decide we need more
accuracy in space and we halve the space discretization&nbsp;$\Delta x$,
the number of time steps will be multiplied by four.
</p>

<p name="switchToTextMode">
Let us now consider the stability of the implicit scheme.
Substituting the form of the solution $T_j^k=\beta^ke^{i\ell x_j}$
into the numerical scheme gives
\begin{eqnarray*}
    T_j^{k+1}-T_j^k&=&\dtdxx(T_{j_1}^{k+1}-2T_j^{k+1}+T_{j+1}^{k+1})\\
    \Rightarrow \beta^{k+1}e^{i\ell \Delta x}-\beta^ke^{i\ell x_j}&=&
    \dtdxx(\beta^{k+1}e^{i\ell x_{j-1}}-2\beta^{k+1}e^{i\ell x_j}
    +\beta^{k+1}e^{i\ell x_{j+1}})
\end{eqnarray*}
Dividing out $e^{i\ell x_j}\beta^{k+1}$ gives
\begin{eqnarray*}
    &&1=\beta\inv+2\alpha\frac{\Delta t}{\Delta x^2}(\cos\ell\Delta
    x-1)\\
    &&\beta=\frac1{1+2\alpha\frac{\Delta t}{\Delta x^2}(1-\cos\ell\Delta x)}
\end{eqnarray*}
Since $1-\cos\ell\Delta x\in(0,2)$, the denominator is strictly&nbsp;$&gt;1$.
Therefore the condition $|\beta|&lt;1$ is always satisfied, regardless
the value of $\ell$ and the choices
of $\Delta x$ and&nbsp;$\Delta t$: the method is always stable.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The schemes we considered here are of first order in time and second
  order in space: their discretization order are $O(\Delta t)+O(\Delta
  x^2)$. Derive the 
<i>Crank-Nicolson method</i>
 that is obtained
  by averaging the explicit and implicit schemes, show that it is
  unconditionally stable, and of second order in time.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
</p>

</div>
<a href="index.html">Back to Table of Contents</a>
