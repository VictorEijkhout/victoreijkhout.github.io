<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Sorting</h1>
        <h5>Experimental html version of downloadable textbook, see https://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>

\[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mathjax.tex : macros to facility mathjax use in html version
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\newcommand\macro[1]{$\langle$#1$\rangle$}
\newcommand\dtdxx{\frac{\alpha\Delta t}{\Delta x^2}}
\]


8.1 : <a href="sorting.html#Briefintroductiontosorting">Brief introduction to sorting</a><br>
8.1.1 : <a href="sorting.html#Complexity">Complexity</a><br>
8.1.2 : <a href="sorting.html#Sortingnetworks">Sorting networks</a><br>
8.1.3 : <a href="sorting.html#Parallelcomplexity">Parallel complexity</a><br>
8.2 : <a href="sorting.html#Odd-eventranspositionsort">Odd-even transposition sort</a><br>
8.3 : <a href="sorting.html#Quicksort">Quicksort</a><br>
8.3.1 : <a href="sorting.html#Quicksortinsharedmemory">Quicksort in shared memory</a><br>
8.3.2 : <a href="sorting.html#Quicksortonahypercube">Quicksort on a hypercube</a><br>
8.3.3 : <a href="sorting.html#Quicksortonageneralparallelprocessor">Quicksort on a general parallel processor</a><br>
8.4 : <a href="sorting.html#Radixsort">Radixsort</a><br>
8.4.1 : <a href="sorting.html#Parallelradixsort">Parallel radix sort</a><br>
8.4.2 : <a href="sorting.html#Radixsortbymostsignificantdigit">Radix sort by most significant digit</a><br>
8.5 : <a href="sorting.html#Samplesort">Samplesort</a><br>
8.5.1 : <a href="sorting.html#SortingthroughMapReduce">Sorting through MapReduce</a><br>
8.6 : <a href="sorting.html#Bitonicsort">Bitonic sort</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>8 Sorting</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- index -->
<!-- index -->
<!-- index -->
<!-- index -->
<!-- index -->
</p>


<!-- index -->
<p name="switchToTextMode">

Sorting is not a common operation in scientific computing: one expects
it to be more important in databases, whether these be financial or
biological (for instance in sequence alignment). However, it sometimes
comes up, for instance in 
<i>AMR</i>
 and other applications where
significant manipulations of data structures occurs.
</p>

<p name="switchToTextMode">
In this section we will briefly look at some basic
algorithms and how they can be done in parallel. For more details,
see~
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Kumar:parcomp-book">[Kumar:parcomp-book]</a>
 and the references therein.
</p>

<h2><a id="Briefintroductiontosorting">8.1</a> Brief introduction to sorting</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Briefintroductiontosorting">Brief introduction to sorting</a>
</p>
<p name="switchToTextMode">

<h3><a id="Complexity">8.1.1</a> Complexity</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Briefintroductiontosorting">Brief introduction to sorting</a> > <a href="sorting.html#Complexity">Complexity</a>
</p>
</p>

<p name="switchToTextMode">
There are many sorting algorithms. Traditionally, they have
been distinguished by
their computational complexity, that is, given an array of $n$
elements, how many operations does it take to sort them, as a function
of~$n$.
</p>

<p name="switchToTextMode">
Theoretically one can show that a sorting algorithm has to have at
least complexity~$O(n\log n)$\footnote{One can consider a sorting
  algorithm as a decision tree: a first comparison is made, depending
  on it two other comparisons are made, et cetera. Thus, an actual
  sorting becomes a path through this decision tree. If every path has
  running time~$h$, the tree has $2^h$ nodes. Since a sequence of $n$
  elements can be ordered in $n!$ ways, the tree needs to have enough
  paths to accomodate all of these; in other words, $2^h\geq
  n!$. Using Stirling's formula, this means that $n\geq O(n\log
  n)$}. There are indeed several algorithms that are guaranteed to
attain this complexity, but a very popular algorithm, called
<i>Quicksort</i>
<!-- index -->
 has only an
`expected' complexity of~$O(n\log n)$, and a worst case complexity
of~$O(n^2)$. This behaviour results from the fact that quicksort needs
to choose `pivot elements' (we will go into more detail below in
section~
8.3
), and if these choices are consistently
the worst possible, the optimal complexity is not reached.
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>\While{the input array has length~$>1$}{    Find a pivot element of intermediate size\;    Split the array in two, based on the pivot\</p>
Sort the two arrays.  }  \caption{The quicksort algorithm}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

On the other hand, the very simple 
<i>bubble   sort</i>

<!-- index -->
 algorithm
always has the same complexity, since it has a static structure:
</p>

<!-- environment: displayalgorithm start embedded generator -->
\For{$\mathit{pass}$ from $1$ to $n-1$}{    \For{$e$ from 1 to $n-\mathit{pass}$}{      \If{elements $e$ and $e+1$ are ordered the wrong way}{exchange      them}    }  }  \caption{The bubble sort algorithm}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

It is easy to see that this algorithm has a complexity of~$O(n^2)$:
the inner loop does $t$ comparisons and up to $t$ exchanges. Summing
this from $1$ to $n-1$ gives approximately $n^2/2$ comparisons and
at most the same number of exchanges.
</p>

<h3><a id="Sortingnetworks">8.1.2</a> Sorting networks</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Briefintroductiontosorting">Brief introduction to sorting</a> > <a href="sorting.html#Sortingnetworks">Sorting networks</a>
</p>
<p name="switchToTextMode">

Above we saw that some sorting algorithms operate independently of the
actual input data, and some make decisions based on that data.
The former class is sometimes
called 
<i>sorting network</i>
. It can be considered
as custom hardware that implements just one algorithm. The
basic hardware element is the 
<i>compare-and-swap</i>
element, which has two inputs and two outputs. For two inputs $x,y$
the outputs are $\max(x,y)$ and $\min(x,y)$.
</p>

<p name="switchToTextMode">
In figure~
8.1
 we show buble sort, built up
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bubble-pass.jpeg" width=800></img>
<p name="caption">
FIGURE 8.1: Bubble sort as a sorting network
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
out of compare and swap elements.
</p>

<p name="switchToTextMode">
Below we will consider the Bitonic sort algorithm
as a prime example of a sorting network.
</p>

<h3><a id="Parallelcomplexity">8.1.3</a> Parallel complexity</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Briefintroductiontosorting">Brief introduction to sorting</a> > <a href="sorting.html#Parallelcomplexity">Parallel complexity</a>
</p>
<p name="switchToTextMode">

Above we remarked that sorting sequentially takes at least $O(N\log N)$ time.
If we can have perfect speedup, using for simplicity $P=N$ processors,
we would have parallel time~$O(\log N)$. If the parallel time
is more than that, we define the
</p>

<!-- index -->
<p name="switchToTextMode">
as the total number of operations over all processors.
</p>

<p name="switchToTextMode">
This equals the number of operations if the parallel algorithm were
executed by one process, emulating all the others. Ideally this would
be the same as the number of operations for a single-process
algorithm, but it need not be. If it is larger, we have found another
source of overhead: an intrinsic penalty for using a parallel
algorithm.
</p>

<p name="switchToTextMode">
For instance, below we will see that sorting algorithm often have a
sequential complexity of $O(N\log^2N)$.
</p>

<p name="switchToTextMode">

<h2><a id="Odd-eventranspositionsort">8.2</a> Odd-even transposition sort</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Odd-eventranspositionsort">Odd-even transposition sort</a>
</p>
<!-- index -->
</p>

<p name="switchToTextMode">
Taking another look at figure~
8.1
, you see that the
second pass
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bubble-collapse.jpeg" width=800></img>
<p name="caption">
FIGURE 8.2: Overlapping passes in the bubble sort network
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
can actually be started long before the first pass is totally
finished. This is illustrated in figure~
8.2
.
If we now look at what happens at any given time, we obtain the
<i>odd-even transposition sort</i>
</p>

<p name="switchToTextMode">
Odd-even transposition sort is a simple parallel sorting
algorithm, with as main virtue that it is relatively easy to implement
on a linear area of processors. On the other hand, it is not
particularly efficient.
</p>

<p name="switchToTextMode">
A~single step of the
algorithm consists of two substeps:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Every even-numbered processor does a compare-and-swap
  with its right neighbour; then
<li>
Every odd-numbered processor does a compare-and-swap
  with its right neighbour.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: theorem start embedded generator -->
</p>
<!-- TranslatingLineGenerator theorem ['theorem'] -->
  After $N/$ steps, each consisting of the two substeps just given,
  a sequency is sorted.
</p name="theorem">
</theorem>
<!-- environment: theorem end embedded generator -->
<!-- environment: proof start embedded generator -->
<!-- TranslatingLineGenerator proof ['proof'] -->
  In each triplet $2i,2i+1,2i+2$, after an even and an odd step the
  largest element will be in rightmost position. Proceed by induction.
</p name="proof">
</proof>
<!-- environment: proof end embedded generator -->
<p name="switchToTextMode">

With a parallel time of $N$, this gives a sequential complexity~$N^2$
compare-and-swap operations.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Discuss speedup and efficiency of swap sort, where we sort $N$
  numbers of $P$ processors; for simplicity we set $N=P$ so that
  each processor contains a single number. Express execution time
  in 
<i>compare-and-swap</i>
 operations.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
How many compare-and-swap operations does the parallel code
    take in total?
<li>
How many sequential steps does the algorithm take? What are
    $T_1$, $T_p$, $T_\infty$, $S_p$, $E_p$ for sorting $N$ numbers?
    What is the average amount of parallelism?
<li>
Swap sort can be considered a parallel implementation of
    bubble sort. Now let $T_1$ refer to the execution time of
    (sequential) bubble sort. How does this change $S_p$ and~$E_p$?
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h2><a id="Quicksort">8.3</a> Quicksort</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Quicksort">Quicksort</a>
</p>

<!-- index -->
<p name="switchToTextMode">

Quicksort is a recursive algorithm, that, unlike bubble sort, is not
deterministic. It is a two step procedure, based on a reordering of
the sequence\footnote{The name is explained by its origin with the
  Dutch computer scientist Edsger Dijkstra; see
  
<a href=http://en.wikipedia.org/wiki/Dutch_national_flag_problem>http://en.wikipedia.org/wiki/Dutch_national_flag_problem</a>
.}:
</p>

<!-- environment: displayalgorithm start embedded generator -->
\TitleOfAlgo{Dutch National Flag ordering of an array}  \Input{An array of elements, and a `pivot' value}  \Output{The input array with elements ordered as red-white-blue,    where red elements are larger than the pivot, white elements are    equal to the pivot, and blue elements are less than the pivot}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

We state without proof that this can be done in $O(n)$ operations.
With this, quicksort becomes:
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>\TitleOfAlgo{Quicksort}  \Input{An array of elements}  \Output{The input array, sorted}  \While{The array is longer than one element}{    pick an arbitrary value as pivot \;    apply the Dutch National Flag reordering to this array \;    Quicksort( the blue elements ) \; Quicksort( the red elements ) \</p>
}
<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

The indeterminacy of this algorithm, and the variance in its
complexity, stems from the pivot choice. In the worst case, the pivot
is always the (unique) smallest element of the array. There will then
be no blue elements, the only white element is the pivot, and the
recursive call will be on the array of $n-1$ red elements. It is easy
to see that the running time will then be~$O(n^2)$. On the other hand,
if the pivot is always (close to) the median, that is, the element
that is intermediate in size, then the recursive calls will have an
about equal running time, and we get a recursive formula for the
running time:
\[
 T_n = 2T_{n/2} + O(n) 
\]
which  is (again without proof) $O(n\log n)$.
</p>

<p name="switchToTextMode">
We will now consider parallel implementations of quicksort.
</p>

<h3><a id="Quicksortinsharedmemory">8.3.1</a> Quicksort in shared memory</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Quicksort">Quicksort</a> > <a href="sorting.html#Quicksortinsharedmemory">Quicksort in shared memory</a>
</p>
<p name="switchToTextMode">

A simple parallelization of the quicksort algorithm can be achieved by
executing the two recursive calls in parallel. This is easiest
realized with a shared memory model, and threads
(section~
2.6.1
) for the recursive calls. However, this
implementation is not efficient.
</p>

<p name="switchToTextMode">
On an array of length~$n$, and with perfect pivot choice, there will
be $n$~threads active in the final stage of the algorithm. Optimally,
we would want a parallel algorithm to run in $O(\log n)$ time, but
here the time is dominated by the initial reordering of the array by
the first thread.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Make this argument precise. What is the total running time, the
  speedup, and the efficiency of parallelizing the quicksort algorithm
  this way?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Is there a way to make splitting the array more efficient?
As it turns out, yes, and the key is to use a parallel 
operation}; see appendix~
app:prefix
. If the array of values
is $x_1,\ldots,x_n$, we use a parallel prefix to compute
how many elements are less than the pivot~$\pi$:
\[
 X_i=\#\{ x_j\colon j<i\wedge x_j<\pi \}. 
\]
With this, if a processor looks at~$x_i$, and $x_i$~is less
than the pivot, it needs to be moved to location~$X_i+1$
in the array where elements are split according to the pivot.
Similarly, one would could how many elements there are over the pivot,
and move those accordingly.
</p>

<p name="switchToTextMode">
This shows that each pivoting step can be done in $O(\log n)$ time,
and since there $\log n$ steps to the sorting algorithm, the
total algorithm runs in $O((\log n)^2)$ time.
The
</p>

<!-- index -->
<p name="switchToTextMode">
$(\log_2N)^2$.
</p>

<p name="switchToTextMode">

<h3><a id="Quicksortonahypercube">8.3.2</a> Quicksort on a hypercube</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Quicksort">Quicksort</a> > <a href="sorting.html#Quicksortonahypercube">Quicksort on a hypercube</a>
</p>
</p>

<p name="switchToTextMode">
As was apparent from the previous section, for an efficient
parallelization of the quicksort algorithm, we need to make the Dutch
National Flag reordering parallel too. Let us then assume that the
array has been partitioned over the $p$ processors of a hypercube of
dimension~$d$ (meaning that $p=2^d$).
</p>

<p name="switchToTextMode">
In the first step of the parallel algorithm, we choose a pivot, and
broadcast it to all processors. All processors will then apply the
reordering independently on their local data.
</p>

<p name="switchToTextMode">
In order to bring together the red and blue elements in this first
level, every processor is now paired up with one that has a binary
address that is the same in every bit but the most significant one. In
each pair, the blue elements are sent to the processor that has a
1~value in that bit; the red elements go to the processor that has a
0~value in that bit.
</p>

<p name="switchToTextMode">
After this exchange (which is local, and therefore fully parallel),
the processors with an address $1xxxxx$ have all the red elements, and
the processors with an address $0xxxxx$ have all the blue
elements. The previous steps can now be repeated on the subcubes.
</p>

<p name="switchToTextMode">
This algorithm keeps all processors working in every step; however, it
is susceptible to load imbalance if the chosen pivots are far from the
median. Moreover, this load imbalance is not lessened during the sort
process.
</p>

<h3><a id="Quicksortonageneralparallelprocessor">8.3.3</a> Quicksort on a general parallel processor</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Quicksort">Quicksort</a> > <a href="sorting.html#Quicksortonageneralparallelprocessor">Quicksort on a general parallel processor</a>
</p>
<p name="switchToTextMode">

Quicksort can also be done on any parallel machine that has a linear
ordering of the processors. We assume at first that every processor
holds exactly one array element, and, because of the flag reordering,
sorting will always involve a consecutive set of processors.
</p>

<p name="switchToTextMode">
Parallel quicksort of an array (or subarray in a recursive call)
starts by constructing a binary tree on the processors storing the
array. A~pivot value is chosen and broadcast through the tree. The
tree structure is then used to count on each processor how many
elements in the left and right subtree are less than, equal to, or
more than the pivot value.
</p>

<p name="switchToTextMode">
With this information, the root processor can compute where the
red/white/blue regions are going to be stored. This information is
sent down the tree, and every subtree computes the target locations
for the elements in its subtree.
</p>

<p name="switchToTextMode">
If we ignore network contention, the reordering can now be done in
unit time, since each processor sends at most one element. This means
that each stage only takes time in summing the number of blue and red
elements in the subtrees, which is $O(\log n)$ on the top level,
$O(\log n/2)$~on the next, et cetera. This makes for almost perfect
speedup.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Radixsort">8.4</a> Radixsort</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Radixsort">Radixsort</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
Most sorting algorithms are based on comparing the full item value. By
contrast, 
<i>radix sort</i>
 does a number of partial sorting stages on
the digits of the number. For each digit value a `bin' is allocated,
and numbers are moved into these bins. Concatenating these bins gives
a partially sorted array, and by moving through the digit
positions, the array gets increasingly sorted.
</p>

<p name="switchToTextMode">
Consider an example with number of at most two digits, so two stages
are needed:
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  </td></tr>
<tr><td>
  array     </td><td> 25</td><td>52</td><td>71</td><td>12</td></tr>
<tr><td>
  </td></tr>
<tr><td>
  last digit</td><td>  5</td><td> 2</td><td> 1</td><td> 2</td></tr>
<tr><td>
  (only bins 1,2,5 receive data)</td></tr>
<tr><td>
  sorted    </td><td> 71</td><td>52</td><td>12</td><td>25</td></tr>
<tr><td>
  next digit</td><td>  7</td><td> 5</td><td> 1</td><td> 2</td></tr>
<tr><td>
  sorted    </td><td> 12</td><td>25</td><td>52</td><td>71</td></tr>
<tr><td>
  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

It is important that the partial ordering of one stage is preserved
during the next. Inductively we then arrive at a totally sorted array
in the end.
</p>

<h3><a id="Parallelradixsort">8.4.1</a> Parallel radix sort</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Radixsort">Radixsort</a> > <a href="sorting.html#Parallelradixsort">Parallel radix sort</a>
</p>
<!-- index -->
<p name="switchToTextMode">

A distributed memory sorting algorithm already has an obvious
`binning' of the data, so a natural parallel implementation of radix
sort is based on using~$P$, the number of processes, as radix.
</p>

<p name="switchToTextMode">
We illustrate this with an example on two processors, meaning that we
look at binary representations of the values.
</p>

<!-- environment: tabular start embedded generator -->
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  </td></tr>
<tr><td>
            </td><td> proc0</td><td>proc0</td></tr>
<tr><td>
  array     </td><td> 2</td><td>5</td><td>7</td><td>1</td><td></td></tr>
<tr><td>
  binary    </td><td> 010</td><td> 101</td><td> 111</td><td> 001</td><td></td></tr>
<tr><td>
  </td></tr>
<tr><td>
  stage 1: sort by least significant bit</td></tr>
<tr><td>
  </td></tr>
<tr><td>
  last digit</td><td>   $\hphantom{00}0$</td><td>   $\hphantom{00}1$</td><td>   $\hphantom{00}1$</td><td>   $\hphantom{00}1$</td><td></td></tr>
<tr><td>
  </td><td>(this serves as bin number)</td></tr>
<tr><td>
  sorted    </td><td> 010</td><td>    </td><td> 101</td><td> 111</td><td> 001</td></tr>
<tr><td>
  </td></tr>
<tr><td>
  stage 2: sort by middle bit</td></tr>
<tr><td>
  </td></tr>
<tr><td>
  next digit</td><td>  $\hphantom{0}1 $</td><td>                    </td><td>$\hphantom{0}0 $</td><td>  $\hphantom{0}1 $</td><td>  $\hphantom{0}0$</td></tr>
<tr><td>
  </td><td>(this serves as bin number)</td></tr>
<tr><td>
  sorted    </td><td> 101</td><td> 001</td><td> 010</td><td> 111</td><td></td></tr>
<tr><td>
  </td></tr>
<tr><td>
  stage 3: sort by most significant bit</td></tr>
<tr><td>
  </td></tr>
<tr><td>
  next digit</td><td> $\hphantom{}1  $</td><td> $\hphantom{}0  $</td><td> $\hphantom{}0  $</td><td> $\hphantom{}1$</td><td></td></tr>
<tr><td>
  </td><td>(this serves as bin number)</td></tr>
<tr><td>
  sorted    </td><td> 001</td><td> 010</td><td> 101</td><td> 111</td><td></td></tr>
<tr><td>
  decimal   </td><td> 1  </td><td> 2  </td><td> 5  </td><td> 7</td><td></td></tr>
<tr><td>
  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

(We see that there can be load imbalance during the algorithm.)
</p>

<p name="switchToTextMode">
Analysis:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Determining the digits under consideration, and determining how
  many local values go into which bin are local operations. We can
  consider this as a connectivity matrix~$C$ where $C[i,j]$ is the
  amount of data that process~$i$ will send to process~$j$. Each
  process owns its row of this matrix.
<li>
In order to receive data in the shuffle, each process needs to know
  how much data will receive from every other process. This requires a
  `transposition' of the connectivity matrix. In MPI terms, this is an
<i>all-to-all</i>
 operation: 
<tt>MPI_Alltoall</tt>
.
<li>
After this, the actual data can be shuffled in another
  all-to-all operation. However, this the amounts differ per $i,j$
  combination, we need the 
<tt>MPI_Alltoallv</tt>
 routine.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Radixsortbymostsignificantdigit">8.4.2</a> Radix sort by most significant digit</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Radixsort">Radixsort</a> > <a href="sorting.html#Radixsortbymostsignificantdigit">Radix sort by most significant digit</a>
</p>
</p>

<p name="switchToTextMode">
It is perfectly possible to let the stages of the radix sort progress
from most to least significant digit, rather than the
reverse. Sequentially this does not change anything of significance.
</p>

<p name="switchToTextMode">
However,
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Rather than full shuffles, we are shuffling in increasingly
  small subsets, so even the sequential algorithm will have increased
<i>spatial locality</i>
.
<li>
A shared memory parallel version will show similar locality
  improvements.
<li>
An MPI version no longer needs all-to-all operations. If we
  recognize that every next stage will be within a subset of
  processes, we can use the communicator splitting mechanism.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The locality argument above was done somewhat hand-wavingly. Argue
  that the algorithm can be done in both a 
<i>breadth-first</i>
  and 
<i>depth-first</i>
 fashion. Discuss the relation of this
  distinction with the locality argument.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->

</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Samplesort">8.5</a> Samplesort</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Samplesort">Samplesort</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
You saw in 
<i>Quicksort</i>
 (section&nbsp;
8.3
) that
it is possible to use probabilistic elements in a sorting
algorithm. We can extend the idea of picking a single pivot, as in
Quicksort, to that of picking as many pivots as there are processors.
Instead of a 
<i>bisection</i>
 of the elements, this divides the
elements into as many `buckets' as there are processors. Each
processor then sorts its elements fully in parallel.
</p>

<!-- environment: displayalgorithm start embedded generator -->
<p>Input: $p$: the number of processors, $N$: the numbers of elements to    sort; $\{x\_i\}\_{i&lt;N}$ the elements to sort}  Let $x\_0=b\_0&lt;b\_1&lt;\cdots&lt;b\_{p-1}&lt;b\_p=x\_N$ (where $x\_N&gt;x\_{N-1}$ arbitrary)\;  \For { $i=0,&hellip;,p-1$ } { Let $s\_i=[b\_i,&hellip; b\_{i+1}-1]$ }  \For { $i=0,&hellip;,p-1$ } { Assign the elements in $s\_i$ to    processor&nbsp;$i$ }  \For { $i=0,&hellip;,p-1$ in parallel } { Let processor&nbsp;$i$ sort its    elements }  \caption{The Samplesort algorithm</p>

<!-- environment: displayalgorithm end embedded generator -->
<p name="switchToTextMode">

Clearly this algorithm can have severe 
<i>load imbalance</i>
if the buckets are not chosen carefully. Randomly picking $p$ elements is
probably not good enough; instead, some form of 
<i>sampling</i>
of the elements is needed. Correspondingly, this algorithm is known as
\textbf{Samplesort}&nbsp;
<a href="https://pages.tacc.utexas.edu/~eijkhout/istc/html/bibliography.html#Blelloch:1991:CM2sort">[Blelloch:1991:CM2sort]</a>
.
</p>

<p name="switchToTextMode">
While the sorting of the buckets, once assigned, is fully parallel,
this algorithm still has some problems regarding parallelism.
First of all, the sampling is a sequential bottleneck for the algorithm.
Also, the step where buckets are assigned to processors is essentially
an 
<i>all-to-all</i>
 operation,
</p>

<p name="switchToTextMode">
For an analysis of this, assume that there are $P$ processes that
first function as mappers, then as reducers. Let $N$&nbsp;be the number of
data points, and define a block size $b\equiv N/P$. The cost of the
processing steps of the algorithm is:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The local determination of the bin for each element, which takes
  time&nbsp;$O(b)$; and
<li>
The local sort, for which is we can assume an optimal complexity
  of&nbsp;$b\log b$.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
However, the shuffle step is non-trivial. Unless the data is partially
pre-sorted, we can expect the shuffle to be a full
<i>all-to-all</i>
, with a time complexity of $P\alpha+b\beta$.
Also, this may become a network bottleneck.
Note that in Quicksort on a hypercube there was never any
<i>contention</i>
 for the wires.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Argue that for a small number of processes, $P\ll N$, this algorithm
  has perfect speedup and a sequential complexity (see above) of
  $N\log N$.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Comparing this algorithm to sorting networks like bitonic sort
this sorting algorithm looks considerable simpler: it has only a
one-step network. The previous question argued that in the `optimistic
scaling' (work can increase while keeping number of processors
constant) the sequential complexity is the same as for the sequential
algorithm. However, in the weak scaling analysis where we increase
work and processors proportionally, the sequential complexity is
considerably worse.
</p>

<!-- environment: exercise start embedded generator -->
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider the case where we scale both&nbsp;$N,P$, keeping $b$
  constant.
  Argue that in this case the shuffle step introduces an $N^2$ term
  into the algorithm.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h3><a id="SortingthroughMapReduce">8.5.1</a> Sorting through MapReduce</h3>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Samplesort">Samplesort</a> > <a href="sorting.html#SortingthroughMapReduce">Sorting through MapReduce</a>
</p>

<!-- index -->
<p name="switchToTextMode">

The 
<i>terasort</i>
 benchmark concerns
<i>sorting</i>
<!-- index -->
 on a large
file-based dataset. Thus, it is somewhat of a standard in
<i>big data</i>
 systems. In particular,
<i>MapReduce</i>
 is a prime candidate; see

<a href=http://perspectives.mvdirona.com/2008/07/hadoop-wins-terasort/>http://perspectives.mvdirona.com/2008/07/hadoop-wins-terasort/</a>
.
</p>

<p name="switchToTextMode">
Using MapReduce, the algorithm proceeds as follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
A set of key values is determined through sampling or from prior
  information: the keys are such that an approximately equal number of
  records is expected to fall in between each pair. The number of
  intervals equals the number of reducer processes.
<li>
The mapper processes then produces key/value pairs where the key
  is the interval or reducer number.
<li>
The reducer processes then perform a local sort.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

We see that, modulo terminology changes, this is really samplesort.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Bitonicsort">8.6</a> Bitonic sort</h2>
<p name=crumbs>
crumb trail:  > <a href="sorting.html">sorting</a> > <a href="sorting.html#Bitonicsort">Bitonic sort</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
To motivate bitonic sorting,
suppose a sequence $x=\langle x_0,&hellip;,x_n-1\rangle$
consists of an ascending
followed by a descending part.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bitonic1.jpg" width=800></img>
<p name="caption">
FIGURE 8.3: Illustration of a splitting a bitonic sequence
</p>
</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Now split this sequence into two subsequences of equal length
defined by:
<!-- environment: equation start embedded generator -->
</p>
\begin{array}{cc}
s\_1 = \langle \min\{x\_0,x\_{n/2}\},&hellip; \min\{ x\_{n/2-1},x\_{n-1}\}\\
s\_2 = \langle \max\{x\_0,x\_{n/2}\},&hellip; \max\{ x\_{n/2-1},x\_{n-1}\}\\
\end{array}
\label{eq:bitonic-split}
\end{equation}
</equation>
<!-- environment: equation end embedded generator -->
<p name="switchToTextMode">
From the picture it is easy to see that $s_1,s_2$ are again
sequences with an ascending and descending part. Moreover,
all the elements in $s_1$ are less than all the elements in&nbsp;$s_2$.
</p>

<p name="switchToTextMode">
We call \eqref{eq:bitonic-split} an ascending bitonic sorter, sinc the
second subsequence contains elements larger than in the
first. Likewise we can construct a descending sorter by reversing the
roles of maximum and minimum.
</p>

<p name="switchToTextMode">
It's not hard to imagine that this is a step in a sorting algorithm:
starting out with a sequence on this form, recursive application
of formula&nbsp;\eqref{eq:bitonic-split} gives a sorted sequence.
<!-- environment: figure start embedded generator -->
</p>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bitonic2.jpg" width=800></img>
<p name="caption">
FIGURE 8.4: Illustration of a  bitonic network that sorts a bitonic sequence of length&nbsp;16
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
8.4
 shows how 4 bitonic sorters, over
distances 8,4,2,1 respectively, will sort a sequence of length&nbsp;16.
</p>

<p name="switchToTextMode">
The actual definition of a 
<i>bitonic sequence</i>
 is slightly more
complicated. A&nbsp;sequence is bitonic if it conists of an ascending part followed
by a descending part, or is a cyclic permutation of such a sequence.
<!-- environment: exercise start embedded generator -->
</p>
<!-- TranslatingLineGenerator exercise ['exercise'] -->
Prove that splitting a bitonic sequence according to
formula&nbsp;\eqref{eq:bitonic-split} gives two bitonic sequences.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

So the question is how to get a bitonic sequence. The answer is to
use larger and larger bitonic networks.
<!-- environment: itemize start embedded generator -->
</p>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
A bitonic sort of two elements gives you a sorted sequence.
<li>
If you have two sequences of length two, one sorted up, the
  other sorted down, that is a bitonic sequence.
<li>
So this sequence of length four can be sorted in two bitonic steps.
<li>
And two sorted sequences of length four form a bitonic sequence of length;
<li>
which can be sorted in three bitonic steps; et cetera.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<!-- environment: figure start embedded generator -->
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/full-bitonic.png" width=800></img>
<p name="caption">
FIGURE 8.5: Full bitonic sort for 16 elements
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
From this description you see that you $\log_2N$ stages to sort $N$
elements, where the $i$-th stage is of length&nbsp;$\log_2i$. This makes
the total
<i>sequential complexity of bitonic sort</i>
<!-- index -->
&nbsp;$(\log_2N)^2$.
</p>

<p name="switchToTextMode">
The sequence of operations in figure&nbsp;
8.5
is called a 
<i>sorting network</i>
, built up out of simple
<i>compare-and-swap</i>
 elements. There is no dependence
on the value of the data elements, as is the case with quicksort.
</p>

<!-- index -->
<p name="switchToTextMode">

<!-- index -->
</p>

</div>
<a href="index.html">Back to Table of Contents</a>
