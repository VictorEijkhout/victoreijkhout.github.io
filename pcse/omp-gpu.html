<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="http://ccrs.cac.cornell.edu:8080/client.0.1.js"></script>
<style>
</style>

<script type="application/javascript">
  // First we declare some metadata, primarily to describe
  // the container environment.
  var ccrsApiNamespace = "org.xsede.jobrunner.model.ModelApi";
  var mpiExampleMetaJson = {
    // CHANGE: for now, leave the appended string as .SysJobMetaData;
    //         other options will be supported in the future
    "$type": ccrsApiNamespace + ".SysJobMetaData",
    // CHANGE: shell to use implicitly when running commands in the container
    "shell": ["bash"],
    // CHANGE: should currently be one of: .NixOS, .Singularity
    "containerType": {
      "$type":  ccrsApiNamespace + ".NixOS"
    },
    // CHANGE: Specify for NixOS for all jobs, or for Singularity when resuming existing jobs
    "containerId": ["vicOpenMPI"],
    // CHANGE: Specify the singularity image name
    "image": [],
    // Directories on the host to mount in the container, if any:
    "binds": [],
    // Only for singularity:
    "overlay": [],
    // CHANGE: should be filled in dynamically to contain the (student) user,
    //         but this is a demo, so we use a static user name:
    "user": "test0",
    "address": [],
    "hostname": [],
    "url": window.location.href
  };
  var mpiExampleMeta = CCRS.sysJobMetaData(mpiExampleMetaJson);
</script>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>OpenMP topic: Offloading</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


28.1.1 : <a href="omp-gpu.html#Targetsandtasks">Targets and tasks</a><br>
28.2 : <a href="omp-gpu.html#Dataonthedevice">Data on the device</a><br>
28.3 : <a href="omp-gpu.html#Executiononthedevice">Execution on the device</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>28 OpenMP topic: Offloading</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

This chapter explains the mechanisms for offloading work to a 
<span title="acronym" ><i>GPU</i></span>
.
</p>

<p name="switchToTextMode">
The memory of a processor and that of an attached 
<span title="acronym" ><i>GPU</i></span>
 are not
<i>coherent</i>
<!-- index -->
:
there are separate memory spaces and writing data in one
is not automatically reflected in the other.
</p>

<p name="switchToTextMode">
OpenMP transfers data (or maps it) when you enter an 
construct.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp target
{
  // do stuff on the GPU
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

You can test whether the target region is indeed executed on a device
with 
<tt>omp_is_initial_device</tt>
:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp target
  if (omp_is_initial_device()) printf("Offloading failed\n");
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Targetsandtasks">28.1.1</a> Targets and tasks</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-gpu.html">omp-gpu</a>
</p>
</p>

<p name="switchToTextMode">
The 
<i>target task</i>
.
This is a task running on the host, dedicated to managing the
offloaded region.
</p>

<p name="switchToTextMode">
The 
by a new 
<i>initial task</i>
.
This is distinct from the initial task that executes the main program.
</p>

<p name="switchToTextMode">
The task that created the target task is called the
<i>generating task</i>
.
</p>

<p name="switchToTextMode">
By default, the generating task is blocked while the task on the device is running,
but adding the 
This requires a 
<tt>taskwait</tt>
 directive to synchronize host and device.
</p>

<h2><a id="Dataonthedevice">28.2</a> Data on the device</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-gpu.html">omp-gpu</a> > <a href="omp-gpu.html#Dataonthedevice">Data on the device</a>
</p>
<p name="switchToTextMode">

<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Scalars are treated as 
  they are copied in but not out.
<li>
Stack arrays 
<li>
Heap arrays are not mapped by default.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

For explicit mapping with 
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp target map(...)
{
  // do stuff on the GPU
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
The following map options exist:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
 <tt>map(to: x,y,z)</tt>  copy from host to device
  when entering the target region.
<li>
 <tt>map(from: x,y,z)</tt>  copy from devince to host
  when exiting the target region.
<li>
 <tt>map(tofrom: x,y,z)</tt>  is equivalent to combining the previous two.
<li>
 <tt>map(allo: x,y,z)</tt>  allocates data on the device.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: fortrannote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=fortrannote ]] -->
<remark>
<b>Fortran note</b>
<p name="remark">
<!-- TranslatingLineGenerator fortrannote ['fortrannote'] -->
  If the compiler can deduce the array bounds and size,
  it is not necessary to specify them in the `map' clause.
</p name="remark">
</remark>
<!-- environment: fortrannote end embedded generator -->
<p name="switchToTextMode">
{Array sizes in map clause}
</p>

<p name="switchToTextMode">
Data transfer to a device is probably slow,
so mapping the data at the start of an offloaded section of code
is probably not the best idea.
Additionally, in many cases data will stay resident on the device
throughout several iterations of, for instance, a time-stepping 
<span title="acronym" ><i>PDE</i></span>
 solver.
For such reasons, it is possible to move data onto, and off from,
the device explicitly,
using the 
</p>

<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp target enter data map(to: x,y)
#pragma omp target
{
  // do something
}
#pragma omp target enter data map(from: x,y)
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

Also 
(synchronize data from host to device),
(synchronize data to host from device).
</p>

<h2><a id="Executiononthedevice">28.3</a> Execution on the device</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-gpu.html">omp-gpu</a> > <a href="omp-gpu.html#Executiononthedevice">Execution on the device</a>
</p>
<p name="switchToTextMode">

For parallel execution of a loop on the device
use the 
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp target teams distribute parallel do
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

On GPU devices and the like, there is a structure to threads:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
threads are grouped in 
  and they can be synchronized only within these teams;
<li>
teams are groups in 
  and no synchronization between leagues is possible
  inside a 
<tt>target</tt>
 region.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

The combination 
<tt>teams distribute</tt>
 splits the iteration space over teams.
By default a static schedule is used,
but the option 
However, this combination only gives the chunk of space to the master thread
in each team.
Next we need 
<tt>parallel for</tt>
 or 
<tt>parallel do</tt>
 to spread the chunk over the
threads in the team.
</div>
<a href="index.html">Back to Table of Contents</a>
