<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?...">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src=https://ccrs.cac.cornell.edu:8443/client.0.2.js></script>
<script id="script">
class Example {
  constructor(buttonID, editorID, outputID, sourceFile, fileName, commandStr) {
    this.buttonID = buttonID;
    this.editorID = editorID;
    this.outputID = outputID;
    this.sourceFile = sourceFile;
    this.fileName = fileName;
    this.commandStr = commandStr;
  }
    
  async display(results, object) {
    if (results.stdout.length > 0)
      document.getElementById(object.outputID).textContent = results.stdout;
    else
      document.getElementById(object.outputID).textContent = results.stderr;
  }

  async initialize() {
    this.editor = await MonacoEditorFileSource.create(this.editorID);
    this.editor.setTextFromFile(this.sourceFile);
    this.job = await Job.create(JobType.MPI);
    this.command = new CommandWithFiles(this.job, this.commandStr);
    this.command.addFileSource(this.editor, this.fileName);
    this.trigger = new ButtonTrigger(this.command, this.display, this.buttonID, this);
    document.getElementById(this.buttonID).disabled = false;
  }
}
</script>
<style></style>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>MPI topic: Collectives</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


3.1 : <a href="mpi-collective.html#Workingwithglobalinformation">Working with global information</a><br>
3.1.1 : <a href="mpi-collective.html#Practicaluseofcollectives">Practical use of collectives</a><br>
3.1.2 : <a href="mpi-collective.html#Synchronization">Synchronization</a><br>
3.1.3 : <a href="mpi-collective.html#CollectivesinMPI">Collectives in MPI</a><br>
3.2 : <a href="mpi-collective.html#Reduction">Reduction</a><br>
3.2.1 : <a href="mpi-collective.html#Reducetoall">Reduce to all</a><br>
3.2.1.1 : <a href="mpi-collective.html#Bufferdescription">Buffer description</a><br>
3.2.1.2 : <a href="mpi-collective.html#Examplesandexercises">Examples and exercises</a><br>
3.2.2 : <a href="mpi-collective.html#Innerproductasallreduce">Inner product as allreduce</a><br>
3.2.3 : <a href="mpi-collective.html#Reductionoperations">Reduction operations</a><br>
3.2.4 : <a href="mpi-collective.html#Databuffers">Data buffers</a><br>
3.3 : <a href="mpi-collective.html#Rootedcollectives:broadcast,reduce">Rooted collectives: broadcast, reduce</a><br>
3.3.1 : <a href="mpi-collective.html#Reducetoaroot">Reduce to a root</a><br>
3.3.2 : <a href="mpi-collective.html#Reduceinplace">Reduce in place</a><br>
3.3.3 : <a href="mpi-collective.html#Broadcast">Broadcast</a><br>
3.4 : <a href="mpi-collective.html#Scanoperations">Scan operations</a><br>
3.4.1 : <a href="mpi-collective.html#Exclusivescan">Exclusive scan</a><br>
3.4.2 : <a href="mpi-collective.html#Useofscanoperations">Use of scan operations</a><br>
3.5 : <a href="mpi-collective.html#Rootedcollectives:gatherandscatter">Rooted collectives: gather and scatter</a><br>
3.5.1 : <a href="mpi-collective.html#Examples">Examples</a><br>
3.5.2 : <a href="mpi-collective.html#Allgather">Allgather</a><br>
3.6 : <a href="mpi-collective.html#All-to-all">All-to-all</a><br>
3.6.1 : <a href="mpi-collective.html#All-to-allasdatatranspose">All-to-all as data transpose</a><br>
3.6.2 : <a href="mpi-collective.html#All-to-all-v">All-to-all-v</a><br>
3.7 : <a href="mpi-collective.html#Reduce-scatter">Reduce-scatter</a><br>
3.7.1 : <a href="mpi-collective.html#Examples">Examples</a><br>
3.8 : <a href="mpi-collective.html#Barrier">Barrier</a><br>
3.9 : <a href="mpi-collective.html#Variable-size-inputcollectives">Variable-size-input collectives</a><br>
3.9.1 : <a href="mpi-collective.html#ExampleofGatherv">Example of Gatherv</a><br>
3.9.2 : <a href="mpi-collective.html#ExampleofAllgatherv">Example of Allgatherv</a><br>
3.9.3 : <a href="mpi-collective.html#Variableall-to-all">Variable all-to-all</a><br>
3.10 : <a href="mpi-collective.html#MPIOperators">MPI Operators</a><br>
3.10.1 : <a href="mpi-collective.html#Pre-definedoperators">Pre-defined operators</a><br>
3.10.1.1 : <a href="mpi-collective.html#Minlocandmaxloc">Minloc and maxloc</a><br>
3.10.2 : <a href="mpi-collective.html#User-definedoperators">User-defined operators</a><br>
3.10.3 : <a href="mpi-collective.html#Localreduction">Local reduction</a><br>
3.11 : <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a><br>
3.11.1 : <a href="mpi-collective.html#Examples">Examples</a><br>
3.11.1.1 : <a href="mpi-collective.html#Arraytranspose">Array transpose</a><br>
3.11.1.2 : <a href="mpi-collective.html#Stencils">Stencils</a><br>
3.11.2 : <a href="mpi-collective.html#Nonblockingbarrier">Nonblocking barrier</a><br>
3.12 : <a href="mpi-collective.html#Performanceofcollectives">Performance of collectives</a><br>
3.13 : <a href="mpi-collective.html#Collectivesandsynchronization">Collectives and synchronization</a><br>
3.14 : <a href="mpi-collective.html#Performanceconsiderations">Performance considerations</a><br>
3.14.1 : <a href="mpi-collective.html#Scalability">Scalability</a><br>
3.14.2 : <a href="mpi-collective.html#Complexityandscalabilityofcollectives">Complexity and scalability of collectives</a><br>
3.14.2.1 : <a href="mpi-collective.html#Broadcast">Broadcast</a><br>
3.15 : <a href="mpi-collective.html#Reviewquestions">Review questions</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>3 MPI topic: Collectives</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

A certain class of MPI routines are called `collective', or more correctly:
`collective on a communicator'.
This means that if process one in that communicator calls that routine,
they all need to call that routine.
In this chapter we will discuss collective routines
that are about combining the data on all processes
in that communicator,
but there are also operations such as opening a shared file
that are collective, which will be discussed in a later chapter.
</p>

<h2><a id="Workingwithglobalinformation">3.1</a> Working with global information</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Workingwithglobalinformation">Working with global information</a>
</p>

<p name="switchToTextMode">

If all processes have individual data, for instance the result
of a local computation, you may want to bring that information
together, for instance to find the maximal computed value
or the sum of all values. Conversely, sometimes one processor has
information that needs to be shared with all.
For this sort of operation, MPI
has 
<i>collectives</i>
.
</p>

<p name="switchToTextMode">
There are various cases, illustrated in figure~
3.1
,
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/collective_comm.jpg" width=800></img>
<p name="caption">
FIGURE 3.1: The four most common collectives
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
which you can (sort of) motivate by considering some classroom activities:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The teacher tells the class when the exam will be. This is a
<i>broadcast</i>
: the same item of information goes to everyone.
<li>
After the exam, the teacher performs a 
<i>gather</i>
  operation to collect the invidivual exams.
<li>
On the other hand, when the teacher computes the average grade,
  each student has an individual number, but these are now combined to
  compute a single number. This is a 
<i>reduction</i>
.
<li>
Now the teacher has a list of grades and gives each student their
  grade. This is a 
<i>scatter</i>
 operation, where one process
  has multiple data items, and gives a different one to all the other
  processes.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

This story is a little different from what happens with MPI
processes, because these are more symmetric; the process doing the
reducing and broadcasting is no different from the others.
Any process can function as the 
<i>root process</i>
 in such a
collective.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  How would you realize the following scenarios with MPI collectives?
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Let each process compute a random number. You want to print the
    maximum of these numbers to your screen.
<li>
Each process computes a random number again. Now you want to
    scale these numbers by their maximum.
<li>
Let each process compute a random number. You want to print on what processor the
    maximum value is computed.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
  Think about time and space complexity of your suggestions.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Practicaluseofcollectives">3.1.1</a> Practical use of collectives</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Workingwithglobalinformation">Working with global information</a> > <a href="mpi-collective.html#Practicaluseofcollectives">Practical use of collectives</a>
</p>
</p>

<p name="switchToTextMode">
Collectives are quite common in scientific applications. For instance,
if one process reads data from disc or the commandline, it can use a
broadcast or a gather to get the information to other
processes. Likewise, at the end of a program run, a&nbsp;gather or
reduction can be used to collect summary information about the program
run.
</p>

<p name="switchToTextMode">
However, a more common scenario is
that the result of a collective is needed on all processes.
</p>

<p name="switchToTextMode">
Consider the computation of the 
<i>standard deviation</i>
:
\[
 \sigma = \sqrt{\frac1{N-1} \sum_i^N (x_i-\mu)^2 }
\qquad\hbox{where}\qquad \mu = \frac{\sum_i^Nx_i}N
\]
and assume that every process stores just one&nbsp;$x_i$ value.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
The calculation of the average&nbsp;$\mu$ is a reduction, since all
  the distributed values need to be added.
<li>
Now every
  process needs to compute&nbsp;$x_i-\mu$ for its value&nbsp;$x_i$, so $\mu$&nbsp;is
  needed everywhere. You can compute this by doing a reduction followed
  by a broadcast, but it is better to use a so-called
<i>allreduce</i>
 operation, which does the reduction and leaves
  the result on all processors.
<li>
The calculation of $\sum_i(x_i-\mu)$ is another sum of
  distributed data, so we need another reduction operation. Depending
  on whether each process needs to know&nbsp;$\sigma$, we can again use an
  allreduce.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

\[
 y- (x^ty)x 
\]
<i>Eijkhout:IntroHPC</i>
.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
%% // compute local value
%% localvalue = innerproduct( x[ localpart], y[ localpart ] );
%% // compute inner product on the every process
%% AllReduce( localvalue, reducedvalue );
%% 
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Synchronization">3.1.2</a> Synchronization</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Workingwithglobalinformation">Working with global information</a> > <a href="mpi-collective.html#Synchronization">Synchronization</a>
</p>
</p>

<p name="switchToTextMode">
Collectives are operations that involve all processes in a
communicator. %%(See section&nbsp;
 for an informal listing.)
A&nbsp;collective is a
single call, and it blocks on all processors,
meaning that a process calling a collective cannot proceed
until the other processes have similarly called the collective.
</p>

<p name="switchToTextMode">
That does not mean that
all processors exit the call at the same time: because of
implementational details and network
latency they need not be synchronized in their execution.
However, semantically we can say that
a&nbsp;process can not finish
a collective until every other process has at least started the collective.
</p>

<p name="switchToTextMode">
In addition to these collective operations, there are operations that
are said to be `collective on their communicator', but which do not
involve data movement. Collective then means that all processors must
call this routine; not to do so is an error that will
manifest itself in `hanging' code. One such example is
<tt>MPI_File_open</tt>
.
</p>

<h3><a id="CollectivesinMPI">3.1.3</a> Collectives in MPI</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Workingwithglobalinformation">Working with global information</a> > <a href="mpi-collective.html#CollectivesinMPI">Collectives in MPI</a>
</p>
<p name="switchToTextMode">

We will now explain the MPI collectives in the following order.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
[Allreduce] We use the allreduce as an introduction to the
  concepts behind collectives; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reduction">3.2</a>
. As
  explained above, this routines serves many practical scenarios.
<li>
[Broadcast and reduce] We then introduce the concept of a root
  in the reduce (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reducetoaroot">3.3.1</a>
) and broadcast
  (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Broadcast">3.3.3</a>
) collectives.
<li>
Sometimes you want a reduction with partial results, where each processor
  computes the sum (or other operation) on the values of lower-numbered processors.
  For this, you use a 
<i>scan</i>
 collective (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Scanoperations">3.4</a>
).
<li>
[Gather and scatter] The gather/scatter collectives deal with
  more than a single data item; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Rootedcollectives:gatherandscatter">3.5</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

There are more collectives or variants on the above.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
If every processor needs to broadcast to every other, you use an
<i>all-to-all</i>
 operation (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#All-to-all">3.6</a>
).
<li>
The reduce-scatter is a lesser known combination of collectives;
  section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reduce-scatter">3.7</a>
.
<li>
A barrier is an operation that makes all processes wait until every
  process has reached the barrier (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Barrier">3.8</a>
).
<li>
If you want to gather or scatter information, but the contribution
  of each processor is of a different size, there are `variable' collectives;
  they have a&nbsp;
<tt>v</tt>
 in the name (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Variable-size-inputcollectives">3.9</a>
).
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Finally, there are some advanced topics in collectives.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
User-defined reduction operators; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#User-definedoperators">3.10.2</a>
.
<li>
Nonblocking collectives; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Nonblockingcollectives">3.11</a>
.
<li>
We briefly discuss performance aspects of collectives in section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Performanceofcollectives">3.12</a>
.
<li>
We discuss synchronization aspects in section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Collectivesandsynchronization">3.13</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- TranslatingLineGenerator file ['file'] -->
</p>

<h2><a id="Reduction">3.2</a> Reduction</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a>
</p>

<p name="switchToTextMode">

<h3><a id="Reducetoall">3.2.1</a> Reduce to all</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Reducetoall">Reduce to all</a>
</p>
</p>

<p name="switchToTextMode">
Above we saw a couple of scenarios where a quantity is reduced, with
all proceses getting the result. The MPI call for this is
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Allreduce" aria-expanded="false" aria-controls="MPI_Allreduce">
        Routine reference: MPI_Allreduce
      </button>
    </h5>
  </div>
  <div id="MPI_Allreduce" class="collapse">
  <pre>
C:
int MPI_Allreduce(const void* sendbuf,
  void* recvbuf, int count, MPI_Datatype datatype,
  MPI_Op op, MPI_Comm comm)

Semantics:
IN sendbuf: starting address of send buffer (choice)
OUT recvbuf: starting address of receive buffer (choice)
IN count: number of elements in send buffer (non-negative integer)
IN datatype: data type of elements of send buffer (handle)
IN op: operation (handle)
IN comm: communicator (handle)

Fortran:
MPI_Allreduce(sendbuf, recvbuf, count, datatype, op, comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: count
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Op), INTENT(IN) :: op
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python native:
recvobj = MPI.Comm.allreduce(self, sendobj, op=SUM)
Python numpy:
MPI.Comm.Allreduce(self, sendbuf, recvbuf, Op op=SUM)

MPL:
template<typename T , typename F >
void mpl::communicator::allreduce
   ( F,	const T &, T & ) const;
   ( F,	const T *, T *,
     const contiguous_layout< T > & ) const;
   ( F,	T & ) const;
   ( F,	T *, const contiguous_layout< T > & ) const;
F : reduction function
T : type
</pre>
</div>
</div>
<i>MPI_Allreduce</i>
.
</p>

<p name="switchToTextMode">
Example: we give each process a random number, and sum these numbers together.
The result should be approximately $1/2$ times the number of processes.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allreducec" aria-expanded="false" aria-controls="allreducec">
        C Code: allreducec
      </button>
    </h5>
  </div>
  <div id="allreducec" class="collapse">
  <pre>
// allreduce.c
float myrandom,sumrandom;
myrandom = (float) rand()/(float)RAND_MAX;
// add the random variables together
MPI_Allreduce(&myrandom,&sumrandom,
		1,MPI_FLOAT,MPI_SUM,comm);
// the result should be approx nprocs/2:
if (procno==nprocs-1)
  printf("Result %6.9f compared to .5\n",sumrandom/nprocs);
</pre>
</div>
</div>
<p name="switchToTextMode">

Or:
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducecount" aria-expanded="false" aria-controls="reducecount">
        C Code: reducecount
      </button>
    </h5>
  </div>
  <div id="reducecount" class="collapse">
  <pre>
MPI_Count buffersize = 1000;
double *indata,*outdata;
indata = (double*) malloc( buffersize*sizeof(double) );
outdata = (double*) malloc( buffersize*sizeof(double) );
MPI_Allreduce_c(indata,outdata,buffersize,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);
</pre>
</div>
</div>
<p name="switchToTextMode">

<h4><a id="Bufferdescription">3.2.1.1</a> Buffer description</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Reducetoall">Reduce to all</a> > <a href="mpi-collective.html#Bufferdescription">Buffer description</a>
</p>
</p>

<p name="switchToTextMode">
This is the first example in this course that involves MPI data buffers:
the 
<tt>MPI_Allreduce</tt>
 call contains two buffer arguments.
In most MPI calls (with the one-sided ones as big exception)
a buffer is described by three parameters:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
a pointer to the data,
<li>
the number of items in the buffer, and
<li>
the datatype of the items in the buffer.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
Each of these needs some elucidation.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
The buffer specification depends on the programming languages.
  Defailts are in section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Databuffers">3.2.4</a>
.
<li>
The count was a 4-byte integer in MPI standard
  up to and including~\mpistandard{3}.
  In the \mpistandard{4} standard the 
<tt>MPI_Count</tt>
 data type
  become allowed. See section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-data.html#Bigdatatypes">6.4</a>
 for details.
<li>
Datatypes can be elementary, as in the above example, or user-defined.
  See chapter~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-data.html">MPI topic: Data types</a>
 for details.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  Routines with both a send and receive buffer should not alias these.
  Instead, see the discussion of 
<tt>MPI_IN_PLACE</tt>
;
  section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reduceinplace">3.3.2</a>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<h4><a id="Examplesandexercises">3.2.1.2</a> Examples and exercises</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Reducetoall">Reduce to all</a> > <a href="mpi-collective.html#Examplesandexercises">Examples and exercises</a>
</p>
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Let each process compute a random number,
  and compute the sum of these numbers using the 
<tt>MPI_Allreduce</tt>
  routine.
\[
 \xi = \sum_i x_i 
\]
</p>

<p name="switchToTextMode">
  Each process then scales its value
  by this sum.
\[
 x_i' \leftarrow x_i/ \xi 
\]
  Compute the sum of the scaled numbers
\[
 \xi' = \sum_i x_i' 
\]
  and check that it is~1.
<!-- skeleton start: randommax -->
<button id="runBtnrandommax">Compile and run randommax</button>
<div id="editorDivrandommax" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPrerandommax"></pre>
<script name="defSkeletonrandommax">
let examplerandommax = new Example(
    "runBtnrandommax", "editorDivrandommax", "outputPrerandommax", 
    "skeletons/randommax.c", "randommax.c",
    "mpicc randommax.c && mpiexec -n 4 ./a.out" );
examplerandommax.initialize();
</script>
<!-- skeleton end: randommax -->
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Implement a (very simple-minded) Fourier transform: if $f$ is a
  function on the interval $[0,1]$, then the $n$-th Fourier
  coefficient is
\[
 f_n\hat = \int_0^1 f(t)e^{-2\pi x}\,dx 
\]
  which we approximate by
\[
 f_n\hat = \sum_{i=0}^{N-1} f(ih)e^{-in\pi/N} 
\]
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Make one distributed array for the $e^{-inh}$ coefficients,
<li>
make one distributed array for the $f(ih)$ values
<li>
calculate a couple of coefficients
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  In the previous exercise you worked with a distributed array,
  computing a local quantity and combining that into a global
  quantity.
  Why is it not a good idea to gather the whole distributed array on a
  single processor, and do all the computation locally?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  The usual reduction operators are given as templated operators:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplallreduce" aria-expanded="false" aria-controls="mplallreduce">
        C++ Code: mplallreduce
      </button>
    </h5>
  </div>
  <div id="mplallreduce" class="collapse">
  <pre>
float
  xrank = static_cast&lt;float&gt;( comm_world.rank() ),
  xreduce;
// separate recv buffer
comm_world.allreduce(mpl::plus&lt;float&gt;(), xrank,xreduce);
// in place
comm_world.allreduce(mpl::plus&lt;float&gt;(), xrank);
</pre>
</div>
</div>
  Note the parentheses after the operator.
  Also note that the operator comes first, not last.
<!-- environment: mplimpl start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplimpl ]] -->
<remark>

<!-- TranslatingLineGenerator mplimpl ['mplimpl'] -->
<p name="switchToTextMode">
    The reduction operator has to be compatible with
     <tt>T(T,T)&gt;</tt> 
</remark>
<!-- environment: mplimpl end embedded generator -->
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

For more about operators, see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#MPIOperators">3.10</a>
.
</p>

<h3><a id="Innerproductasallreduce">3.2.2</a> Inner product as allreduce</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Innerproductasallreduce">Inner product as allreduce</a>
</p>

<p name="switchToTextMode">

One of the more common applications of the reduction operation
is the 
<i>inner product</i>
 computation. Typically, you have two vectors $x,y$
that have the same distribution, that is,
where all processes store equal parts of $x$ and&nbsp;$y$.
The computation is then
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
local_inprod = 0;
for (i=0; i&lt;localsize; i++)
  local_inprod += x[i]*y[i];
MPI_Allreduce( &local_inprod, &global_inprod, 1,MPI_DOUBLE ... )
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The 
<i>Gram-Schmidt</i>
 method is a simple way to orthogonalize
  two vectors:
\[
 u \leftarrow u- (u^tv)/(u^tu) 
\]
  Implement this, and check that the result is indeed orthogonal.
</p>

<p name="switchToTextMode">
  Suggestion: fill $v$ with the values $\sin 2nh\pi$ where $n=2\pi/N$,
  and $u$ with $\sin 2nh\pi + \sin 4nh\pi$. What does $u$ become after orthogonalization?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Reductionoperations">3.2.3</a> Reduction operations</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Reductionoperations">Reduction operations</a>
</p>

</p>

<p name="switchToTextMode">
Several 
<tt>MPI_Op</tt>
 values are pre-defined. For the list,
see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Pre-definedoperators">3.10.1</a>
.
</p>

<p name="switchToTextMode">
For use in reductions and scans it is possible to define your own operator.
</p>

<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Op_create( MPI_User_function *func, int commute, MPI_Op *op);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

For more details, see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#User-definedoperators">3.10.2</a>
.
</p>

<h3><a id="Databuffers">3.2.4</a> Data buffers</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduction">Reduction</a> > <a href="mpi-collective.html#Databuffers">Data buffers</a>
</p>

<p name="switchToTextMode">

Collectives are the first example you see of MPI routines that
involve transfer of user data. Here, and in every other case,
you see that the data description involves:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
A buffer. This can be a scalar or an array.
<li>
A datatype. This describes whether the buffer contains integers,
  single/double floating point numbers, or more complicated types, to
  be discussed later.
<li>
A count. This states how many of the elements of the given
  datatype are in the send buffer, or will be received into the receive buffer.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
These three together describe what MPI needs to send through the network.
</p>

<p name="switchToTextMode">
In the various languages such a buffer/count/datatype triplet is specified in
different ways.
</p>

<p name="switchToTextMode">
First of all, in&nbsp;
<i>C</i>
 the
<i>buffer</i>
<!-- index -->
is always an 
<i>opaque handle</i>
, that is,
a  <tt>void*</tt>  parameter to which you supply an address.
This means that an MPI call can take two forms.
</p>

<p name="switchToTextMode">
For scalars we need to use the ampersand operator to take the address:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
float x,y;
MPI_Allreduce( &x,&y,1,MPI_FLOAT, ... );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
But for arrays we use the fact that arrays and addresses are more-or-less
equivalent in:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
float xx[2],yy[2];
MPI_Allreduce( xx,yy,2,MPI_FLOAT, ... );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
You could 
<i>cast</i>
 the buffers and write:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Allreduce( (void*)&x,(void*)&y,1,MPI_FLOAT, ... );
MPI_Allreduce( (void*)xx,(void*)yy,2,MPI_FLOAT, ... );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
but that is not necessary. The compiler will not complain
if you leave out the cast.
</p>

<p name="switchToTextMode">
\begin{cppnote}
{Buffer treatment}
  Treatment of scalars in C++ is the same as in&nbsp;C.
  However, for arrays you have the choice between C-style arrays,
  and \lstinline+std::vector+ or  <tt>std::array</tt> .
  For the latter there are two ways of dealing with buffers:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
vector&lt;float&gt; xx(25);
MPI_Send( xx.data(),25,MPI_FLOAT, .... );
MPI_Send( &xx[0],25,MPI_FLOAT, .... );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
\end{cppnote}
</p>

<!-- environment: fortrannote start embedded generator -->
<!-- environment block purpose: [[ environment=fortrannote ]] -->
<remark>
<b>Fortran note</b>
<p name="remark">
<!-- TranslatingLineGenerator fortrannote ['fortrannote'] -->
  In&nbsp;
<i>Fortran</i>
 parameters are always passed by reference,
  so the 
<i>buffer</i>

<!-- index -->
  is treated the same way:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
Real*4 :: x
Real*4,dimension(2) :: xx
call MPI_Allreduce( x,1,MPI_REAL4, ... )
call MPI_Allreduce( xx,2,MPI_REAL4, ... )
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</remark>
<!-- environment: fortrannote end embedded generator -->
<p name="switchToTextMode">
{MPI send/recv buffers}
</p>

<p name="switchToTextMode">
In discussing 
<span title="acronym" ><i>OO</i></span>
 languages, we first note that
the official C++ 
<span title="acronym" ><i>API</i></span>
 has been removed from the standard.
</p>

<p name="switchToTextMode">
Specification of the buffer/count/datatype triplet is not
needed explicitly in 
<span title="acronym" ><i>OO</i></span>
 languages.
</p>

<!-- environment: pythonnote start embedded generator -->
<!-- environment block purpose: [[ environment=pythonnote ]] -->
<remark>
<b>Python note</b>
<!-- TranslatingLineGenerator pythonnote ['pythonnote'] -->
<p name="switchToTextMode">
  Most MPI routines in Python have both a variant that can send arbitrary Python data,
  and one that is based on 
<i>numpy</i>
 arrays.
  The former looks the most `pythonic', and is very flexible,
  but is usually demonstrably inefficient.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allreducep" aria-expanded="false" aria-controls="allreducep">
        Python Code: allreducep
      </button>
    </h5>
  </div>
  <div id="allreducep" class="collapse">
  <pre>
## allreduce.py
random_number = random.randint(1,random_bound)
# native mode send
max_random = comm.allreduce(random_number,op=MPI.MAX)
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
  In the numpy variant, all 
<i>buffers</i>

<!-- index -->
  are 
<i>numpy</i>
 objects, which carry information about their type and size.
  For scalar reductions this means we have to create an array for the receive buffer,
  even though only one element is used.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allreducenump" aria-expanded="false" aria-controls="allreducenump">
        Python Code: allreducenump
      </button>
    </h5>
  </div>
  <div id="allreducenump" class="collapse">
  <pre>
myrandom = np.empty(1,dtype=np.int)
myrandom[0] = random_number
allrandom = np.empty(nprocs,dtype=np.int)
# numpy mode send
comm.Allreduce(myrandom,allrandom[:1],op=MPI.MAX)
</pre>
</div>
</div>
</remark>
<!-- environment: pythonnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  
<i>Buffer</i>

<!-- index -->
 type handling
  is done through polymorphism and templating: no explicit indiation of types.
</p>

<p name="switchToTextMode">
  Scalars are handled as such:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
float x,y;
comm.bcast( 0,x ); // note: root first
comm.allreduce( mpl::plus&lt;float&gt;(), x,y );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
where the reduction function needs to be compatible with the type of the buffer.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->

If your buffer is a  <tt>std::vector</tt>  you need
to take the  <tt>.data()</tt>  component of it:
<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
vector&lt;float&gt; xx(2),yy(2);
comm.allreduce( mpl::plus&lt;float&gt;(),
    xx.data(), yy.data(), mpl::contiguous_layout&lt;float&gt;(2) );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
  The 
<tt>contiguous_layout</tt>
this will be discussed in more detail elsewhere
(see note&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-data.html#Basiccalls">6.3.1</a>
 and later).
For now, interpret it as a way of indicating the count/type
part of a buffer specification.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  MPL point-to-point routines have a way of specifying the buffer(s)
  through a 
<tt>begin</tt>
 and 
<tt>end</tt>
 iterator.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplsendrange" aria-expanded="false" aria-controls="mplsendrange">
        C++ Code: mplsendrange
      </button>
    </h5>
  </div>
  <div id="mplsendrange" class="collapse">
  <pre>
// sendrange.cxx
vector&lt;double&gt; v(15);
  comm_world.send(v.begin(), v.end(), 1);  // send to rank 1
  comm_world.recv(v.begin(), v.end(), 0);  // receive from rank 0
</pre>
</div>
</div>
  Not available for collectives.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Rootedcollectives:broadcast,reduce">3.3</a> Rooted collectives: broadcast, reduce</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:broadcast,reduce">Rooted collectives: broadcast, reduce</a>
</p>

</p>

<p name="switchToTextMode">
In some scenarios there is a certain process that has a privileged status.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
  One process can generate or read in the initial data for a program
  run. This then needs to be communicated to all other processes.
<li>
  At the end of a program run, often
  one process needs to output some summary information.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
This process is called the 
<i>root</i>
 process, and we will now
consider routines that have a root.
</p>

<h3><a id="Reducetoaroot">3.3.1</a> Reduce to a root</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:broadcast,reduce">Rooted collectives: broadcast, reduce</a> > <a href="mpi-collective.html#Reducetoaroot">Reduce to a root</a>
</p>

<p name="switchToTextMode">

In the broadcast operation a single data item was communicated to all
processes. A&nbsp;reduction operation with
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Reduce" aria-expanded="false" aria-controls="MPI_Reduce">
        Routine reference: MPI_Reduce
      </button>
    </h5>
  </div>
  <div id="MPI_Reduce" class="collapse">
  <pre>
C:
int MPI_Reduce(
    const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype,
    MPI_Op op, int root, MPI_Comm comm)

Fortran:
MPI_Reduce(sendbuf, recvbuf, count, datatype, op, root, comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: count, root
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Op), INTENT(IN) :: op
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
native:
comm.reduce(self, sendobj=None, recvobj=None, op=SUM, int root=0)
numpy:
comm.Reduce(self, sendbuf, recvbuf, Op op=SUM, int root=0)
</pre>
</div>
</div>
<i>MPI_Reduce</i>
goes the other way: each process has a
data item, and these are all brought together into a single item.
</p>

<p name="switchToTextMode">
Here are the essential elements of a reduction operation:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Reduce( senddata, recvdata..., operator,
    root, comm );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<!-- environment: itemize start embedded generator -->
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
There is the original data, and the data resulting from the
  reduction. It is a design decision of MPI that it will not by
  default overwrite the original data. The send data and receive data
  are of the same size and type: if every processor has one real
  number, the reduced result is again one real number.
<li>
It is possible to indicate explicitly that a single buffer
  is used, and thereby the original data overwritten;
  see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reduceinplace">3.3.2</a>
 for this `in place' mode.
<li>
There is a reduction operator. Popular choices are
<tt>MPI_SUM</tt>
, 
<tt>MPI_PROD</tt>
 and
<tt>MPI_MAX</tt>
, but complicated operators such as finding
  the location of the maximum value exist.
  (For the full list, see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Pre-definedoperators">3.10.1</a>
.)
  You can also define your
  own operators; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#User-definedoperators">3.10.2</a>
.
<li>
There is a root process that receives the result of the
  reduction. Since the nonroot processes do not receive the reduced
  data, they can actually leave the receive buffer undefined.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reduce" aria-expanded="false" aria-controls="reduce">
        C Code: reduce
      </button>
    </h5>
  </div>
  <div id="reduce" class="collapse">
  <pre>
// reduce.c
float myrandom = (float) rand()/(float)RAND_MAX,
  result;
int target_proc = nprocs-1;
// add all the random variables together
MPI_Reduce(&myrandom,&result,1,MPI_FLOAT,MPI_SUM,
           target_proc,comm);
// the result should be approx nprocs/2:
if (procno==target_proc)
  printf("Result %6.3f compared to nprocs/2=%5.2f\n",
         result,nprocs/2.);
</pre>
</div>
</div>
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Write a program where each process computes a random number, and process&nbsp;0
  finds and prints the maximum generated value. Let each process print its value,
  just to check the correctness of your program.
<!-- environment: book start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=book ]] -->
<book>

<p name="book">
<!-- TranslatingLineGenerator book ['book'] -->
  (See&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/random.html">Random number generation</a>
 for a discussion of random number generation.)
</p name="book">
</book>
<!-- environment: book end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

Collective operations can also take an array argument, instead of just a scalar.
In that case, the operation is applied pointwise to each location in the array.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Create on each process an array of length&nbsp;2 integers, and put the
  values $1,2$ in it on each process. Do a sum reduction on that
  array. Can you predict what the result should be?  Code it. Was your
  prediction right?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Reduceinplace">3.3.2</a> Reduce in place</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:broadcast,reduce">Rooted collectives: broadcast, reduce</a> > <a href="mpi-collective.html#Reduceinplace">Reduce in place</a>
</p>

</p>

<p name="switchToTextMode">
By default MPI will not overwrite the original data with the reduction
result, but you can tell it to do so
using the 
<tt>MPI_IN_PLACE</tt>
 specifier:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allreduceinplace" aria-expanded="false" aria-controls="allreduceinplace">
        C Code: allreduceinplace
      </button>
    </h5>
  </div>
  <div id="allreduceinplace" class="collapse">
  <pre>
// allreduceinplace.c
for (int irand=0; irand&lt;nrandoms; irand++)
	myrandoms[irand] = (float) rand()/(float)RAND_MAX;
// add all the random variables together
MPI_Allreduce(MPI_IN_PLACE,myrandoms,
		    nrandoms,MPI_FLOAT,MPI_SUM,comm);
</pre>
</div>
</div>
Now every process only has a receive buffer, so this
has the advantage of saving half the memory.
Each process puts its input values in the receive buffer,
and these are overwritten by the reduced result.
</p>

<p name="switchToTextMode">
The above example used 
<tt>MPI_IN_PLACE</tt>
 in
<tt>MPI_Allreduce</tt>
;
in 
<tt>MPI_Reduce</tt>
 it's little  tricky.
The reasoning is a follows:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
In 
<tt>MPI_Reduce</tt>
 every process has a buffer to
  contribute, but only the root needs a receive buffer. Therefore,
<tt>MPI_IN_PLACE</tt>
 takes the place of the receive buffer
  on any processor except for the root&nbsp;&hellip;
<li>
&hellip;
&nbsp;while the root, which needs a receive buffer,
  has 
<tt>MPI_IN_PLACE</tt>
 takes the place of the send buffer.
  In order to contribute its value, the root needs to put this in the
  receive buffer.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Here is one way you could write the in-place version of 
<tt>MPI_Reduce</tt>
:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#onereduceinplace" aria-expanded="false" aria-controls="onereduceinplace">
        C Code: onereduceinplace
      </button>
    </h5>
  </div>
  <div id="onereduceinplace" class="collapse">
  <pre>
if (procno==root)
  MPI_Reduce(MPI_IN_PLACE,myrandoms,
		   nrandoms,MPI_FLOAT,MPI_SUM,root,comm);
else
  MPI_Reduce(myrandoms,MPI_IN_PLACE,
		   nrandoms,MPI_FLOAT,MPI_SUM,root,comm);
</pre>
</div>
</div>
However, as a point of style, having different versions of a a collective
in different branches of a condition is infelicitous. The following may be preferable:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#tworeduceinplace" aria-expanded="false" aria-controls="tworeduceinplace">
        C Code: tworeduceinplace
      </button>
    </h5>
  </div>
  <div id="tworeduceinplace" class="collapse">
  <pre>
float *sendbuf,*recvbuf;
if (procno==root) {
  sendbuf = MPI_IN_PLACE; recvbuf = myrandoms;
} else {
  sendbuf = myrandoms; recvbuf = MPI_IN_PLACE;
}
MPI_Reduce(sendbuf,recvbuf,
  	 nrandoms,MPI_FLOAT,MPI_SUM,root,comm);
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
In Fortran  you can not do these address calculations.
You can use the solution with a conditional:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reduceinplace-f" aria-expanded="false" aria-controls="reduceinplace-f">
        Fortran Code: reduceinplace-f
      </button>
    </h5>
  </div>
  <div id="reduceinplace-f" class="collapse">
  <pre>
!! reduceinplace.F90
  call random_number(mynumber)
  target_proc = ntids-1;
  ! add all the random variables together
  if (mytid.eq.target_proc) then
     result = mytid
     call MPI_Reduce(MPI_IN_PLACE,result,1,MPI_REAL,MPI_SUM,&
          target_proc,comm,err)
  else
     mynumber = mytid
     call MPI_Reduce(mynumber,result,1,MPI_REAL,MPI_SUM,&
          target_proc,comm,err)
  end if
</pre>
</div>
</div>
but you can also solve this with pointers:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reduceinplace-fptr" aria-expanded="false" aria-controls="reduceinplace-fptr">
        Fortran Code: reduceinplace-fptr
      </button>
    </h5>
  </div>
  <div id="reduceinplace-fptr" class="collapse">
  <pre>
!! reduceinplaceptr.F90
  real,target :: mynumber,result,in_place_val
  real,pointer :: mynumber_ptr,result_ptr
  in_place_val = MPI_IN_PLACE
  if (mytid.eq.target_proc) then
     result_ptr =&gt; result
     mynumber_ptr =&gt; in_place_val
     result_ptr = mytid
  else
     mynumber_ptr =&gt; mynumber
     result_ptr = in_place_val
     mynumber_ptr = mytid
  end if
  call MPI_Reduce(mynumber_ptr,result_ptr,1,MPI_REAL,MPI_SUM,&
       target_proc,comm,err)
</pre>
</div>
</div>
</p>

<!-- environment: pythonnote start embedded generator -->
<!-- environment block purpose: [[ environment=pythonnote ]] -->
<remark>
<b>Python note</b>
<!-- TranslatingLineGenerator pythonnote ['pythonnote'] -->
  The value  <tt>MPI.IN_PLACE</tt>  can be used for the send buffer:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allreduceip" aria-expanded="false" aria-controls="allreduceip">
        Python Code: allreduceip
      </button>
    </h5>
  </div>
  <div id="allreduceip" class="collapse">
  <pre>
## allreduceinplace.py
myrandom = np.empty(1,dtype=np.int)
myrandom[0] = random_number

comm.Allreduce(MPI.IN_PLACE,myrandom,op=MPI.MAX)
</pre>
</div>
</div>
</remark>
<!-- environment: pythonnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  The in-place variant is activated by
  specifying only one instead of two buffer arguments.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplallreduce" aria-expanded="false" aria-controls="mplallreduce">
        C++ Code: mplallreduce
      </button>
    </h5>
  </div>
  <div id="mplallreduce" class="collapse">
  <pre>
float
  xrank = static_cast&lt;float&gt;( comm_world.rank() ),
  xreduce;
// separate recv buffer
comm_world.allreduce(mpl::plus&lt;float&gt;(), xrank,xreduce);
// in place
comm_world.allreduce(mpl::plus&lt;float&gt;(), xrank);
</pre>
</div>
</div>
  Reducing a buffer requires specification of a  <tt>contiguous_layout</tt> :
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplallreducebuffer" aria-expanded="false" aria-controls="mplallreducebuffer">
        C++ Code: mplallreducebuffer
      </button>
    </h5>
  </div>
  <div id="mplallreducebuffer" class="collapse">
  <pre>
// collectbuffer.cxx
float
  xrank = static_cast&lt;float&gt;( comm_world.rank() );
vector&lt;float&gt; rank2p2p1{ 2*xrank,2*xrank+1 },reduce2p2p1{0,0};
mpl::contiguous_layout&lt;float&gt; two_floats(rank2p2p1.size());
comm_world.allreduce
  (mpl::plus&lt;float&gt;(), rank2p2p1.data(),reduce2p2p1.data(),two_floats);
if ( iprint )
  cout &lt;&lt; "Got: " &lt;&lt; reduce2p2p1.at(0) &lt;&lt; ","
	 &lt;&lt; reduce2p2p1.at(1) &lt;&lt; endl;
</pre>
</div>
</div>
  Note that the buffers are of type  <tt>T *</tt> , so it is necessary
  to take the \lstinline+data()+ of any  <tt>std::vector</tt>  and such.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<!-- environment: mplnote start embedded generator -->
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  There is a separate variant for non-root usage of rooted collectives:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplreduceroot" aria-expanded="false" aria-controls="mplreduceroot">
        C++ Code: mplreduceroot
      </button>
    </h5>
  </div>
  <div id="mplreduceroot" class="collapse">
  <pre>
// scangather.cxx
if (procno==0) {
  comm_world.reduce
    ( mpl::plus&lt;int&gt;(),0,my_number_of_elements,total_number_of_elements );
} else {
  comm_world.reduce
    ( mpl::plus&lt;int&gt;(),0,my_number_of_elements );
}
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Broadcast">3.3.3</a> Broadcast</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:broadcast,reduce">Rooted collectives: broadcast, reduce</a> > <a href="mpi-collective.html#Broadcast">Broadcast</a>
</p>

</p>

<p name="switchToTextMode">
A broadcast models the scenario where one process,
the `root' process,
owns some data, and it communicates it to all other processes.
</p>

<p name="switchToTextMode">
The broadcast routine
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Bcast" aria-expanded="false" aria-controls="MPI_Bcast">
        Routine reference: MPI_Bcast
      </button>
    </h5>
  </div>
  <div id="MPI_Bcast" class="collapse">
  <pre>
C:
int MPI_Bcast(
    void* buffer, int count, MPI_Datatype datatype,
    int root, MPI_Comm comm)

Fortran:
MPI_Bcast(buffer, count, datatype, root, comm, ierror)
TYPE(*), DIMENSION(..) :: buffer
INTEGER, INTENT(IN) :: count, root
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python native:
rbuf = MPI.Comm.bcast(self, obj=None, int root=0)
Python numpy:
MPI.Comm.Bcast(self, buf, int root=0)

MPL:
template<typename T >
void mpl::communicator::bcast
   ( int  root, T &  data ) const
   ( int  root, T *  data, const layout< T > &  l ) const
</pre>
</div>
</div>
<i>MPI_Bcast</i>
has the following structure:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Bcast( data..., root , comm);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Here:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
There is only one buffer, the send buffer.
  Before the call, the root process has data in this buffer;
  the other processes allocate a same sized buffer, but
  for them the contents are irrelevant.
<li>
  The root is the process that is sending its data.
  Typically, it will be the root of a broadcast tree.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Example: in general we can not assume that all processes get the
commandline arguments, so we broadcast them from process&nbsp;0.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#usage" aria-expanded="false" aria-controls="usage">
        C Code: usage
      </button>
    </h5>
  </div>
  <div id="usage" class="collapse">
  <pre>
// init.c
if (procno==0) {
  if ( argc==1 || // the program is called without parameter
       ( argc&gt;1 && !strcmp(argv[1],"-h") ) // user asked for help
       ) {
    printf("\nUsage: init [0-9]+\n");
    MPI_Abort(comm,1);
  }
  input_argument = atoi(argv[1]);
}
MPI_Bcast(&input_argument,1,MPI_INT,0,comm);
</pre>
</div>
</div>
<p name="switchToTextMode">

<!-- environment: comment start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=comment ]] -->
<comment>


</comment>
<!-- environment: comment end embedded generator -->
<p name="switchToTextMode">

<!-- environment: pythonnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=pythonnote ]] -->
<remark>
<b>Python note</b>
<!-- TranslatingLineGenerator pythonnote ['pythonnote'] -->
<p name="switchToTextMode">
  In python it is both possible to send objects, and to send more
  C-like buffers. The two possibilities correspond (see
  section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-started.html#Python">1.5.4</a>
) to different routine names; the
  buffers have to be created as 
<tt>numpy</tt>
 objects.
</p>

<p name="switchToTextMode">
  We illustrate both the general Python and numpy variants.
  In the former variant the result is given as a function return;
  in the numpy variant the send buffer is reused.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#bcastp" aria-expanded="false" aria-controls="bcastp">
        Python Code: bcastp
      </button>
    </h5>
  </div>
  <div id="bcastp" class="collapse">
  <pre>
## bcast.py
# first native
if procid==root:
    buffer = [ 5.0 ] * dsize
else:
    buffer = [ 0.0 ] * dsize
buffer = comm.bcast(obj=buffer,root=root)
if not reduce( lambda x,y:x and y,
               [ buffer[i]==5.0 for i in  range(len(buffer)) ] ):
    print( "Something wrong on proc %d: native buffer &lt;&lt;%s&gt;&gt;" \
           % (procid,str(buffer)) )

# then with NumPy
buffer = np.arange(dsize, dtype=np.float64)
if procid==root:
    for i in range(dsize):
        buffer[i] = 5.0
comm.Bcast( buffer,root=root )
if not all( buffer==5.0 ):
    print( "Something wrong on proc %d: numpy buffer &lt;&lt;%s&gt;&gt;" \
           % (procid,str(buffer)) )
else:
    if procid==root:
        print("Success.")
</pre>
</div>
</div>
</remark>
<!-- environment: pythonnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  The broadcast call comes in two variants, with scalar argument
  and general layout:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
 template&lt;typename T &gt;
void mpl::communicator::bcast
   ( int root_rank, T &data ) const;
void mpl::communicator::bcast
   ( int root_rank, T *data, const layout&lt; T &gt; &l ) const;
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
  Note that the root argument comes first.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">
For the following exercise, study figure&nbsp;
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The 
<i>Gauss-Jordan algorithm</i>
 for solving a linear system
  with a matrix&nbsp;$A$ (or computing its inverse) runs as follows:
<!-- environment: tabbing start embedded generator -->
</p>
<p style="margin-left: 40px">
<!-- TranslatingLineGenerator tabbing ['tabbing'] --><br>
<p name="switchToTextMode"><br>
    for pivot $k=1,&hellip;,n$<br>
    \&gt;let the vector of scalings $\ell^{(k)}_i=A_{ik}/A_{kk}$<br>
    \&gt;for row $r\not=k$<br>
    \&gt;\&gt;for column $c=1,&hellip;,n$<br>
    \&gt;\&gt;\&gt; $A_{rc}\leftarrow A_{rc} - \ell^{(k)}_r A_{kc}$<br>
</p>
<!-- environment: tabbing end embedded generator -->
<p name="switchToTextMode">
  where we ignore the update of the righthand side, or the formation
  of the inverse.
</p>

<p name="switchToTextMode">
  Let a matrix be distributed with each process storing one
  column. Implement the Gauss-Jordan algorithm as a series of
  broadcasts: in iteration $k$ process $k$ computes and broadcasts the
  scaling vector&nbsp;$\{\ell^{(k)}_i\}_i$. Replicate the right-hand side on
  all processors.
<!-- skeleton start: jordan -->
<button id="runBtnjordan">Compile and run jordan</button>
<div id="editorDivjordan" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPrejordan"></pre>
<script name="defSkeletonjordan">
let examplejordan = new Example(
    "runBtnjordan", "editorDivjordan", "outputPrejordan", 
    "skeletons/jordan.c", "jordan.c",
    "mpicc jordan.c && mpiexec -n 4 ./a.out" );
examplejordan.initialize();
</script>
<!-- skeleton end: jordan -->
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Add partial pivoting to your implementation of Gauss-Jordan elimination.
</p>

<p name="switchToTextMode">
  Change your implementation to let each processor store multiple columns,
  but still do one broadcast per column. Is there a way to have only one
  broadcast per processor?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- TranslatingLineGenerator file ['file'] -->
</p>

<h2><a id="Scanoperations">3.4</a> Scan operations</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Scanoperations">Scan operations</a>
</p>

<p name="switchToTextMode">

The 
<tt>MPI_Scan</tt>
 operation also performs a reduction, but it keeps
the partial results. That is, if processor~$i$ contains a number~$x_i$,
and $\oplus$ is an operator,
then the scan operation leaves $x_0\oplus\cdots\oplus x_i$ on processor~$i$.
This type of operation is often called a 
<i>prefix operation</i>
;
see 
<i>Eijkhout:IntroHPC</i>
.
</p>

<p name="switchToTextMode">
The 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Scan" aria-expanded="false" aria-controls="MPI_Scan">
        Routine reference: MPI_Scan
      </button>
    </h5>
  </div>
  <div id="MPI_Scan" class="collapse">
  <pre>
C:
int MPI_Scan(const void* sendbuf, void* recvbuf,
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)
IN sendbuf: starting address of send buffer (choice)
OUT recvbuf: starting address of receive buffer (choice)
IN count: number of elements in input buffer (non-negative integer)
IN datatype: data type of elements of input buffer (handle)
IN op: operation (handle)
IN comm: communicator (handle)

Fortran:
MPI_Scan(sendbuf, recvbuf, count, datatype, op, comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: count
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Op), INTENT(IN) :: op
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
res = Intracomm.scan( sendobj=None,recvobj=None,op=MPI.SUM)
res = Intracomm.exscan( sendobj=None,recvobj=None,op=MPI.SUM)
</pre>
</div>
</div>
<i>MPI_Scan</i>
 routine is an 
<i>inclusive scan</i>
 operation,
meaning that it includes the data on the process itself;
<tt>MPI_Exscan</tt>
 (see section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Exclusivescan">3.4.1</a>
)
is exclusive, and does not include the data on the calling process.
</p>

<p name="switchToTextMode">
\[
\begin{array}{rccccc}
  \mathrm{process:}
      &0&1&2&\cdots&p-1\\
  \mathrm{data:}
      &x_0&x_1&x_2&\cdots&x_{p-1}\\
  \mathrm{inclusive:}\mathstrut
      &x_0&x_0\oplus x_1&x_0\oplus x_1\oplus x_2&\cdots&\mathop\oplus_{i=0}^{p-1} x_i\\
  \mathrm{exclusive:}\mathstrut
      &\mathrm{unchanged}&x_0&x_0\oplus x_1&\cdots&\mathop\oplus_{i=0}^{p-2} x_i\\
\end{array}
\]
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#iscan" aria-expanded="false" aria-controls="iscan">
        C Code: iscan
      </button>
    </h5>
  </div>
  <div id="iscan" class="collapse">
  <pre>
// scan.c
// add all the random variables together
MPI_Scan(&myrandom,&result,1,MPI_FLOAT,MPI_SUM,comm);
// the result should be approaching nprocs/2:
if (procno==nprocs-1)
  printf("Result %6.3f compared to nprocs/2=%5.2f\n",
         result,nprocs/2.);
</pre>
</div>
</div>
<p name="switchToTextMode">

In python mode the result is a function return value,
with numpy the result is passed as the second parameter.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#scanp" aria-expanded="false" aria-controls="scanp">
        Python Code: scanp
      </button>
    </h5>
  </div>
  <div id="scanp" class="collapse">
  <pre>
## scan.py
mycontrib = 10+random.randint(1,nprocs)
myfirst = 0
mypartial = comm.scan(mycontrib)
sbuf = np.empty(1,dtype=np.int)
rbuf = np.empty(1,dtype=np.int)
sbuf[0] = mycontrib
comm.Scan(sbuf,rbuf)
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
You can use any of the given reduction operators,
  (for the list, see section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Pre-definedoperators">3.10.1</a>
),
or a user-defined one. In the latter case,
the 
<tt>MPI_Op</tt>
 operations do not return an error code.
</p>

<!-- environment: mplnote start embedded generator -->
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  As in the C/F interfaces, 
<span title="acronym" ><i>MPL</i></span>
 interfaces to the scan routines
  have the same calling sequences as the `Allreduce' routine.
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Exclusivescan">3.4.1</a> Exclusive scan</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Scanoperations">Scan operations</a> > <a href="mpi-collective.html#Exclusivescan">Exclusive scan</a>
</p>

</p>

<p name="switchToTextMode">
Often, the more useful variant is the 
<i>exclusive scan</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Exscan" aria-expanded="false" aria-controls="MPI_Exscan">
        Routine reference: MPI_Exscan
      </button>
    </h5>
  </div>
  <div id="MPI_Exscan" class="collapse">
  <pre>
C:
int MPI_Exscan(const void *sendbuf, void *recvbuf, int count,
    MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)
int MPI_Iexscan(const void *sendbuf, void *recvbuf, int count,
    MPI_Datatype datatype, MPI_Op op, MPI_Comm comm,
    MPI_Request *request)

Fortran:

MPI_EXSCAN(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, IERROR)
    <type>    SENDBUF(*), RECVBUF(*)
    INTEGER    COUNT, DATATYPE, OP, COMM, IERROR
MPI_IEXSCAN(SENDBUF, RECVBUF, COUNT, DATATYPE, OP, COMM, REQUEST, IERROR)
    <type>    SENDBUF(*), RECVBUF(*)
    INTEGER    COUNT, DATATYPE, OP, COMM, REQUEST, IERROR

Input Parameters

sendbuf: Send buffer (choice).
count: Number of elements in input buffer (integer).
datatype: Data type of elements of input buffer (handle).
op: Operation (handle).
comm: Communicator (handle).

Output Parameters

recvbuf: Receive buffer (choice).
request: Request (handle, non-blocking only).
</pre>
</div>
</div>
<i>MPI_Exscan</i>
with the same signature.
</p>

<p name="switchToTextMode">
The result of the exclusive scan is undefined on processor~0
(
<tt>None</tt>
 in python),
and on processor~1 it is a copy of the send value of processor~1.
In particular, the 
<tt>MPI_Op</tt>
 need not be called on these two
processors.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The exclusive definition, which computes $x_0\oplus x_{i-1}$ on
  processor~$i$, can be derived from the inclusive operation
  for operations such as 
<tt>MPI_SUM</tt>
 or
<tt>MPI_PROD</tt>
.  Are there operators where that is not the
  case?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Useofscanoperations">3.4.2</a> Use of scan operations</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Scanoperations">Scan operations</a> > <a href="mpi-collective.html#Useofscanoperations">Use of scan operations</a>
</p>
</p>

<p name="switchToTextMode">
The 
<tt>MPI_Scan</tt>
 operation is often useful with indexing data. Suppose that
every processor&nbsp;$p$ has a local vector where the number of elements&nbsp;$n_p$ is dynamically
determined. In order to translate the local numbering $0&hellip; n_p-1$ to a global numbering
one does a scan with the number of local elements as input. The output is then the global
number of the first local variable.
</p>

<p name="switchToTextMode">
As an example, setting 
<i>FFT</i>
 coefficients requires this translation.
If the local sizes are all equal, determining the global index of the
first element is an easy calculation.
For the irregular case, we first do a scan:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#fftsetcoeff" aria-expanded="false" aria-controls="fftsetcoeff">
        C Code: fftsetcoeff
      </button>
    </h5>
  </div>
  <div id="fftsetcoeff" class="collapse">
  <pre>
// fft.c
MPI_Allreduce( &localsize,&globalsize,1,MPI_INT,MPI_SUM, comm );
globalsize += 1;
int myfirst=0;
MPI_Exscan( &localsize,&myfirst,1,MPI_INT,MPI_SUM, comm );

for (int i=0; i&lt;localsize; i++)
  vector[i] = sin( pi*freq* (i+1+myfirst) / globalsize );
</pre>
</div>
</div>
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/scanints.png" width=800></img>
<p name="caption">
FIGURE 3.2: Local arrays that together form a consecutive range
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Let each process compute a random value $n_{\scriptstyle\mathrm{local}}$,
    and allocate an array of that length.
    Define 
\[
 N=\sum n_{\scriptstyle\mathrm{local}} 
\]
<li>
Fill the array with consecutive integers, so that all local arrays,
    laid end-to-end,
    contain the numbers&nbsp;$0\cdots N-1$. (See figure&nbsp;
3.2
.)
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<!-- skeleton start: scangather -->
<button id="runBtnscangather">Compile and run scangather</button>
<div id="editorDivscangather" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPrescangather"></pre>
<script name="defSkeletonscangather">
let examplescangather = new Example(
    "runBtnscangather", "editorDivscangather", "outputPrescangather", 
    "skeletons/scangather.c", "scangather.c",
    "mpicc scangather.c && mpiexec -n 4 ./a.out" );
examplescangather.initialize();
</script>
<!-- skeleton end: scangather -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Did you use 
<tt>MPI_Scan</tt>
 or 
<tt>MPI_Exscan</tt>
 for
  the previous exercise?
  How would you describe the result of the other scan operation, given the
  same input?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

</p>

<p name="switchToTextMode">
It is possible to do a 
<i>segmented scan</i>
. Let $x_i$ be a series of numbers
that we want to sum to $X_i$ as follows. Let $y_i$ be a series of booleans such that
\[
\begin{cases}
  X_i=x_i&\hbox{if $y_i=0$}\\
  X_i=X_{i-1}+x_i&\hbox{if $y_i=1$}
\end{cases}
\]
(This is the basis for the implementation of the 
<i>sparse matrix vector product</i>
as prefix operation; see 
<i>Eijkhout:IntroHPC</i>
.)
This means that $X_i$ sums the segments between locations where $y_i=0$ and the
first subsequent place where $y_i=1$. To implement this, you need a user-defined operator
\[
\begin{pmatrix}
  X\\ x\\ y
\end{pmatrix}
=
\begin{pmatrix}
  X_1\\ x_1\\ y_1
\end{pmatrix}
\bigoplus
\begin{pmatrix}
  X_2\\ x_2\\ y_2
\end{pmatrix}
\colon
\begin{cases}
    X=x_1+x_2&\hbox{if $y_2==1$}\\ X=x_2&\hbox{if $y_2==0$}
\end{cases}
\]
This operator is not communitative, and it needs to be declared as such
with 
<tt>MPI_Op_create</tt>
; see section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#User-definedoperators">3.10.2</a>
</p>

<p name="switchToTextMode">

</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<h2><a id="Rootedcollectives:gatherandscatter">3.5</a> Rooted collectives: gather and scatter</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:gatherandscatter">Rooted collectives: gather and scatter</a>
</p>

</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/gather.png" width=800></img>
<p name="caption">
FIGURE 3.3: Gather collects all data onto a root
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

In the 
<tt>MPI_Scatter</tt>
 operation, the root spreads information to
all other processes. The difference with a broadcast is that it involves
individual information from/to every process. Thus, the gather operation typically
has an array of items, one coming from each sending process, and scatter has an array,
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/scatter-simple.jpeg" width=800></img>
<p name="caption">
FIGURE 3.4: A scatter operation
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
with an individual item for each receiving process; see figure~
3.4
.
</p>

<p name="switchToTextMode">
These gather and scatter collectives have a different parameter list from
the broadcast/reduce. The broadcast/reduce involves the same amount
of data on each process, so it was enough to have a single
datatype/size specification; for one buffer in the broadcast, and
for both buffers in the reduce call.
In the gather/scatter calls you have
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
a large buffer on the root, with a datatype and size specification, and
<li>
a smaller buffer on each process, with its own type and size specification.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

In the gather and scatter calls, each processor has $n$ elements of individual
data. There is also a root processor that has an array of length~$np$, where $p$
is the number of processors. The gather call collects all this data from the
processors to the root; the scatter call assumes that the information is
initially on the root and it is spread to the individual processors.
</p>

<p name="switchToTextMode">
Here is a small example:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#gather" aria-expanded="false" aria-controls="gather">
        C Code: gather
      </button>
    </h5>
  </div>
  <div id="gather" class="collapse">
  <pre>
// gather.c
// we assume that each process has a value "localsize"
// the root process collects these values

  if (procno==root)
    localsizes = (int*) malloc( nprocs*sizeof(int) );

// everyone contributes their info
  MPI_Gather(&localsize,1,MPI_INT,
             localsizes,1,MPI_INT,root,comm);
</pre>
</div>
</div>
This will also be the basis of a more elaborate example in
section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Variable-size-inputcollectives">3.9</a>
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Let each process compute a random number.
  You want to print the maximum value and on what processor it
  is computed. What collective(s) do you use? Write a short program.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The 
<tt>MPI_Scatter</tt>
 operation is in some sense the inverse of the gather:
the root process has an array of length $np$ where $p$&nbsp;is the number of processors
and $n$&nbsp;the number of elements each processor will receive.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
int MPI_Scatter
  (void* sendbuf, int sendcount, MPI_Datatype sendtype,
   void* recvbuf, int recvcount, MPI_Datatype recvtype,
   int root, MPI_Comm comm)
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

Two things to note about these routines:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The signature for 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Gather" aria-expanded="false" aria-controls="MPI_Gather">
        Routine reference: MPI_Gather
      </button>
    </h5>
  </div>
  <div id="MPI_Gather" class="collapse">
  <pre>
C:
int MPI_Gather(
  const void* sendbuf, int sendcount, MPI_Datatype sendtype,
  void* recvbuf, int recvcount, MPI_Datatype recvtype,
  int root, MPI_Comm comm)

Semantics:
IN sendbuf: starting address of send buffer (choice)
IN sendcount: number of elements in send buffer (non-negative integer)
IN sendtype: data type of send buffer elements (handle)
OUT recvbuf: address of receive buffer (choice, significant only at root)
IN recvcount: number of elements for any single receive (non-negative integer, significant only at root)
IN recvtype: data type of recv buffer elements (significant only at root) (handle)
IN root: rank of receiving process (integer)
IN comm: communicator (handle)

Fortran:
MPI_Gather
   (sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype,
    root, comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: sendcount, recvcount, root
TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
MPI.Comm.Gather
   (self, sendbuf, recvbuf, int root=0)
</pre>
</div>
</div>
<i>MPI_Gather</i>
 has two `count' parameters, one
  for the length of the individual send buffers, and one for the receive buffer.
  However, confusingly, the second parameter (which is only relevant on the root)
  does not indicate the total amount of information coming in, but
  rather the size of 
<i>each</i>
 contribution. Thus, the two count parameters
  will usually be the same (at least on the root); they can differ if you
  use different 
<tt>MPI_Datatype</tt>
 values for the sending and receiving
  processors.
<li>
While every process has a sendbuffer in the gather, and a receive buffer
  in the scatter call, only the root process needs the long array in which to gather,
  or from which to scatter.
  However, because in 
<span title="acronym" ><i>SPMD</i></span>
 mode all processes need to use the same routine,
  a&nbsp;parameter for this long array is always present. Nonroot processes can use
  a 
<i>null pointer</i>
 here.
<li>
More elegantly, the 
<tt>MPI_IN_PLACE</tt>
 option can be
  used for buffers that are not applicable, such as the receive buffer on a sending process.
  See section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Reduceinplace">3.3.2</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
  Gathering (by  <tt>communicator::</tt> 
<tt>gather</tt>
  or scattering (by  <tt>communicator::</tt> 
<tt>scatter</tt>
<p name="switchToTextMode">
  a single scalar takes a scalar argument
  and a raw array:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplscattergatherx" aria-expanded="false" aria-controls="mplscattergatherx">
        C++ Code: mplscattergatherx
      </button>
    </h5>
  </div>
  <div id="mplscattergatherx" class="collapse">
  <pre>
vector&lt;float&gt; v;
float x;
comm_world.scatter(0, v.data(), x);
</pre>
</div>
</div>
  If more than a single scalar is gathered, or scattered into,
  it becomes necessary to specify a layout:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplscattergatherv" aria-expanded="false" aria-controls="mplscattergatherv">
        C++ Code: mplscattergatherv
      </button>
    </h5>
  </div>
  <div id="mplscattergatherv" class="collapse">
  <pre>
vector&lt;float&gt; vrecv(2),vsend(2*nprocs);
mpl::contiguous_layout&lt;float&gt; twonums(2);
comm_world.scatter
  (0, vsend.data(),twonums, vrecv.data(),twonums );
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  Logically speaking, on every nonroot process,
  the gather call only has a send buffer.
  MPL supports this by having two variants that only specify the send data.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplgathernonroot" aria-expanded="false" aria-controls="mplgathernonroot">
        C++ Code: mplgathernonroot
      </button>
    </h5>
  </div>
  <div id="mplgathernonroot" class="collapse">
  <pre>
if (procno==0) {
  vector&lt;int&gt; size_buffer(nprocs);
  comm_world.gather
    (
     0,my_number_of_elements,size_buffer.data()
     );
} else {
  /*
   * If you are not the root, do versions with only send buffers
   */
  comm_world.gather
    ( 0,my_number_of_elements );
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Examples">3.5.1</a> Examples</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:gatherandscatter">Rooted collectives: gather and scatter</a> > <a href="mpi-collective.html#Examples">Examples</a>
</p>
</p>

<p name="switchToTextMode">
In some applications, each process computes a row or column of a
matrix, but for some calculation (such as the determinant) it is more
efficient to have the whole matrix on one process. You should of
course only do this if this matrix is essentially smaller than the
full problem, such as an interface system or the last coarsening level
in multigrid.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/allgathermatrix.png" width=800></img>
<p name="caption">
FIGURE 3.5: Gather a distributed matrix onto one process
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Figure&nbsp;
3.5
 pictures this. Note that conceptually
we are gathering a two-dimensional object, but the buffer is of course
one-dimensional. You will later see how this can be done more
elegantly with the `subarray' datatype; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-data.html#Subarraytype">6.3.4</a>
.
</p>

<p name="switchToTextMode">
Another thing you can do with a distributed matrix is to transpose it.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#transposescatterref" aria-expanded="false" aria-controls="transposescatterref">
        C Code: transposescatterref
      </button>
    </h5>
  </div>
  <div id="transposescatterref" class="collapse">
  <pre>
// itransposeblock.c
for (int iproc=0; iproc&lt;nprocs; iproc++) {
  MPI_Scatter( regular,1,MPI_DOUBLE,
		 &(transpose[iproc]),1,MPI_DOUBLE,
		 iproc,comm);
}
</pre>
</div>
</div>
In this example, each process scatters its column.
This needs to be done only once, yet the scatter happens in a loop.
The trick here is that a process only originates the scatter when it
is the root, which happens only once.
Why do we need a loop? That is because each element of a process' row
originates from a different scatter operation.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you rewrite this code so that it uses a gather rather than a scatter?
  Does that change anything essential about structure of the code?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Take the code from exercise&nbsp;
3.2
 and extend it to gather
  all local buffers onto rank zero.
  Since the local arrays are of differing lengths,
  this requires 
<tt>MPI_Gatherv</tt>
.
</p>

<p name="switchToTextMode">
  How do you construct the lengths and displacements arrays?
<!-- skeleton start: scangather -->
<button id="runBtnscangather">Compile and run scangather</button>
<div id="editorDivscangather" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPrescangather"></pre>
<script name="defSkeletonscangather">
let examplescangather = new Example(
    "runBtnscangather", "editorDivscangather", "outputPrescangather", 
    "skeletons/scangather.c", "scangather.c",
    "mpicc scangather.c && mpiexec -n 4 ./a.out" );
examplescangather.initialize();
</script>
<!-- skeleton end: scangather -->
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Allgather">3.5.2</a> Allgather</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Rootedcollectives:gatherandscatter">Rooted collectives: gather and scatter</a> > <a href="mpi-collective.html#Allgather">Allgather</a>
</p>
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/allgather.png" width=800></img>
<p name="caption">
FIGURE 3.6: All gather collects all data onto every process
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Allgather" aria-expanded="false" aria-controls="MPI_Allgather">
        Routine reference: MPI_Allgather
      </button>
    </h5>
  </div>
  <div id="MPI_Allgather" class="collapse">
  <pre>
C:
int MPI_Allgather(const void *sendbuf, int  sendcount,
     MPI_Datatype sendtype, void *recvbuf, int recvcount,
     MPI_Datatype recvtype, MPI_Comm comm)
int MPI_Iallgather(const void *sendbuf, int  sendcount,
     MPI_Datatype sendtype, void *recvbuf, int recvcount,
     MPI_Datatype recvtype, MPI_Comm comm, MPI_Request *request)

Fortran:
MPI_ALLGATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT,
        RECVTYPE, COMM, IERROR)
    <type>    SENDBUF (*), RECVBUF (*)
    INTEGER    SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM,
    INTEGER    IERROR
MPI_IALLGATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT,
        RECVTYPE, COMM, REQUEST, IERROR)
    <type>    SENDBUF(*), RECVBUF (*)
    INTEGER    SENDCOUNT, SENDTYPE, RECVCOUNT, RECVTYPE, COMM
    INTEGER    REQUEST, IERROR
C++ Syntax

Parameters:
sendbuf : Starting address of send buffer (choice).
sendcount: Number of elements in send buffer (integer).
sendtype: Datatype of send buffer elements (handle).
recvbuf: Starting address of recv buffer (choice).
recvcount: Number of elements received from any process (integer).
recvtype: Datatype of receive buffer elements (handle).
comm; Communicator (handle).

recvbuf: Address of receive buffer (choice).
request: Request (handle, non-blocking only).
</pre>
</div>
</div>
<i>MPI_Allgather</i>
 routine does the same gather onto
every process: each process winds up with the totality of all data;
figure&nbsp;
3.6
.
</p>

<p name="switchToTextMode">
This routine can be used in the simplest implementation of the
<i>dense matrix-vector product</i>
 to give each processor the
full input; see&nbsp;
<i>Eijkhout:IntroHPC</i>
.
</p>

<p name="switchToTextMode">
Some cases look like an all-gather but can be implemented more
efficiently. Suppose you have two distributed vectors, and you want to
create a new vector that contains those elements of the one that do
not appear in the other. You could implement this by gathering the
second vector on each processor, but this may be prohibitive in memory
usage.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you think of another algorithm for taking the set difference of
  two distributed vectors. Hint: look up 
<i>bucket brigade</i>
 algorithm;
  section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-ptp.html#Bucketbrigade">4.1.5</a>
.
  What is the time and space complexity
  of this algorithm? Can you think of other advantages beside a
  reduction in workspace?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<h2><a id="All-to-all">3.6</a> All-to-all</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#All-to-all">All-to-all</a>
</p>

</p>

<p name="switchToTextMode">
The all-to-all operation
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Alltoall" aria-expanded="false" aria-controls="MPI_Alltoall">
        Routine reference: MPI_Alltoall
      </button>
    </h5>
  </div>
  <div id="MPI_Alltoall" class="collapse">
  <pre>
int MPI_Alltoallv
  (void *sendbuf, int sendcnt, MPI_Datatype sendtype,
   void *recvbuf, int recvcnt, MPI_Datatype recvtype,
   MPI_Comm comm)

</pre>
</div>
</div>
<i>MPI_Alltoall</i>
can be seen as a collection of simultaneous
broadcasts or simultaneous gathers. The parameter specification is much
like an allgather, with a separate send and receive buffer, and no
root specified. As with the gather call, the receive count corresponds
to an individual receive, not the total amount.
</p>

<p name="switchToTextMode">
Unlike the gather call, the send buffer now obeys the same principle:
with a send count of~1, the buffer has a length of the number of
processes.
</p>

<h3><a id="All-to-allasdatatranspose">3.6.1</a> All-to-all as data transpose</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#All-to-all">All-to-all</a> > <a href="mpi-collective.html#All-to-allasdatatranspose">All-to-all as data transpose</a>
</p>
<!-- index -->

<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/alltoall.png" width=800></img>
<p name="caption">
FIGURE 3.7: All-to-all transposes data
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The all-to-all operation can be considered as a data transpose. For
instance, assume that each process knows how much data to send to
every other process. If you draw a connectivity matrix of size $P\times P$,
denoting who-sends-to-who, then the send information can be put in
rows:
\[
 \forall_i\colon C[i,j]>0\quad\hbox{if process $i$ sends to process $j$}. 
\]
Conversely, the columns then denote the receive information:
\[
 \forall_j\colon C[i,j]>0\quad\hbox{if process $j$ receives from process $i$}. 
\]
</p>

<p name="switchToTextMode">
The typical application for such data transposition is in the 
<span title="acronym" ><i>FFT</i></span>
algorithm, where it can take tens of percents of the running time on
large clusters.
</p>

<p name="switchToTextMode">
We will consider
another application of data transposition, namely 
<i>radix sort</i>
,
but we will do that in a couple of steps. First of all:
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  In the initial stage of a 
<i>radix sort</i>
, each process
  considers how many elements to send to every other process.
  Use 
<tt>MPI_Alltoall</tt>
 to derive from this how many
  elements they will receive from every other process.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h3><a id="All-to-all-v">3.6.2</a> All-to-all-v</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#All-to-all">All-to-all</a> > <a href="mpi-collective.html#All-to-all-v">All-to-all-v</a>
</p>

<p name="switchToTextMode">

The major part of the 
<i>radix sort</i>
 algorithm consist
of every process sending some of its elements to
each of the other processes.
The routine 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Alltoallv" aria-expanded="false" aria-controls="MPI_Alltoallv">
        Routine reference: MPI_Alltoallv
      </button>
    </h5>
  </div>
  <div id="MPI_Alltoallv" class="collapse">
  <pre>
int MPI_Alltoallv
  (void *sendbuf, int *sendcnts, int *sdispls, MPI_Datatype sendtype,
   void *recvbuf, int *recvcnts, int *rdispls, MPI_Datatype recvtype,
   MPI_Comm comm)

</pre>
</div>
</div>
<i>MPI_Alltoallv</i>
 is used for this pattern:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Every process scatters its data to all others,
<li>
but the amount of data is different per process.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  The actual data shuffle of a 
<i>radix sort</i>
 can be done
  with 
<tt>MPI_Alltoallv</tt>
. Finish the code of
  exercise~
3.7
.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Reduce-scatter">3.7</a> Reduce-scatter</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduce-scatter">Reduce-scatter</a>
</p>

</p>

<p name="switchToTextMode">
There are several MPI collectives that are functionally equivalent to
a combination of others. You have already seen 
<tt>MPI_Allreduce</tt>
 which
is equivalent to a reduction followed by a broadcast. Often such
combinations can be more efficient than using the individual calls;
see~
<i>Eijkhout:IntroHPC</i>
.
</p>

<p name="switchToTextMode">
Here is another example: 
<tt>MPI_Reduce_scatter</tt>
 is equivalent
to a reduction on an array of data (meaning a pointwise reduction on each
array location) followed by a scatter of this array to the individual
processes.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/reducescatter.png" width=800></img>
<p name="caption">
FIGURE 3.8: Reduce scatter
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

We will discuss this routine,
or rather its variant 
<i>MPI_Reduce_scatter_block</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Reduce_scatter_block" aria-expanded="false" aria-controls="MPI_Reduce_scatter_block">
        Routine reference: MPI_Reduce_scatter_block
      </button>
    </h5>
  </div>
  <div id="MPI_Reduce_scatter_block" class="collapse">
  <pre>
Semantics:
MPI_Reduce_scatter
   ( sendbuf, recvbuf, recvcounts, datatype, op, comm)
MPI_Reduce_scatter_block
   ( sendbuf, recvbuf, recvcount, datatype, op, comm)

Input parameters:
sendbuf: starting address of send buffer (choice)
recvcount: element count per block (non-negative integer)
recvcounts: non-negative integer array (of length group size)
    specifying the number of elements of the result distributed to each
    process.
datatype: data type of elements of send and receive buffers (handle)
op: operation (handle)
comm: communicator (handle)

Output parameters:
recvbuf: starting address of receive buffer (choice)

C:
int MPI_Reduce_scatter
   (const void* sendbuf, void* recvbuf, const int recvcounts[],
    MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)

F:
MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm,
ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: recvcounts(*)
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Op), INTENT(IN) :: op
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Py:
comm.Reduce_scatter(sendbuf, recvbuf, recvcounts=None, Op op=SUM)
</pre>
</div>
</div>
,
using an important example: the
<i>sparse matrix-vector product</i>
(see~
<i>Eijkhout:IntroHPC</i>
 for background information).
Each process contains one or more matrix rows, so by looking at indices
the process can decide what other processes it needs
to receive data from,
that is, each process knows how many messages it will receive,
and from which processes.
The problem is for a process to find out what other processes
it needs to send data to.
</p>

<p name="switchToTextMode">
Let's set up the data:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducescatterdata" aria-expanded="false" aria-controls="reducescatterdata">
        C Code: reducescatterdata
      </button>
    </h5>
  </div>
  <div id="reducescatterdata" class="collapse">
  <pre>
// reducescatter.c
int
// data that we know:
  *i_recv_from_proc = (int*) malloc(nprocs*sizeof(int)),
  *procs_to_recv_from, nprocs_to_recv_from=0,
// data we are going to determin:
  *procs_to_send_to,nprocs_to_send_to;
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
Each process creates an array of ones and zeros, describing who
it needs data from.
Ideally, we only need the array  <tt>procs_to_recv_from</tt> 
but initially we need the (possibly much larger) array
 <tt>i_recv_from_proc</tt> .
</p>

<p name="switchToTextMode">
Next, the 
<tt>MPI_Reduce_scatter_block</tt>
 call then
computes, on each process, how many messages it needs to send.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducescattercall" aria-expanded="false" aria-controls="reducescattercall">
        C Code: reducescattercall
      </button>
    </h5>
  </div>
  <div id="reducescattercall" class="collapse">
  <pre>
MPI_Reduce_scatter_block
  (i_recv_from_proc,&nprocs_to_send_to,1,MPI_INT,
  MPI_SUM,comm);
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
We do not yet have the information to which processes to send.
For that, each process sends a zero-size message to
each of its senders.
Conversely, it then does a receive to with  <tt>MPI_ANY_SOURCE</tt> 
to discover who is requesting data from it.
The crucial point to the  <tt>MPI_Reduce_scatter_block</tt>  call
is that, without it, a~process would not know how many
of these zero-size messages to expect.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducescattertest" aria-expanded="false" aria-controls="reducescattertest">
        C Code: reducescattertest
      </button>
    </h5>
  </div>
  <div id="reducescattertest" class="collapse">
  <pre>
/*
 * Send a zero-size msg to everyone that you receive from,
 * just to let them know that they need to send to you.
 */
MPI_Request send_requests[nprocs_to_recv_from];
for (int iproc=0; iproc&lt;nprocs_to_recv_from; iproc++) {
  int proc=procs_to_recv_from[iproc];
  double send_buffer=0.;
  MPI_Isend(&send_buffer,0,MPI_DOUBLE, /*to:*/ proc,0,comm,
	      &(send_requests[iproc]));
}

/*
 * Do as many receives as you know are coming in;
 * use wildcards since you don't know where they are coming from.
 * The source is a process you need to send to.
 */
procs_to_send_to = (int*)malloc( nprocs_to_send_to * sizeof(int) );
for (int iproc=0; iproc&lt;nprocs_to_send_to; iproc++) {
  double recv_buffer;
  MPI_Status status;
  MPI_Recv(&recv_buffer,0,MPI_DOUBLE,MPI_ANY_SOURCE,MPI_ANY_TAG,comm,
	     &status);
  procs_to_send_to[iproc] = status.MPI_SOURCE;
}
MPI_Waitall(nprocs_to_recv_from,send_requests,MPI_STATUSES_IGNORE);
</pre>
</div>
</div>
<p name="switchToTextMode">

The 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Reduce_scatter" aria-expanded="false" aria-controls="MPI_Reduce_scatter">
        Routine reference: MPI_Reduce_scatter
      </button>
    </h5>
  </div>
  <div id="MPI_Reduce_scatter" class="collapse">
  <pre>
Semantics:
MPI_Reduce_scatter
   ( sendbuf, recvbuf, recvcounts, datatype, op, comm)
MPI_Reduce_scatter_block
   ( sendbuf, recvbuf, recvcount, datatype, op, comm)

Input parameters:
sendbuf: starting address of send buffer (choice)
recvcount: element count per block (non-negative integer)
recvcounts: non-negative integer array (of length group size)
    specifying the number of elements of the result distributed to each
    process.
datatype: data type of elements of send and receive buffers (handle)
op: operation (handle)
comm: communicator (handle)

Output parameters:
recvbuf: starting address of receive buffer (choice)

C:
int MPI_Reduce_scatter
   (const void* sendbuf, void* recvbuf, const int recvcounts[],
    MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)

F:
MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm,
ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: recvcounts(*)
TYPE(MPI_Datatype), INTENT(IN) :: datatype
TYPE(MPI_Op), INTENT(IN) :: op
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Py:
comm.Reduce_scatter(sendbuf, recvbuf, recvcounts=None, Op op=SUM)
</pre>
</div>
</div>
<i>MPI_Reduce_scatter</i>
 call is more general:
instead of indicating the mere presence of a message
between two processes,
by having individual receive counts one can, for instance,
indicate the size of the messages.
</p>

<p name="switchToTextMode">
We can look at reduce-scatter as a limited form of the all-to-all data
transposition discussed above (section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#All-to-allasdatatranspose">3.6.1</a>
).
Suppose that the matrix~$C$ contains only~$0/1$, indicating
whether or not a messages is send, rather than the actual amounts.
If a receiving process only needs to know how many messages to
receive, rather than where they come from, it is enough to know the
column sum, rather than the full column (see figure~
3.8
).
</p>

<p name="switchToTextMode">
Another application of the reduce-scatter mechanism is in the
dense matrix-vector product, if a two-dimensional data distribution
is used.
</p>

<h3><a id="Examples">3.7.1</a> Examples</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reduce-scatter">Reduce-scatter</a> > <a href="mpi-collective.html#Examples">Examples</a>
</p>
<p name="switchToTextMode">

An important application of this is establishing an irregular
communication pattern.  Assume that each process knows which
other processes it wants to communicate with; the problem is to
let the other processes know about this.
The solution is to use 
<tt>MPI_Reduce_scatter</tt>
 to find out how many processes
want to communicate with you
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducescattercall" aria-expanded="false" aria-controls="reducescattercall">
        C Code: reducescattercall
      </button>
    </h5>
  </div>
  <div id="reducescattercall" class="collapse">
  <pre>
MPI_Reduce_scatter_block
  (i_recv_from_proc,&nprocs_to_send_to,1,MPI_INT,
  MPI_SUM,comm);
</pre>
</div>
</div>
and then wait for precisely that many messages
with a source value of 
<tt>MPI_ANY_SOURCE</tt>
.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#reducescattertest" aria-expanded="false" aria-controls="reducescattertest">
        C Code: reducescattertest
      </button>
    </h5>
  </div>
  <div id="reducescattertest" class="collapse">
  <pre>
/*
 * Send a zero-size msg to everyone that you receive from,
 * just to let them know that they need to send to you.
 */
MPI_Request send_requests[nprocs_to_recv_from];
for (int iproc=0; iproc&lt;nprocs_to_recv_from; iproc++) {
  int proc=procs_to_recv_from[iproc];
  double send_buffer=0.;
  MPI_Isend(&send_buffer,0,MPI_DOUBLE, /*to:*/ proc,0,comm,
	      &(send_requests[iproc]));
}

/*
 * Do as many receives as you know are coming in;
 * use wildcards since you don't know where they are coming from.
 * The source is a process you need to send to.
 */
procs_to_send_to = (int*)malloc( nprocs_to_send_to * sizeof(int) );
for (int iproc=0; iproc&lt;nprocs_to_send_to; iproc++) {
  double recv_buffer;
  MPI_Status status;
  MPI_Recv(&recv_buffer,0,MPI_DOUBLE,MPI_ANY_SOURCE,MPI_ANY_TAG,comm,
	     &status);
  procs_to_send_to[iproc] = status.MPI_SOURCE;
}
MPI_Waitall(nprocs_to_recv_from,send_requests,MPI_STATUSES_IGNORE);
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
Use of 
<tt>MPI_Reduce_scatter</tt>
 to implement the two-dimensional
matrix-vector product.
Set up separate row and column communicators with
<tt>MPI_Comm_split</tt>
, use 
<tt>MPI_Reduce_scatter</tt>
 to combine
local products.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Allgather(&my_x,1,MPI_DOUBLE,
   	  local_x,1,MPI_DOUBLE,environ.col_comm);
MPI_Reduce_scatter(local_y,&my_y,&ione,MPI_DOUBLE,
	  MPI_SUM,environ.row_comm);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Barrier">3.8</a> Barrier</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Barrier">Barrier</a>
</p>

</p>

<p name="switchToTextMode">
A&nbsp;barrier call,
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Barrier" aria-expanded="false" aria-controls="MPI_Barrier">
        Routine reference: MPI_Barrier
      </button>
    </h5>
  </div>
  <div id="MPI_Barrier" class="collapse">
  <pre>
C:
int MPI_Barrier( MPI_Comm comm )

Fortran2008:
MPI_BARRIER(COMM, IERROR)
Type(MPI_Comm),intent(int) :: COMM
INTEGER,intent(out) :: IERROR

Fortran 95:
MPI_BARRIER(COMM, IERROR)
INTEGER :: COMM, IERROR

Input parameter:
comm : Communicator (handle)

Output parameter:
Ierror : Error status (integer), Fortran only
</pre>
</div>
</div>
<i>MPI_Barrier</i>
is a
routine that blocks all processes until they have all reached the barrier
call. Thus it achieves time synchronization of the processes.
</p>

<p name="switchToTextMode">
This call's simplicity is contrasted with its usefulness, which
is very limited. It is almost never necessary to synchronize processes
through a barrier: for most purposes it does not matter if processors
are out of sync. Conversely, collectives (except the new nonblocking
ones; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Nonblockingcollectives">3.11</a>
) introduce a barrier of sorts themselves.
</p>

<p name="switchToTextMode">

<!-- TranslatingLineGenerator file ['file'] -->
</p>

<h2><a id="Variable-size-inputcollectives">3.9</a> Variable-size-input collectives</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Variable-size-inputcollectives">Variable-size-input collectives</a>
</p>

<p name="switchToTextMode">

In the gather and scatter call above each processor received or sent
an identical number of items. In many cases this is appropriate, but
sometimes each processor wants or contributes an individual number of
items.
</p>

<p name="switchToTextMode">
Let's take the gather calls as an example. Assume that each processor
does a local computation that produces a number of data elements,
and this number is different for each processor (or at least not
the same for all). In the regular 
<tt>MPI_Gather</tt>
 call the root processor
had a buffer of size~$nP$, where $n$~is the number of elements produced
on each processor, and $P$~the number of processors. The contribution
from processor~$p$ would go into locations $pn,\ldots,(p+1)n-1$.
</p>

<p name="switchToTextMode">
For the variable case, we first need to compute the total required
buffer size. This can be done through a simple 
<tt>MPI_Reduce</tt>
with 
<tt>MPI_SUM</tt>
 as reduction operator:
the buffer size is $\sum_p n_p$ where $n_p$~is the number of elements
on processor~$p$. But you can also postpone
this calculation for a minute.
</p>

<p name="switchToTextMode">
The next question is where the contributions of the processor will
go into this buffer. For the contribution from processor~$p$
that is $\sum_{q<p}n_p,\ldots\sum_{q\leq p}n_p-1$. To compute this,
the root processor needs to have all the $n_p$ numbers, and it can collect
them with an 
<tt>MPI_Gather</tt>
 call.
</p>

<p name="switchToTextMode">
We now have all the ingredients.
All the processors specify a send buffer just as with 
<tt>MPI_Gather</tt>
.
However, the receive buffer specification on the root is more complicated.
It now consists of:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
outbuffer, array-of-outcounts, array-of-displacements, outtype
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
and you have just seen how to construct that information.
</p>

<p name="switchToTextMode">
For example, in an 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Gatherv" aria-expanded="false" aria-controls="MPI_Gatherv">
        Routine reference: MPI_Gatherv
      </button>
    </h5>
  </div>
  <div id="MPI_Gatherv" class="collapse">
  <pre>
C:
int MPI_Gatherv(
  const void* sendbuf, int sendcount, MPI_Datatype sendtype,
  void* recvbuf, const int recvcounts[], const int displs[],
  MPI_Datatype recvtype, int root, MPI_Comm comm)

Semantics:
IN sendbuf: starting address of send buffer (choice)
IN sendcount: number of elements in send buffer (non-negative integer)
IN sendtype: data type of send buffer elements (handle)
OUT recvbuf: address of receive buffer (choice, significant only at root)
IN recvcounts: non-negative integer array (of length group size) containing the number of elements that are received from each process (significant only at root)
IN displs: integer array (of length group size). Entry i specifies the displacement relative to recvbuf at which to place the incoming data from process i (significant only at root)
IN recvtype: data type of recv buffer elements (significant only at root) (handle)
IN root: rank of receiving process (integer)
IN comm: communicator (handle)

Fortran:
MPI_Gatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: sendcount, recvcounts(*), displs(*), root
TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
Gatherv(self, sendbuf, [recvbuf,counts], int root=0)
</pre>
</div>
</div>
<i>MPI_Gatherv</i>
 call each process has an individual
number of items to contribute. To gather this, the root process needs
to find these individual amounts with an 
<tt>MPI_Gather</tt>
 call, and
locally construct the offsets array. Note how the offsets array has
size 
<tt>ntids+1</tt>
: the final offset value is automatically the total
size of all incoming data. See the example below.
</p>

<p name="switchToTextMode">
There are various calls where processors can have
buffers of differing sizes.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
In 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Scatterv" aria-expanded="false" aria-controls="MPI_Scatterv">
        Routine reference: MPI_Scatterv
      </button>
    </h5>
  </div>
  <div id="MPI_Scatterv" class="collapse">
  <pre>
</pre>
</div>
</div>
<i>MPI_Scatterv</i>
 the root process has a different
  amount of data for each recipient.
<li>
In 
<tt>MPI_Gatherv</tt>
, conversely, each process
  contributes a different sized send buffer to the received result;
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Allgatherv" aria-expanded="false" aria-controls="MPI_Allgatherv">
        Routine reference: MPI_Allgatherv
      </button>
    </h5>
  </div>
  <div id="MPI_Allgatherv" class="collapse">
  <pre>
Semantics:
MPI_ALLGATHERV(
    sendbuf, sendcount, sendtype,
    recvbuf, recvcounts, displs, recvtype,
    comm)
IN sendbuf: starting address of send buffer (choice)
IN sendcount: number of elements in send buffer (non-negative integer)
IN sendtype: data type of send buffer elements (handle)
OUT recvbuf: address of receive buffer (choice)
IN recvcounts: non-negative integer array (of length group size)
    containing the number of elements that are received from each process
IN displs: integer array (of length group size). Entry i specifies the
    displacement (relative to recvbuf) at which to place the incoming data
    from process i
IN recvtype: data type of receive buffer elements (handle)
IN comm: communicator (handle)

C:
int MPI_Allgatherv(
    const void* sendbuf, int sendcount, MPI_Datatype sendtype,
    void* recvbuf, const int recvcounts[], const int displs[], MPI_Datatype recvtype,
    MPI_Comm comm)

Fortran:
MPI_Allgatherv(
    sendbuf, sendcount, sendtype,
    recvbuf, recvcounts, displs, recvtype,
    comm, ierror)
TYPE(*), DIMENSION(..), INTENT(IN) :: sendbuf
TYPE(*), DIMENSION(..) :: recvbuf
INTEGER, INTENT(IN) :: sendcount, recvcounts(*), displs(*)
TYPE(MPI_Datatype), INTENT(IN) :: sendtype, recvtype
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python native:
not implemented
Python numpy:
MPI.Comm.Allgatherv(self, sendbuf, recvbuf)
    where recvbuf = "[ array, counts, displs, type]"

</pre>
</div>
</div>
<i>MPI_Allgatherv</i>
 does the same, but leaves its result
  on all processes; 
<tt>MPI_Alltoallv</tt>
 does a different
  variable-sized gather on each process.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h3><a id="ExampleofGatherv">3.9.1</a> Example of Gatherv</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Variable-size-inputcollectives">Variable-size-input collectives</a> > <a href="mpi-collective.html#ExampleofGatherv">Example of Gatherv</a>
</p>
</p>

<p name="switchToTextMode">
We use 
<tt>MPI_Gatherv</tt>
 to do an irregular gather onto a root. We first need an
<tt>MPI_Gather</tt>
 to determine offsets.
\csnippetwithoutput{gatherv}{examples/mpi/c}{gatherv}
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#gathervp" aria-expanded="false" aria-controls="gathervp">
        Python Code: gathervp
      </button>
    </h5>
  </div>
  <div id="gathervp" class="collapse">
  <pre>
## gatherv.py
# implicitly using root=0
globalsize = comm.reduce(localsize)
if procid==0:
    print("Global size=%d" % globalsize)
collecteddata = np.empty(globalsize,dtype=np.int)
counts = comm.gather(localsize)
comm.Gatherv(localdata, [collecteddata, counts])
</pre>
</div>
</div>
</p>

<!-- environment: mplnote start embedded generator -->
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<tt>layouts</tt>
<tt>layout</tt>
<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
%% comm_world.gatherv
%%    ( root, sendbuffer, gatherbuffer,
%%      receive_layout );
%% 
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="ExampleofAllgatherv">3.9.2</a> Example of Allgatherv</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Variable-size-inputcollectives">Variable-size-input collectives</a> > <a href="mpi-collective.html#ExampleofAllgatherv">Example of Allgatherv</a>
</p>
</p>

<p name="switchToTextMode">
Prior to the actual gatherv call, we need to construct the count and
displacement arrays. The easiest way is to use a reduction.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allgathervc" aria-expanded="false" aria-controls="allgathervc">
        C Code: allgathervc
      </button>
    </h5>
  </div>
  <div id="allgathervc" class="collapse">
  <pre>
// allgatherv.c
MPI_Allgather
  ( &my_count,1,MPI_INT,
    recv_counts,1,MPI_INT, comm );
int accumulate = 0;
for (int i=0; i&lt;nprocs; i++) {
  recv_displs[i] = accumulate; accumulate += recv_counts[i]; }
int *global_array = (int*) malloc(accumulate*sizeof(int));
MPI_Allgatherv
  ( my_array,procno+1,MPI_INT,
    global_array,recv_counts,recv_displs,MPI_INT, comm );
</pre>
</div>
</div>
<p name="switchToTextMode">

In python the receive buffer has to contain the counts and
displacements arrays.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#bufallocp" aria-expanded="false" aria-controls="bufallocp">
        Python Code: bufallocp
      </button>
    </h5>
  </div>
  <div id="bufallocp" class="collapse">
  <pre>
## allgatherv.py
mycount = procid+1
my_array = np.empty(mycount,dtype=np.float64)
</pre>
</div>
</div>
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allgathervp" aria-expanded="false" aria-controls="allgathervp">
        Python Code: allgathervp
      </button>
    </h5>
  </div>
  <div id="allgathervp" class="collapse">
  <pre>
my_count = np.empty(1,dtype=np.int)
my_count[0] = mycount
comm.Allgather( my_count,recv_counts )

accumulate = 0
for p in range(nprocs):
    recv_displs[p] = accumulate; accumulate += recv_counts[p]
global_array = np.empty(accumulate,dtype=np.float64)
comm.Allgatherv( my_array, [global_array,recv_counts,recv_displs,MPI.DOUBLE] )
</pre>
</div>
</div>
</p>

<h3><a id="Variableall-to-all">3.9.3</a> Variable all-to-all</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Variable-size-inputcollectives">Variable-size-input collectives</a> > <a href="mpi-collective.html#Variableall-to-all">Variable all-to-all</a>
</p>
<p name="switchToTextMode">

The variable all-to-all routine 
<tt>MPI_Alltoallv</tt>
is discussed in section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#All-to-all-v">3.6.2</a>
.
</p>

<p name="switchToTextMode">

</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<h2><a id="MPIOperators">3.10</a> MPI Operators</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#MPIOperators">MPI Operators</a>
</p>

<!-- index -->
</p>

<p name="switchToTextMode">
MPI 
<i>operators</i>
,
that is, objects of type 
<tt>MPI_Op</tt>
,
are used in reduction operators.
Most common
operators, such as sum or maximum, have been built into the MPI
library; see section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Pre-definedoperators">3.10.1</a>
.
It is also possible to define new operators;
see section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#User-definedoperators">3.10.2</a>
.
</p>

<h3><a id="Pre-definedoperators">3.10.1</a> Pre-defined operators</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#MPIOperators">MPI Operators</a> > <a href="mpi-collective.html#Pre-definedoperators">Pre-defined operators</a>
</p>

<p name="switchToTextMode">

The following is the list of 
<i>pre-defined operators</i>

<!-- index -->
<tt>MPI_Op</tt>
 values.
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
    </td></tr>
<tr><td>
  MPI type</td><td>meaning</td><td>applies to\ </td></tr>
<tr><td>
<tt>MPI_MAX</tt>
</td><td>maximum</td><td>integer, floating point</td></tr>
<tr><td>
<tt>MPI_MIN</tt>
</td><td>minimum</td><td></td></tr>
<tr><td>
<tt>MPI_SUM</tt>
</td><td>sum</td><td>integer, floating point, complex,
  multilanguage types</td></tr>
<tr><td>
<tt>MPI_REPLACE</tt>
</td><td>overwrite</td><td></td></tr>
<tr><td>
<tt>MPI_NO_OP</tt>
</td><td>no change</td><td></td></tr>
<tr><td>
<tt>MPI_PROD</tt>
</td><td>product</td><td></td></tr>
<tr><td>
<tt>MPI_LAND</tt>
</td><td>logical and</td><td>C integer, logical</td></tr>
<tr><td>
<tt>MPI_LOR</tt>
</td><td>logical or</td><td></td></tr>
<tr><td>
<tt>MPI_LXOR</tt>
</td><td>logical xor</td><td></td></tr>
<tr><td>
<tt>MPI_BAND</tt>
</td><td>bitwise and</td><td>integer, byte, multilanguage types</td></tr>
<tr><td>
<tt>MPI_BOR</tt>
</td><td>bitwise or</td><td></td></tr>
<tr><td>
<tt>MPI_BXOR</tt>
</td><td>bitwise xor</td><td></td></tr>
<tr><td>
<tt>MPI_MAXLOC</tt>
</td><td>max value and
  location</td><td>
<tt>MPI_DOUBLE_INT</tt>
 and such</td></tr>
<tr><td>
<tt>MPI_MINLOC</tt>
</td><td>min value and location</td><td></td></tr>
<tr><td>
  </td></tr>
<tr><td>
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

</p>

<h4><a id="Minlocandmaxloc">3.10.1.1</a> Minloc and maxloc</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#MPIOperators">MPI Operators</a> > <a href="mpi-collective.html#Pre-definedoperators">Pre-defined operators</a> > <a href="mpi-collective.html#Minlocandmaxloc">Minloc and maxloc</a>
</p>
<p name="switchToTextMode">

The 
<tt>MPI_MAXLOC</tt>
 and 
<tt>MPI_MINLOC</tt>
 operations
yield both the maximum and the rank on which it occurs. Their result
is a 
<tt>struct</tt>
 of the data over which the reduction happens,
and an int.
</p>

<p name="switchToTextMode">
In C, the types to use in the reduction call are:
<tt>MPI_FLOAT_INT</tt>
,
<tt>MPI_LONG_INT</tt>
,
<tt>MPI_DOUBLE_INT</tt>
,
<tt>MPI_SHORT_INT</tt>
,
<tt>MPI_2INT</tt>
,
<tt>MPI_LONG_DOUBLE_INT</tt>
.
Likewise, the input needs to consist of such structures:
the input should be an array of such struct types, where the 
<tt>int</tt>
 is
the rank of the number.
</p>

<p name="switchToTextMode">
The original Fortran interface to MPI was designed around
\fstandard{77} features,
so it is not using Fortran derived types (
<tt>Type</tt>
 keyword).
Instead, all integer indices are stored in whatever the type is that is
being reduced. The available result types are then
<tt>MPI_2REAL</tt>
,
<tt>MPI_2DOUBLE_PRECISION</tt>
,
<tt>MPI_2INTEGER</tt>
.
</p>

<p name="switchToTextMode">
Likewise, the input needs to be arrays of such type. Consider this example:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
Real*8,dimension(2,N) :: input,output
call MPI_Reduce( input,output, N, MPI_2DOUBLE_PRECISION, &
                 MPI_MAXLOC, root, comm )
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  Arithmetic: 
<tt>plus</tt>
<tt>multiplies</tt>
<tt>max</tt>
<tt>min</tt>
</p>

<p name="switchToTextMode">
  Logic: 
<tt>logical_and</tt>
<tt>logical_or</tt>
<tt>logical_xor</tt>
</p>

<p name="switchToTextMode">
  Bitwise: 
<tt>bit_and</tt>
<tt>bit_or</tt>
<tt>bit_xor</tt>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="User-definedoperators">3.10.2</a> User-defined operators</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#MPIOperators">MPI Operators</a> > <a href="mpi-collective.html#User-definedoperators">User-defined operators</a>
</p>

</p>

<p name="switchToTextMode">
In addition to predefined operators, MPI has the possibility of
<i>user-defined operators</i>
<!-- index -->
to use in a reduction or scan operation.
</p>

<p name="switchToTextMode">
The routine for this is 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Op_create" aria-expanded="false" aria-controls="MPI_Op_create">
        Routine reference: MPI_Op_create
      </button>
    </h5>
  </div>
  <div id="MPI_Op_create" class="collapse">
  <pre>
Semantics:
MPI_OP_CREATE( function, commute, op)
[ IN function] user defined function (function)
[ IN commute] true if commutative; false otherwise.
[ OUT op] operation (handle)

C:
int MPI_Op_create
   (MPI_User_function *function, int commute,
    MPI_Op *op)

Fortran 2008:
USE mpi_f08
MPI_Op_create(user_fn, commute, op, ierror)
PROCEDURE(MPI_User_function) :: user_fn
LOGICAL, INTENT(IN) :: commute
TYPE(MPI_Op), INTENT(OUT) :: op
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Fortran90:
MPI_OP_CREATE( FUNCTION, COMMUTE, OP, IERROR)
EXTERNAL FUNCTION
LOGICAL COMMUTE
INTEGER OP, IERROR

Python:
MPI.Op.create(cls,function,bool commute=False)
</pre>
</div>
</div>
<i>MPI_Op_create</i>
,
which takes a user function and turns it into
an object of type 
<tt>MPI_Op</tt>
, which can then be
used in any reduction:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#opcreatec" aria-expanded="false" aria-controls="opcreatec">
        C Code: opcreatec
      </button>
    </h5>
  </div>
  <div id="opcreatec" class="collapse">
  <pre>
MPI_Op rwz;
MPI_Op_create(reduce_without_zero,1,&rwz);
MPI_Allreduce(data+procno,&positive_minimum,1,MPI_INT,rwz,comm);
</pre>
</div>
</div>
</p>

<!-- environment: pythonnote start embedded generator -->
<!-- environment block purpose: [[ environment=pythonnote ]] -->
<remark>
<b>Python note</b>
<!-- TranslatingLineGenerator pythonnote ['pythonnote'] -->
  In python, \lstinline+Op.Create+ is a class method for the  <tt>MPI</tt>  class.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#opcreatep" aria-expanded="false" aria-controls="opcreatep">
        Python Code: opcreatep
      </button>
    </h5>
  </div>
  <div id="opcreatep" class="collapse">
  <pre>
rwz = MPI.Op.Create(reduceWithoutZero)
positive_minimum = np.zeros(1,dtype=np.intc)
comm.Allreduce(data[procid],positive_minimum,rwz);
</pre>
</div>
</div>
</remark>
<!-- environment: pythonnote end embedded generator -->
<p name="switchToTextMode">

The user function needs to have the following signature:
</p>

<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
typedef void MPI_User_function
    ( void *invec, void *inoutvec, int *len,
      MPI_Datatype *datatype);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
FUNCTION USER_FUNCTION( INVEC(*), INOUTVEC(*), LEN, TYPE)
&lt;type&gt; INVEC(LEN), INOUTVEC(LEN)
INTEGER LEN, TYPE
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

For example, here is an operator for finding the smallest nonzero
number in an array of nonnegative integers:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpirwz" aria-expanded="false" aria-controls="mpirwz">
        C Code: mpirwz
      </button>
    </h5>
  </div>
  <div id="mpirwz" class="collapse">
  <pre>
// reductpositive.c
void reduce_without_zero(void *in,void *inout,int *len,MPI_Datatype *type) {
// r is the already reduced value, n is the new value
  int n = *(int*)in, r = *(int*)inout;
  int m;
  if (n==0) { // new value is zero: keep r
    m = r;
  } else if (r==0) {
    m = n;
  } else if (n&lt;r) { // new value is less but not zero: use n
    m = n;
  } else { // new value is more: use r
    m = r;
  };
  *(int*)inout = m;
}
</pre>
</div>
</div>
</p>

<!-- environment: pythonnote start embedded generator -->
<!-- environment block purpose: [[ environment=pythonnote ]] -->
<remark>
<b>Python note</b>
<!-- TranslatingLineGenerator pythonnote ['pythonnote'] -->
<p name="switchToTextMode">
  The python equivalent of such a function receives bare buffers as
  arguments. Therefore, it is best to turn them first into NumPy arrays
  using 
<tt>np.frombuffer</tt>
:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#pmpirwz" aria-expanded="false" aria-controls="pmpirwz">
        Python Code: pmpirwz
      </button>
    </h5>
  </div>
  <div id="pmpirwz" class="collapse">
  <pre>
## reductpositive.py
def reduceWithoutZero(in_buf, inout_buf, datatype):
    typecode = MPI._typecode(datatype)
    assert typecode is not None ## check MPI datatype is built-in
    dtype = np.dtype(typecode)

    in_array = np.frombuffer(in_buf, dtype)
    inout_array = np.frombuffer(inout_buf, dtype)

    n = in_array[0]; r = inout_array[0]
    if n==0:
        m = r
    elif r==0:
        m = n
    elif n&lt;r:
        m = n
    else:
        m = r
    inout_array[0] = m
</pre>
</div>
</div>
  The 
<tt>assert</tt>
 statement accounts for the fact that this mapping of
  MPI datatype to NumPy dtype only works for built-in MPI datatypes.
</remark>
<!-- environment: pythonnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
  A user-defined operator can be a templated class with an  <tt>operator()</tt> .
<p name="switchToTextMode">
  Example:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpluserreduct" aria-expanded="false" aria-controls="mpluserreduct">
        C++ Code: mpluserreduct
      </button>
    </h5>
  </div>
  <div id="mpluserreduct" class="collapse">
  <pre>
// reduceuser.cxx
template&lt;typename T&gt;
class lcm {
public:
  T operator()(T a, T b) {
    T zero=T();
    T t((a/gcd(a, b))*b);
    if (t&lt;zero)
      return -t;
    return t;
  }
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  You can also do the reduction by lambda:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpllambdareduce" aria-expanded="false" aria-controls="mpllambdareduce">
        C++ Code: mpllambdareduce
      </button>
    </h5>
  </div>
  <div id="mpllambdareduce" class="collapse">
  <pre>
comm_world.reduce
  ( [] (int i,int j) -&gt; int
       { return i+j; },
	0,data );
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

The function has an array length argument&nbsp;
<tt>len</tt>
, to allow for
pointwise reduction on a whole array at once. The 
<tt>inoutvec</tt>
 array
contains partially reduced results, and is typically overwritten by
the function.
</p>

<p name="switchToTextMode">
There are some restrictions on the user function:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
It may not call MPI functions, except for
<tt>MPI_Abort</tt>
.
<li>
It must be associative; it can be optionally commutative, which
  fact is passed to the 
<tt>MPI_Op_create</tt>
 call.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Write the reduction function to implement the
  
<i>one-norm</i>

<!-- index -->
 of a vector:
\[
 \|x\|_1 \equiv \sum_i |x_i|. 
\]
<!-- skeleton start: onenorm -->
<button id="runBtnonenorm">Compile and run onenorm</button>
<div id="editorDivonenorm" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPreonenorm"></pre>
<script name="defSkeletononenorm">
let exampleonenorm = new Example(
    "runBtnonenorm", "editorDivonenorm", "outputPreonenorm", 
    "skeletons/onenorm.c", "onenorm.c",
    "mpicc onenorm.c && mpiexec -n 4 ./a.out" );
exampleonenorm.initialize();
</script>
<!-- skeleton end: onenorm -->
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

The operator can be destroyed with a corresponding
<tt>MPI_Op_free</tt>
.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
int MPI_Op_free(MPI_Op *op)
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
This sets the operator to 
<tt>MPI_OP_NULL</tt>
.
This is not necessary in 
<span title="acronym" ><i>OO</i></span>
 languages,
where the destructor takes care of it.
</p>

<p name="switchToTextMode">
You can query the commutativity of an operator with
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Op_commutative" aria-expanded="false" aria-controls="MPI_Op_commutative">
        Routine reference: MPI_Op_commutative
      </button>
    </h5>
  </div>
  <div id="MPI_Op_commutative" class="collapse">
  <pre>
Semantics:
MPI_Op_commutative(op, commute)
IN  op : handle
OUT commute : true/false

C:
int MPI_Op_commutative(MPI_Op op, int *commute)

Fortran:
MPI_OP_COMMUTATIVE( op, commute)
TYPE(MPI_Op), INTENT(IN) :: op
LOGICAL, INTENT(OUT) ::  commute
INTEGER, OPTIONAL, INTENT(OUT) ::  ierror
</pre>
</div>
</div>
<i>MPI_Op_commutative</i>
.
</p>

<h3><a id="Localreduction">3.10.3</a> Local reduction</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#MPIOperators">MPI Operators</a> > <a href="mpi-collective.html#Localreduction">Local reduction</a>
</p>
<p name="switchToTextMode">

The application of an 
<tt>MPI_Op</tt>
 can be performed with the routine
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Reduce_local" aria-expanded="false" aria-controls="MPI_Reduce_local">
        Routine reference: MPI_Reduce_local
      </button>
    </h5>
  </div>
  <div id="MPI_Reduce_local" class="collapse">
  <pre>
Semantics:
MPI_REDUCE_LOCAL( inbuf, inoutbuf, count, datatype, op)

Input parameters:
inbuf: input buffer (choice)
count: number of elements in inbuf and inoutbuf buffers
    (non-negative integer)
datatype: data type of elements of inbuf and inoutbuf buffers
    (handle)
op: operation (handle)

Input/output parameters:
inoutbuf: combined input and output buffer (choice)

C:
int MPI_Reduce_local
   (void* inbuf, void* inoutbuf, int count,
    MPI_Datatype datatype, MPI_Op op)

Fortran:
MPI_REDUCE_LOCAL(INBUF, INOUBUF, COUNT, DATATYPE, OP, IERROR)
<type> INBUF(*), INOUTBUF(*)
INTEGER :: COUNT, DATATYPE, OP, IERROR
</pre>
</div>
</div>
<i>MPI_Reduce_local</i>
. Using this routine and some
send/receive scheme you can build your own global reductions. Note
that this routine does not take a communicator because it is purely local.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Nonblockingcollectives">3.11</a> Nonblocking collectives</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a>
</p>

</p>

<p name="switchToTextMode">
Above you have seen how the `Isend' and `Irecv' routines can overlap communication
with computation. This is not possible with the collectives you have seen so far:
they act like blocking sends or receives.
However, there are also 
<i>nonblocking collectives</i>
,
introduced in \mpistandard{3}.
</p>

<p name="switchToTextMode">
Such operations can be used to increase efficiency.
For instance, computing
\[
 y \leftarrow Ax + (x^tx)y 
\]
involves a matrix-vector product, which is dominated by computation
in the 
<i>sparse matrix</i>
 case, and an inner product which is
typically dominated by the communication cost. You would code this as
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Iallreduce( .... x ..., &request);
// compute the matrix vector product
MPI_Wait(request);
// do the addition
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

This can also be used for 3D FFT operations&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/bibliography.html#Hoefler:case-for-nbc">[Hoefler:case-for-nbc]</a>
.
Occasionally, a nonblocking collective can be used for nonobvious purposes,
such as the 
<tt>MPI_Ibarrier</tt>
 in&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/bibliography.html#Hoefler:2010:SCP">[Hoefler:2010:SCP]</a>
.
</p>

<p name="switchToTextMode">
These have roughly the same calling sequence as their blocking counterparts,
except that they output an 
<tt>MPI_Request</tt>
. You
can then use an 
<tt>MPI_Wait</tt>
 call to make sure the collective
has completed.
</p>

<p name="switchToTextMode">
Nonblocking collectives offer a number of performance advantages:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Do two reductions (on the same communicator) with different
  operators simultaneously:
\[
\begin{array}{l}
  \alpha\leftarrow x^ty\\
  \beta\leftarrow \|z\|_\infty
\end{array}
\]
which translates to:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Allreduce( &local_xy,  &global_xy, 1,MPI_DOUBLE,MPI_SUM,comm);
MPI_Allreduce( &local_xinf,&global_xin,1,MPI_DOUBLE,MPI_MAX,comm);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<li>
do collectives on overlapping communicators simultaneously;
<li>
overlap a nonblocking collective with a blocking one.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Revisit exercise&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-comm.html#Examples">7.4.1</a>
. Let only the first row and
  first column have certain data, which they broadcast through columns
  and rows respectively. Each process is now involved in two
  simultaneous collectives. Implement this with nonblocking
  broadcasts, and time the difference between a blocking and a
  nonblocking solution.
<!-- skeleton start: procgridnonblock -->
<button id="runBtnprocgridnonblock">Compile and run procgridnonblock</button>
<div id="editorDivprocgridnonblock" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPreprocgridnonblock"></pre>
<script name="defSkeletonprocgridnonblock">
let exampleprocgridnonblock = new Example(
    "runBtnprocgridnonblock", "editorDivprocgridnonblock", "outputPreprocgridnonblock", 
    "skeletons/procgridnonblock.c", "procgridnonblock.c",
    "mpicc procgridnonblock.c && mpiexec -n 4 ./a.out" );
exampleprocgridnonblock.initialize();
</script>
<!-- skeleton end: procgridnonblock -->
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  Blocking and nonblocking don't match: either all processes
  call the nonblocking or all call the blocking one.
  Thus the following code is incorrect:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
if (rank==root)
  MPI_Reduce( &x /* ... */ root,comm );
else
  MPI_Ireduce( &x /* ... */ );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
  This is unlike the point-to-point behavior of nonblocking calls:
  you can catch a message with 
<tt>MPI_Irecv</tt>
  that was sent with 
<tt>MPI_Send</tt>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  Unlike sends and received, collectives have no identifying tag.
  With blocking collectives that does not lead to ambiguity problems.
  With nonblocking collectives it means that all processes
  need to issue them in identical order.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

List of nonblocking collectives:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
<tt>MPI_Igather</tt>
,
<tt>MPI_Igatherv</tt>
,
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Iallgather" aria-expanded="false" aria-controls="MPI_Iallgather">
        Routine reference: MPI_Iallgather
      </button>
    </h5>
  </div>
  <div id="MPI_Iallgather" class="collapse">
  <pre>
Semantics

int MPI_Iallgather(
    const void *sendbuf, int sendcount, MPI_Datatype sendtype,
    void *recvbuf, int recvcount, MPI_Datatype recvtype,
    MPI_Comm comm, MPI_Request *request)

Input Parameters

sendbuf : starting address of send buffer (choice)
sendcount : number of elements in send buffer (integer)
sendtype : data type of send buffer elements (handle)
recvcount : number of elements received from any process (integer)
recvtype : data type of receive buffer elements (handle)
comm : communicator (handle)

Output Parameters

recvbuf : address of receive buffer (choice)
request : communication request (handle)
</pre>
</div>
</div>
<i>MPI_Iallgather</i>
,
<tt>MPI_Iallgatherv</tt>
,
<li>
<tt>MPI_Iscatter</tt>
, 
<tt>MPI_Iscatterv</tt>
,
<li>
<tt>MPI_Ireduce</tt>
,
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Iallreduce" aria-expanded="false" aria-controls="MPI_Iallreduce">
        Routine reference: MPI_Iallreduce
      </button>
    </h5>
  </div>
  <div id="MPI_Iallreduce" class="collapse">
  <pre>
Semantics

int MPI_Iallreduce(
    const void *sendbuf, void *recvbuf,
    int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm,
    MPI_Request *request)

Input Parameters

sendbuf : starting address of send buffer (choice)
count : number of elements in send buffer (integer)
datatype : data type of elements of send buffer (handle)
op : operation (handle)
comm : communicator (handle)

Output Parameters

recvbuf : starting address of receive buffer (choice)
request : communication request (handle)
</pre>
</div>
</div>
<i>MPI_Iallreduce</i>
,
<tt>MPI_Ireduce_scatter</tt>
,
<tt>MPI_Ireduce_scatter_block</tt>
.
<li>
<tt>MPI_Ialltoall</tt>
,
<tt>MPI_Ialltoallv</tt>
, 
<tt>MPI_Ialltoallw</tt>
,
<li>
<tt>MPI_Ibarrier</tt>
; section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-collective.html#Nonblockingbarrier">3.11.2</a>
,
<li>
<tt>MPI_Ibcast</tt>
,
<li>
<tt>MPI_Iexscan</tt>
, 
<tt>MPI_Iscan</tt>
,
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  Nonblocking collectives have the same argument list as the
  corresponding blocking variant, except that
  instead of a  <tt>void</tt>  result,
  they return an 
<tt>irequest</tt>
  (See&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-ptp.html#Requestcompletion:waitcalls">4.2.2</a>
)
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mplireduce" aria-expanded="false" aria-controls="mplireduce">
        C++ Code: mplireduce
      </button>
    </h5>
  </div>
  <div id="mplireduce" class="collapse">
  <pre>
// ireducescalar.cxx
float x{1.},sum;
auto reduce_request =
  comm_world.ireduce(mpl::plus&lt;float&gt;(), 0, x, sum);
reduce_request.wait();
if (comm_world.rank()==0) {
  std::cout &lt;&lt; "sum = " &lt;&lt; sum &lt;&lt; '\n';
}
</pre>
</div>
</div>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Examples">3.11.1</a> Examples</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a> > <a href="mpi-collective.html#Examples">Examples</a>
</p>
</p>

<h4><a id="Arraytranspose">3.11.1.1</a> Array transpose</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a> > <a href="mpi-collective.html#Examples">Examples</a> > <a href="mpi-collective.html#Arraytranspose">Array transpose</a>
</p>
<!-- index -->
<p name="switchToTextMode">

To illustrate the overlapping of multiple nonblocking collectives,
consider transposing a data matrix.
Initially, each process has one row of the matrix;
after transposition each process has a column.
Since each row needs to be distributed to all processes,
algorithmically this corresponds to a series of scatter calls,
one originating from each process.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#transposescatterref" aria-expanded="false" aria-controls="transposescatterref">
        C Code: transposescatterref
      </button>
    </h5>
  </div>
  <div id="transposescatterref" class="collapse">
  <pre>
// itransposeblock.c
for (int iproc=0; iproc&lt;nprocs; iproc++) {
  MPI_Scatter( regular,1,MPI_DOUBLE,
		 &(transpose[iproc]),1,MPI_DOUBLE,
		 iproc,comm);
}
</pre>
</div>
</div>
Introducing the nonblocking 
<tt>MPI_Iscatter</tt>
 call,
this becomes:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#itransposescatter" aria-expanded="false" aria-controls="itransposescatter">
        C Code: itransposescatter
      </button>
    </h5>
  </div>
  <div id="itransposescatter" class="collapse">
  <pre>
MPI_Request scatter_requests[nprocs];
for (int iproc=0; iproc&lt;nprocs; iproc++) {
  MPI_Iscatter( regular,1,MPI_DOUBLE,
		  &(transpose[iproc]),1,MPI_DOUBLE,
		  iproc,comm,scatter_requests+iproc);
}
MPI_Waitall(nprocs,scatter_requests,MPI_STATUSES_IGNORE);
</pre>
</div>
</div>
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Can you implement the same algorithm with 
<tt>MPI_Igather</tt>
?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- index -->
</p>

<h4><a id="Stencils">3.11.1.2</a> Stencils</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a> > <a href="mpi-collective.html#Examples">Examples</a> > <a href="mpi-collective.html#Stencils">Stencils</a>
</p>
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/graphcollective.png" width=800></img>
<p name="caption">
FIGURE 3.9: Illustration of five-point stencil gather
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The ever-popular 
<i>five-point stencil</i>
 evaluation
does not look like a collective operation, and indeed,
it is usually evaluated with (nonblocking) send/recv operations.
However, if we create a subcommunicator on each subdomain
that contains precisely that domain and its neighbors,
(see figure&nbsp;
3.9
)
we can formulate the communication pattern as a gather on each of these.
With ordinary collectives this can not be formulated in a 
<i>deadlock</i>
-free
manner, but nonblocking collectives make this feasible.
</p>

<p name="switchToTextMode">
We will see an even more elegant formulation of this operation
in section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html#Distributedgraphtopology">11.2</a>
.
</p>

<h3><a id="Nonblockingbarrier">3.11.2</a> Nonblocking barrier</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Nonblockingcollectives">Nonblocking collectives</a> > <a href="mpi-collective.html#Nonblockingbarrier">Nonblocking barrier</a>
</p>

<p name="switchToTextMode">

Probably the most surprising nonblocking collective is the
<i>nonblocking barrier</i>
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Ibarrier" aria-expanded="false" aria-controls="MPI_Ibarrier">
        Routine reference: MPI_Ibarrier
      </button>
    </h5>
  </div>
  <div id="MPI_Ibarrier" class="collapse">
  <pre>
C:
int MPI_Ibarrier(MPI_Comm comm, MPI_Request *request)

Input Parameters
comm : communicator (handle)

Output Parameters
request : communication request (handle)

Fortran2008:
MPI_Ibarrier(comm, request, ierror)
Type(MPI_Comm),intent(int) :: comm
TYPE(MPI_Request),intent(out) :: request
INTEGER,intent(out),optional :: ierror
</pre>
</div>
</div>
<i>MPI_Ibarrier</i>
. The way to understand this is to think of
a barrier not in terms of temporal synchronization, but state
agreement: reaching a barrier is a sign that a process has attained a
certain state, and leaving a barrier means that all processes are in
the same state. The ordinary barrier is then a blocking wait for
agreement, while with a nonblocking barrier:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Posting the barrier means that a process has reached a certain
  state; and
<li>
the request being fullfilled means that all processes have
  reached the barrier.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

One scenario would be 
<i>local refinement</i>
, where
some processes decide to refine their subdomain, which
fact they need to communicate to their neighbors.
The problem here is that most processes are not among these neighbors,
so they should not post a receive of any type.
Instead, any refining process sends to its neighbors,
and every process posts a barrier.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ibarrierpost" aria-expanded="false" aria-controls="ibarrierpost">
        C Code: ibarrierpost
      </button>
    </h5>
  </div>
  <div id="ibarrierpost" class="collapse">
  <pre>
// ibarrierprobe.c
if (i_do_send) {
  /*
   * Pick a random process to send to,
   * not yourself.
   */
  int receiver = rand()%nprocs;
  MPI_Ssend(&data,1,MPI_FLOAT,receiver,0,comm);
}
/*
 * Everyone posts the non-block barrier
 * and gets a request to test/wait for
 */
MPI_Request barrier_request;
MPI_Ibarrier(comm,&barrier_request);
</pre>
</div>
</div>
<p name="switchToTextMode">

Now every process alternately probes for messages
and tests for completion of the barrier.
Probing is done through the nonblocking 
<tt>MPI_Iprobe</tt>
 call,
while testing completion of the barrier is done through
<tt>MPI_Test</tt>
.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ibarrierpoll" aria-expanded="false" aria-controls="ibarrierpoll">
        C Code: ibarrierpoll
      </button>
    </h5>
  </div>
  <div id="ibarrierpoll" class="collapse">
  <pre>
for ( ; ; step++) {
  int barrier_done_flag=0;
  MPI_Test(&barrier_request,&barrier_done_flag,
           MPI_STATUS_IGNORE);
//stop if you're done!
  if (barrier_done_flag) {
    break;
  } else {
// if you're not done with the barrier:
    int flag; MPI_Status status;
    MPI_Iprobe
      ( MPI_ANY_SOURCE,MPI_ANY_TAG,
        comm, &flag, &status );
    if (flag) {
// absorb message!
</pre>
</div>
</div>
<p name="switchToTextMode">

We can use a nonblocking barrier to good effect, utilizing the idle
time that would result from a blocking barrier. In the following code
fragment processes test for completion of the barrier, and failing to
detect such completion, perform some local work.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ibarrierwork" aria-expanded="false" aria-controls="ibarrierwork">
        C Code: ibarrierwork
      </button>
    </h5>
  </div>
  <div id="ibarrierwork" class="collapse">
  <pre>
// findbarrier.c
MPI_Request final_barrier;
MPI_Ibarrier(comm,&final_barrier);

int global_finish=mysleep;
do {
  int all_done_flag=0;
  MPI_Test(&final_barrier,&all_done_flag,MPI_STATUS_IGNORE);
  if (all_done_flag) {
    break;
  } else {
    int flag; MPI_Status status;
// force progress
    MPI_Iprobe
      ( MPI_ANY_SOURCE,MPI_ANY_TAG,
        comm, &flag, MPI_STATUS_IGNORE );
    printf("[%d] going to work for another second\n",procid);
    sleep(1);
    global_finish++;
  }
} while (1);
</pre>
</div>
</div>
<p name="switchToTextMode">

<h2><a id="Performanceofcollectives">3.12</a> Performance of collectives</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Performanceofcollectives">Performance of collectives</a>
</p>

</p>

<p name="switchToTextMode">
It is easy to visualize a broadcast as in figure&nbsp;
3.10
:
see figure&nbsp;
3.10
.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bcast-simple.jpeg" width=800></img>
<p name="caption">
FIGURE 3.10: A simple broadcast
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
the root sends all of its data directly to every other process.
While this describes the semantics of the operation, in practice
the implementation works quite differently.
</p>

<p name="switchToTextMode">
The time that a message takes can simply be modeled as
\[
 \alpha +\beta n, 
\]
where $\alpha$&nbsp;is the 
<i>latency</i>
, a&nbsp;one time
delay from establishing the communication between two processes,
and $\beta$&nbsp;is the time-per-byte, or the inverse of the 
<i>bandwidth</i>
,
and $n$&nbsp;the number of bytes sent.
</p>

<p name="switchToTextMode">
Under the assumption that
a processor can only send one message at a time,
the broadcast in
figure&nbsp;
3.10
 would take a time proportional to the
number of processors.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  What is the total time required for a broadcast involving $p$
  processes?
  Give $\alpha$ and $\beta$ terms separately.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

One way to ameliorate that is to structure the
broadcast in a tree-like fashion.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bcast-tree.jpeg" width=800></img>
<p name="caption">
FIGURE 3.11: A tree-based broadcast
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
This is depicted in figure&nbsp;
3.11
.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  How does the
  communication time now depend on the number of processors, again
  $\alpha$ and $\beta$ terms separately.
</p>

<p name="switchToTextMode">
  What would be a lower bound on the $\alpha,\beta$ terms?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

The theory
of the complexity of collectives is described in more detail in
<i>Eijkhout:IntroHPC</i>
; see also&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/bibliography.html#Chan2007Collective">[Chan2007Collective]</a>
.
</p>

<h2><a id="Collectivesandsynchronization">3.13</a> Collectives and synchronization</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Collectivesandsynchronization">Collectives and synchronization</a>
</p>

<p name="switchToTextMode">

Collectives, other than a barrier, have a synchronizing effect between processors.
For instance, in
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
MPI_Bcast( ....data... root);
MPI_Send(....);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
the send operations on all processors will occur after the root executes
the broadcast.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/reduce-two-node.png" width=800></img>
<p name="caption">
FIGURE 3.12: Trace of a reduction operation between two dual-socket 12-core nodes
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Conversely, in a reduce operation the root may have to wait for
other processors. This is illustrated in figure&nbsp;
3.12
, which
gives a TAU trace of
a reduction operation on two nodes, with two six-core sockets (processors) each.
We see that\footnote
{This uses mvapich version 1.6; in version 1.9 the implementation of an on-node reduction
has changed to simulate shared memory.}:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
In each socket, the reduction is a linear accumulation;
<li>
on each node, cores zero and six then combine their result;
<li>
after which the final accumulation is done through the network.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
We also see that the two nodes are not perfectly in sync, which is normal for MPI
applications. As a result, core&nbsp;0 on the first node will sit idle until it receives the partial
result from core&nbsp;12, which is on the second node.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<p name="switchToTextMode">
  \footnotesize
  \leftskip=\unitindent
  \rightskip=\unitindent
The most logical execution is:\par\medskip
</p>

<img src="graphics/backintime1.png" width=800></img>
<p name="switchToTextMode">

However, this ordering is allowed too:\par\medskip
</p>

<img src="graphics/backintime2.png" width=800></img>
<p name="switchToTextMode">

Which looks from a distance like:\par\medskip
</p>

<img src="graphics/backintime3.png" width=800></img>
<p name="switchToTextMode">

In other words, one of the messages seems to go `back in time'.
</p>

<p name="caption">
FIGURE 3.13: Possible temporal orderings of send and collective calls
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

While collectives synchronize in a loose sense, it is not possible to
make any statements about events before and after the collectives
between processors:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
...event 1...
MPI_Bcast(....);
...event 2....
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Consider a specific scenario:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
switch(rank) {
    case 0:
        MPI_Bcast(buf1, count, type, 0, comm);
        MPI_Send(buf2, count, type, 1, tag, comm);
        break;
    case 1:
        MPI_Recv(buf2, count, type, MPI_ANY_SOURCE, tag, comm, &status);
        MPI_Bcast(buf1, count, type, 0, comm);
        MPI_Recv(buf2, count, type, MPI_ANY_SOURCE, tag, comm, &status);
        break;
    case 2:
        MPI_Send(buf2, count, type, 1, tag, comm);
        MPI_Bcast(buf1, count, type, 0, comm);
        break;
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Note the 
<tt>MPI_ANY_SOURCE</tt>
 parameter in the receive calls on processor&nbsp;1.
One obvious execution of this would be:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
The send from&nbsp;2 is caught by processor&nbsp;1;
<li>
Everyone executes the broadcast;
<li>
The send from&nbsp;0 is caught by processor&nbsp;1.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
However, it is equally possible to have this execution:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Processor&nbsp;0 starts its broadcast, then executes the send;
<li>
Processor&nbsp;1's receive catches the data from&nbsp;0, then it executes
  its part of the broadcast;
<li>
Processor&nbsp;1 catches the data sent by&nbsp;2, and finally processor&nbsp;2
  does its part of the broadcast.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

This is illustrated in figure&nbsp;
3.13
.
</p>

<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<h2><a id="Performanceconsiderations">3.14</a> Performance considerations</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Performanceconsiderations">Performance considerations</a>
</p>
</p>

<p name="switchToTextMode">
In this section we will consider how collectives can be implemented in
multiple ways, and the performance implications of such decisions.
You can test the algorithms described here using 
<i>SimGrid</i>
(section~
).
</p>

<h3><a id="Scalability">3.14.1</a> Scalability</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Performanceconsiderations">Performance considerations</a> > <a href="mpi-collective.html#Scalability">Scalability</a>
</p>
<p name="switchToTextMode">

We are motivated to write parallel software from two considerations.
First of all, if we have a certain problem to solve which normally
takes time~$T$, then we hope that with $p$ processors it will take time~$T/p$.
If this is true, we call our parallelization scheme 
<i>scalable in time</i>
.
In practice, we often accept small extra terms:
as you will see below, parallelization often adds a term $\log_2p$
to the running time.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Discuss scalability of the following algorithms:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
    You have an array of floating point numbers.
    You need to compute the sine of each
<li>
You a two-dimensional array, denoting the interval~$[-2,2]^2$.
    You want to make a picture of the 
<i>Mandelbrot set</i>
,
    so you need to compute the color of each point.
<li>
The primality test of exercise~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-functional.html#Functionalparallelism">2.4</a>
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

There is also the notion that a parallel algorithm can be
<i>scalable in space</i>
: more processors gives you more memory
so that you can run a larger problem.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Discuss space scalability in the context of modern processor design.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Complexityandscalabilityofcollectives">3.14.2</a> Complexity and scalability of collectives</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Performanceconsiderations">Performance considerations</a> > <a href="mpi-collective.html#Complexityandscalabilityofcollectives">Complexity and scalability of collectives</a>
</p>
</p>

<h4><a id="Broadcast">3.14.2.1</a> Broadcast</h4>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Performanceconsiderations">Performance considerations</a> > <a href="mpi-collective.html#Complexityandscalabilityofcollectives">Complexity and scalability of collectives</a> > <a href="mpi-collective.html#Broadcast">Broadcast</a>
</p>
<p name="switchToTextMode">


<b>Naive broadcast</b><br>

</p>

<p name="switchToTextMode">
Write a broadcast operation where the root does an 
<tt>MPI_Send</tt>
 to
each other process.
</p>

<p name="switchToTextMode">
What is the expected performance of this in terms of $\alpha,\beta$?
</p>

<p name="switchToTextMode">
Run some tests and confirm.
</p>

<p name="switchToTextMode">

<b>Simple ring</b><br>

</p>

<p name="switchToTextMode">
Let the root only send to the next process, and that one send to its
neighbor. This scheme is known as a 
<i>bucket brigade</i>
; see
also section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-ptp.html#Bucketbrigade">4.1.5</a>
.
</p>

<p name="switchToTextMode">
What is the expected performance of this in terms of $\alpha,\beta$?
</p>

<p name="switchToTextMode">
Run some tests and confirm.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/bucketpipe.png" width=800></img>
<p name="caption">
FIGURE 3.14: A pipelined bucket brigade
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">


<b>Pipelined ring</b><br>

</p>

<p name="switchToTextMode">
In a ring broadcast, each process needs to receive the whole message
before it can pass it on. We can increase the efficiency by breaking
up the message and sending it in multiple parts.
(See figure&nbsp;
3.14
.)
This will be
advantageous for messages that are long enough that the bandwidth cost
dominates the latency.
</p>

<p name="switchToTextMode">
Assume a send buffer of length more than&nbsp;1. Divide the send buffer
into a number of chunks. The root sends the chunks successively to the
next process, and each process sends on whatever chunks it receives.
</p>

<p name="switchToTextMode">
What is the expected performance of this in terms of $\alpha,\beta$?
Why is this better than the simple ring?
</p>

<p name="switchToTextMode">
Run some tests and confirm.
</p>

<p name="switchToTextMode">

<b>Recursive doubling</b><br>

</p>

<p name="switchToTextMode">
Collectives such as broadcast can be
<i>implemented</i>
through
<!-- index -->
<i>recursive doubling</i>
,
where the root sends to another process, then the root and the other
process send to two more, those four send to four more, et cetera.
However, in an actual physical architecture this scheme can be
realized in multiple ways that have drastically different performance.
</p>

<p name="switchToTextMode">
First consider the implementation where process&nbsp;0 is the root, and it
starts by sending to process&nbsp;1; then they send to 2 and&nbsp;3; these four
send to&nbsp;4--7, et cetera. If the architecture is a linear array of
procesors, this will lead to 
<i>contention</i>
: multiple messages
wanting to go through the same wire. (This is also related to the
concept of 
<i>bisecection bandwidth</i>
.)
</p>

<p name="switchToTextMode">
In the following analyses we will assume 
<i>wormhole routing</i>
:
a&nbsp;message sets up a path through the network, reserving the necessary
wires, and performing a send in time independent of the distance
through the network. That is, the send time for any message can be
modeled as 
\[
 T(n)=\alpha+\beta n 
\]
 regardless source and
destination, as long as the necessary connections are available.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Analyze the running time of a recursive doubling broad cast as just
  described, with wormhole routing.
</p>

<p name="switchToTextMode">
  Implement this broadcast in terms of blocking MPI send and receive calls.
  If you have SimGrid available, run
  tests with a number of parameters.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

The alternative, that avoids contention, is to let each doubling stage
divide the network into separate halves. That is, process&nbsp;0 sends
to&nbsp;$P/2$, after which these two repeat the algorithm in the two halves
of the network, sending to $P/4$ and $3P/4$ respectively.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Analyze this variant of recursive doubling. Code it and measure
  runtimes on SimGrid.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Revisit exercise 
3.14
 and replace the
  blocking calls by nonblocking 
<tt>MPI_Isend</tt>
&nbsp;/ 
<tt>MPI_Irecv</tt>
 calls.
</p>

<p name="switchToTextMode">
  Make sure to test that the data is correctly propagated.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

MPI implementations often have multiple algorithms, which they
dynamicaly switch between. Sometimes you can determine the choice yourself
through environment variables.
</p>

<!-- environment: taccnote start embedded generator -->
<!-- environment block purpose: [[ environment=taccnote ]] -->
<remark>
<b>TACC note</b>
<p name="remark">
<!-- TranslatingLineGenerator taccnote ['taccnote'] -->
  For 
<i>Intel MPI</i>
, see
  
<a href=https://software.intel.com/en-us/mpi-developer-reference-linux-i-mpi-adjust-family-environment-variables>https://software.intel.com/en-us/mpi-developer-reference-linux-i-mpi-adjust-family-environment-variables</a>
.
</p name="remark">
</remark>
<!-- environment: taccnote end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Reviewquestions">3.15</a> Review questions</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-collective.html">mpi-collective</a> > <a href="mpi-collective.html#Reviewquestions">Review questions</a>
</p>
</p>

<p name="switchToTextMode">
For all true/false questions, if you answer that a statement is false,
give a one-line explanation.
</p>

<!-- environment: review start embedded generator -->
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  How would you realize the following scenarios with MPI collectives?
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Let each process compute a random number. You want to print the
    maximum of these numbers to your screen.
<li>
Each process computes a random number again. Now you want to
    scale these numbers by their maximum.
<li>
Let each process compute a random number. You want to print on what processor the
    maximum value is computed.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  MPI collectives can be sorted in at least the following categories
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
rooted vs rootless
<li>
using uniform buffer lengths vs variable length buffers
<li>
blocking vs nonblocking.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
  Give examples of each type.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  True or false: collective routines are all about
  communicating user data between the processes.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  True or false: an 
<tt>MPI_Scatter</tt>
 call puts the same data on
  each process.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  True or false: using the option 
<tt>MPI_IN_PLACE</tt>
 you
  only need space for a send buffer in 
<tt>MPI_Reduce</tt>
.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  True or false: using the option 
<tt>MPI_IN_PLACE</tt>
 you
  only need space for a send buffer in 
<tt>MPI_Gather</tt>
.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  Given a distributed array, with every processor storing
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
double x[N]; // N can vary per processor
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
  give the approximate MPI-based code that computes the maximum value
  in the array, and leaves the result on every processor.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
double data[Nglobal];
int myfirst = /* something */, mylast = /* something */;
for (int i=myfirst; i&lt;mylast; i++) {
  if (i&gt;0 && i&lt;N-1) {
    process_point( data,i,Nglobal );
  }
}
void process_point( double *data,int i,int N ) {
    data[i-1] = g(i-1); data[i] = g(i); data[i+1] = g(i+1);
    data[i] = f(data[i-1],data[i],data[i+1]);
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Is this scalable in time? Is this scalable in space?
What is the missing MPI call?
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
double data[Nlocal+2]; // include left and right neighbor
int myfirst = /* something */, mylast = myfirst+Nlocal;
for (int i=0; i&lt;Nlocal; i++) {
  if (i&gt;0 && i&lt;N-1) {
    process_point( data,i,Nlocal );
}
void process_point( double *data,int i0,int n ) {
  int i = i0+1;
  data[i-1] = g(i-1); data[i] = g(i); data[i+1] = g(i+1);
  data[i] = f(data[i-1],data[i],data[i+1]);
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Is this scalable in time? Is this scalable in space?
What is the missing MPI call?
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  With data as in the previous question, given the code for
  normalizing the array, that is, scaling each element so that $\|x\|_2=1$.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  Just like 
<tt>MPI_Allreduce</tt>
 is equivalent to
<tt>MPI_Reduce</tt>
 following by 
<tt>MPI_Bcast</tt>
,
<tt>MPI_Reduce_scatter</tt>
 is equivalent to at least
  one of the following combinations. Select those that are equivalent,
  and discuss differences in time or space complexity:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
<tt>MPI_Reduce</tt>
 followed by 
<tt>MPI_Scatter</tt>
;
<li>
<tt>MPI_Gather</tt>
 followed by 
<tt>MPI_Scatter</tt>
;
<li>
<tt>MPI_Allreduce</tt>
 followed by 
<tt>MPI_Scatter</tt>
;
<li>
<tt>MPI_Allreduce</tt>
 followed by a local operation (which?);
<li>
<tt>MPI_Allgather</tt>
 followed by a local operation (which?).
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
</review>
<!-- environment: review end embedded generator -->
<p name="switchToTextMode">

<!-- environment: review start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=review ]] -->
<review>
<b>Review</b>
<p name="review">
<!-- TranslatingLineGenerator review ['review'] -->
  Think of at least two algorithms for doing a broadcast.
  Compare them with regards to asymptotic behavior.
</p name="review">
</review>
<!-- environment: review end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
