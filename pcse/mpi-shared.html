<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src=https://ccrs.cac.cornell.edu:8443/client.0.2.js></script>
<script id="script">
class Example {
  constructor(buttonID, editorID, outputID, sourceFile, fileName, commandStr) {
    this.buttonID = buttonID;
    this.editorID = editorID;
    this.outputID = outputID;
    this.sourceFile = sourceFile;
    this.fileName = fileName;
    this.commandStr = commandStr;
  }
    
  async display(results, object) {
    if (results.stdout.length > 0)
      document.getElementById(object.outputID).textContent = results.stdout;
    else
      document.getElementById(object.outputID).textContent = results.stderr;
  }

  async initialize() {
    this.editor = await MonacoEditorFileSource.create(this.editorID);
    this.editor.setTextFromFile(this.sourceFile);
    this.job = await Job.create(JobType.MPI);
    this.command = new CommandWithFiles(this.job, this.commandStr);
    this.command.addFileSource(this.editor, this.fileName);
    this.trigger = new ButtonTrigger(this.command, this.display, this.buttonID, this);
    document.getElementById(this.buttonID).disabled = false;
  }
}
</script>
<style></style>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>MPI topic: Shared memory</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


12.1 : <a href="mpi-shared.html#Recognizingsharedmemory">Recognizing shared memory</a><br>
12.2 : <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a><br>
12.2.1 : <a href="mpi-shared.html#Pointerstoasharedwindow">Pointers to a shared window</a><br>
12.2.2 : <a href="mpi-shared.html#Queryingthesharedstructure">Querying the shared structure</a><br>
12.2.3 : <a href="mpi-shared.html#Heatequationexample">Heat equation example</a><br>
12.2.4 : <a href="mpi-shared.html#Sharedbulkdata">Shared bulk data</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>12 MPI topic: Shared memory</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">
Some programmers are under the impression that MPI would not be efficient on
shared memory, since all operations are done through what looks like network calls.
This is not correct: many MPI
implementations have optimizations that detect shared memory and can
exploit it, so that data is copied, rather than going through a communication layer.
(Conversely, programming systems for shared memory such as 
<i>OpenMP</i>
can actually have inefficiencies associated with thread handling.)
The main inefficiency associated with using MPI on shared memory is then
that processes can not actually share data.
</p>

<p name="switchToTextMode">
The one-sided MPI calls (chapter~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html">MPI topic: One-sided communication</a>
) can also be used to
emulate shared memory, in the sense that an origin process can access data
from a target process without the target's active involvement.
However, these calls do not distinguish between actually shared memory
and one-sided access across the network.
</p>

<p name="switchToTextMode">
In this chapter we will look at the ways MPI
can interact with the presence of actual shared memory.
(This functionality was added in the \mpistandard{3} standard.)
This relies on the 
<tt>MPI_Win</tt>
 windows concept, but
otherwise uses direct access of other processes' memory.
</p>

<h2><a id="Recognizingsharedmemory">12.1</a> Recognizing shared memory</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Recognizingsharedmemory">Recognizing shared memory</a>
</p>

<p name="switchToTextMode">

MPI's one-sided routines take a very symmetric view of processes:
each process can access the window of every other process (within a communicator).
Of course, in practice there will be a difference in performance
depending on whether the origin and target are actually
on the same shared memory, or whether they can only communicate through the network.
For this reason MPI makes it easy to group processes by shared memory domains
using 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Comm_split_type" aria-expanded="false" aria-controls="MPI_Comm_split_type">
        Routine reference: MPI_Comm_split_type
      </button>
    </h5>
  </div>
  <div id="MPI_Comm_split_type" class="collapse">
  <pre>
C:
int MPI_Comm_split_type(
  MPI_Comm comm, int split_type, int key,
  MPI_Info info, MPI_Comm *newcomm)

Fortran:
MPI_Comm_split_type(comm, split_type, key, info, newcomm, ierror)
TYPE(MPI_Comm), INTENT(IN) :: comm
INTEGER, INTENT(IN) :: split_type, key
TYPE(MPI_Info), INTENT(IN) :: info
TYPE(MPI_Comm), INTENT(OUT) :: newcomm
INTEGER, OPTIONAL, INTENT(OUT) :: ierror

Python:
MPI.Comm.Split_type(
  self, int split_type, int key=0, Info info=INFO_NULL)
</pre>
</div>
</div>
<i>MPI_Comm_split_type</i>
.
</p>

<p name="switchToTextMode">
Here the 
<tt>split_type</tt>
 parameter has to be from the following (short) list:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
<tt>MPI_COMM_TYPE_SHARED</tt>
: split the communicator into subcommunicators
  of processes sharing a memory area.
\begin{mpifournote}
{Split by guided types}
<li>
<tt>MPI_COMM_TYPE_HW_GUIDED</tt>
 (\mpistandard{4}):
    split using an 
<tt>info</tt>
 value from 
<tt>MPI_Get_hw_resource_types</tt>
.
<li>
<tt>MPI_COMM_TYPE_HW_UNGUIDED</tt>
 (\mpistandard{4}):
    similar to 
<tt>MPI_COMM_TYPE_HW_GUIDED</tt>
, but the resulting communicators
    should be a strict subset of the original communicator.
    On processes where this condition can not be fullfilled,
<tt>MPI_COMM_NULL</tt>
 will be returned.
\end{mpifournote}
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

Splitting by shared memory:
\csnippetwithoutput{commsplittype}{examples/mpi/c}{commsplittype}
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Write a program that uses 
<tt>MPI_Comm_split_type</tt>
  to  analyze for a run
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
How many nodes there are;
<li>
How many processes there are on each node.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
  If you run this program on an unequal distribution,
  say 10~processes on 3~nodes, what distribution do you find?
  Is that $4+4+2$ or $4+3+3$?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  The 
<i>OpenMPI</i>
 implementation of MPI has a number of
  non-standard split types, such as 
<tt>OMPI_COMM_TYPE_SOCKET</tt>
;
  see 
<a href=https://www.open-mpi.org/doc/v4.1/man3/MPI_Comm_split_type.3.php>https://www.open-mpi.org/doc/v4.1/man3/MPI_Comm_split_type.3.php</a>

</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<!-- environment: mplnote start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=mplnote ]] -->
<remark>
<b>MPL note</b>
<!-- TranslatingLineGenerator mplnote ['mplnote'] -->
<p name="switchToTextMode">
  Similar to ordinary communicator splitting~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-comm.html#Splittingacommunicator">7.4</a>
:
   <tt>communicator::</tt> 
<tt>split_shared</tt>
<i>End of MPL note</i>
</remark>
<!-- environment: mplnote end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Sharedmemoryforwindows">12.2</a> Shared memory for windows</h2>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a>
</p>
</p>

<p name="switchToTextMode">
Processes that exist on the same physical shared memory should be able
to move data by copying, rather than through MPI send/receive calls
--~which of course will do a copy operation under the hood.
In order to do such user-level copying:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
We need to create a shared memory area with
<tt>MPI_Win_allocate_shared</tt>
.
  This creates a window with the
  
<i>unified memory model</i>

<!-- index -->
;
  and
<li>
We need to get pointers to where a process' area is in this
  shared space; this is done with 
<tt>MPI_Win_shared_query</tt>
.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Pointerstoasharedwindow">12.2.1</a> Pointers to a shared window</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Pointerstoasharedwindow">Pointers to a shared window</a>
</p>
</p>

<p name="switchToTextMode">
The first step is to create a window (in the sense of one-sided MPI;
section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html#Windows">9.1</a>
) on the processes on one node.
Using the 
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Win_allocate_shared" aria-expanded="false" aria-controls="MPI_Win_allocate_shared">
        Routine reference: MPI_Win_allocate_shared
      </button>
    </h5>
  </div>
  <div id="MPI_Win_allocate_shared" class="collapse">
  <pre>
Semantics:
MPI_WIN_ALLOCATE_SHARED(size, disp_unit, info, comm, baseptr, win)

Input parameters:
size: size of local window in bytes (non-negative integer)
disp_unit local unit size for displacements, in bytes (positive
integer)
info: info argument (handle)
comm: intra-communicator (handle)

Output parameters:
baseptr: address of local allocated window segment (choice)
win: window object returned by the call (handle)

C:
int MPI_Win_allocate_shared
   (MPI_Aint size, int disp_unit, MPI_Info info,
    MPI_Comm comm, void *baseptr, MPI_Win *win)

Fortran:
MPI_Win_allocate_shared
   (size, disp_unit, info, comm, baseptr, win, ierror)
USE, INTRINSIC :: ISO_C_BINDING, ONLY : C_PTR
INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: size
INTEGER, INTENT(IN) :: disp_unit
TYPE(MPI_Info), INTENT(IN) :: info
TYPE(MPI_Comm), INTENT(IN) :: comm
TYPE(C_PTR), INTENT(OUT) :: baseptr
TYPE(MPI_Win), INTENT(OUT) :: win
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
<i>MPI_Win_allocate_shared</i>
 call presumably will
put the memory close to the
<i>socket</i>
 on which the process runs.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpisharedwindow" aria-expanded="false" aria-controls="mpisharedwindow">
        C Code: mpisharedwindow
      </button>
    </h5>
  </div>
  <div id="mpisharedwindow" class="collapse">
  <pre>
// sharedbulk.c
MPI_Win node_window;
MPI_Aint window_size; double *window_data;
if (onnode_procid==0)
  window_size = sizeof(double);
else window_size = 0;
MPI_Win_allocate_shared
  ( window_size,sizeof(double),MPI_INFO_NULL,
    nodecomm,
    &window_data,&node_window);
</pre>
</div>
</div>
<p name="switchToTextMode">

The memory allocated by 
<tt>MPI_Win_allocate_shared</tt>
 is
contiguous between the processes. This makes it possible to do address
calculation. However, if a cluster node has a 
<span title="acronym" ><i>NUMA</i></span>
 structure, for
instance if two sockets have memory directly attached to each, this
would increase latency for some processes. To prevent this, the key
<tt>alloc_shared_noncontig</tt>
 can be set to 
<tt>true</tt>
 in the
<tt>MPI_Info</tt>
 object.
\begin{mpifournote}
{Window memory alignment}
  In the contiguous case, the 
<tt>mpi_minimum_memory_alignment</tt>
  info argument
  (section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-onesided.html#Windowcreationandallocation">9.1.1</a>
)
  applies only to the memory on the first process;
  in the noncontiguous case it applies to all.
\end{mpifournote}
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#winnoncontig" aria-expanded="false" aria-controls="winnoncontig">
        C Code: winnoncontig
      </button>
    </h5>
  </div>
  <div id="winnoncontig" class="collapse">
  <pre>
// numa.c
MPI_Info window_info;
MPI_Info_create(&window_info);
  MPI_Info_set(window_info,"alloc_shared_noncontig","true");
MPI_Win_allocate_shared( window_size,sizeof(double),window_info,
                         nodecomm,
                         &window_data,&node_window);
MPI_Info_free(&window_info);
</pre>
</div>
</div>
<p name="switchToTextMode">

Let's explore this.
We create a shared window where each process stores exactly one double,
that is, 8~bytes.
The following code fragment queries the window locations,
and prints the distance in bytes to the window on process~0.
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#wincontigquery" aria-expanded="false" aria-controls="wincontigquery">
        C Code: wincontigquery
      </button>
    </h5>
  </div>
  <div id="wincontigquery" class="collapse">
  <pre>
for (int p=1; p&lt;onnode_nprocs; p++) {
  MPI_Aint window_sizep; int windowp_unit; double *winp_addr;
  MPI_Win_shared_query( node_window,p,
                        &window_sizep,&windowp_unit, &winp_addr );
  distp = (size_t)winp_addr-(size_t)win0_addr;
  if (procno==0)
    printf("Distance %d to zero: %ld\n",p,(long)distp);
</pre>
</div>
</div>
</p>

<p name="switchToTextMode">
With the default strategy, these windows are contiguous,
and so the distances are multiples of~8 bytes.
Not so for the the non-contiguous allocation:
<!-- environment: multicols start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=multicols ]] -->
<multicols>

<p name="multicols">
<!-- TranslatingLineGenerator multicols ['multicols'] -->
Strategy: default behavior of shared window allocation
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
Distance 1 to zero: 8
Distance 2 to zero: 16
Distance 3 to zero: 24
Distance 4 to zero: 32
Distance 5 to zero: 40
Distance 6 to zero: 48
Distance 7 to zero: 56
Distance 8 to zero: 64
Distance 9 to zero: 72
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

Strategy: allow non-contiguous shared window allocation
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
Distance 1 to zero: 4096
Distance 2 to zero: 8192
Distance 3 to zero: 12288
Distance 4 to zero: 16384
Distance 5 to zero: 20480
Distance 6 to zero: 24576
Distance 7 to zero: 28672
Distance 8 to zero: 32768
Distance 9 to zero: 36864
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
</multicols>
<!-- environment: multicols end embedded generator -->
<p name="switchToTextMode">

The explanation here is that each window is placed
on its own 
<i>small page</i>
,
which on this particular system has a size of&nbsp;4K.
</p>

<!-- environment: remark start embedded generator -->
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  The ampersand operator in&nbsp;C is not a
<i>physical address</i>
, but a
<i>virtual address</i>
.
  The translation of where pages are placed in physical memory
  is determined by the 
<i>page table</i>
.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Queryingthesharedstructure">12.2.2</a> Querying the shared structure</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Queryingthesharedstructure">Querying the shared structure</a>
</p>
</p>

<p name="switchToTextMode">
Even though the window created above is shared, that doesn't mean it's
contiguous. Hence it is necessary to retrieve the pointer to the area
of each process that you want to communicate with:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#MPI_Win_shared_query" aria-expanded="false" aria-controls="MPI_Win_shared_query">
        Routine reference: MPI_Win_shared_query
      </button>
    </h5>
  </div>
  <div id="MPI_Win_shared_query" class="collapse">
  <pre>
Semantics:
MPI_WIN_SHARED_QUERY(win, rank, size, disp_unit, baseptr)

Input arguments:
win:  shared memory window object (handle)
rank: rank in the group of window win (non-negative integer)
      or MPI_PROC_NULL

Output arguments:
size: size of the window segment (non-negative integer)
disp_unit: local unit size for displacements,
           in bytes (positive integer)
baseptr: address for load/store access to window segment (choice)

C:
int MPI_Win_shared_query
   (MPI_Win win, int rank, MPI_Aint *size, int *disp_unit,
    void *baseptr)

Fortran:
MPI_Win_shared_query(win, rank, size, disp_unit, baseptr, ierror)
USE, INTRINSIC :: ISO_C_BINDING, ONLY : C_PTR
TYPE(MPI_Win), INTENT(IN) :: win
INTEGER, INTENT(IN) :: rank
INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(OUT) :: size
INTEGER, INTENT(OUT) :: disp_unit
TYPE(C_PTR), INTENT(OUT) :: baseptr
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
</pre>
</div>
</div>
<i>MPI_Win_shared_query</i>
.
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#mpisharedpointer" aria-expanded="false" aria-controls="mpisharedpointer">
        C Code: mpisharedpointer
      </button>
    </h5>
  </div>
  <div id="mpisharedpointer" class="collapse">
  <pre>
MPI_Aint window_size0; int window_unit; double *win0_addr;
MPI_Win_shared_query
  ( node_window,0,
    &window_size0,&window_unit, &win0_addr );
</pre>
</div>
</div>
<p name="switchToTextMode">

<h3><a id="Heatequationexample">12.2.3</a> Heat equation example</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Heatequationexample">Heat equation example</a>
</p>
</p>

<p name="switchToTextMode">
As an example, which consider the 1D heat equation. On each process we
create a local area of three point:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#allocateshared3pt" aria-expanded="false" aria-controls="allocateshared3pt">
        C Code: allocateshared3pt
      </button>
    </h5>
  </div>
  <div id="allocateshared3pt" class="collapse">
  <pre>
// sharedshared.c
MPI_Win_allocate_shared(3,sizeof(int),info,sharedcomm,&shared_baseptr,&shared_window);
</pre>
</div>
</div>
</p>

<h3><a id="Sharedbulkdata">12.2.4</a> Shared bulk data</h3>
<p name=crumbs>
crumb trail:  > <a href="mpi-shared.html">mpi-shared</a> > <a href="mpi-shared.html#Sharedmemoryforwindows">Shared memory for windows</a> > <a href="mpi-shared.html#Sharedbulkdata">Shared bulk data</a>
</p>
<p name="switchToTextMode">

In applications such as 
<i>ray tracing</i>
, there is a read-only
large data object (the objects in the scene to be rendered) that is
needed by all processes. In traditional MPI, this would need to be
stored redundantly on each process, which leads to large memory
demands. With MPI shared memory we can store the data object once per
node. Using as above 
<tt>MPI_Comm_split_type</tt>
 to find a
communicator per 
<span title="acronym" ><i>NUMA</i></span>
 domain, we store the object on process zero
of this node communicator.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Let the `shared' data originate on process zero in
<tt>MPI_COMM_WORLD</tt>
. Then:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
create a communicator per shared memory domain;
<li>
create a communicator for all the processes with number zero on their
    node;
<li>
broadcast the shared data to the processes zero on each node.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<!-- skeleton start: shareddata -->
<button id="runBtnshareddata">Compile and run shareddata</button>
<div id="editorDivshareddata" 
     style="height:125px;border:1px solid black; 
     resize:vertical; overflow: hidden;"></div>
<pre id="outputPreshareddata"></pre>
<script name="defSkeletonshareddata">
let exampleshareddata = new Example("runBtnshareddata",
    "editorDivshareddata", "outputPreshareddata", 
    "skeletons/shareddata.c", 
    "mpicc skeletons/shareddata.c && mpiexec -n 4 ./a.out" );
exampleshareddata.initialize();
</script>
<!-- skeleton end: shareddata -->
</exercise>
<!-- environment: exercise end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
