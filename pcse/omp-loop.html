<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>

  <link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src=https://ccrs.cac.cornell.edu:8443/client.0.2.js></script>
<script id="script">
class Example {
  constructor(buttonID, editorID, outputID, sourceFile, fileName, commandStr) {
    this.buttonID = buttonID;
    this.editorID = editorID;
    this.outputID = outputID;
    this.sourceFile = sourceFile;
    this.fileName = fileName;
    this.commandStr = commandStr;
  }
    
  async display(results, object) {
    if (results.stdout.length > 0)
      document.getElementById(object.outputID).textContent = results.stdout;
    else
      document.getElementById(object.outputID).textContent = results.stderr;
  }

  async initialize() {
    this.editor = await MonacoEditorFileSource.create(this.editorID);
    this.editor.setTextFromFile(this.sourceFile);
    this.job = await Job.create(JobType.MPI);
    this.command = new CommandWithFiles(this.job, this.commandStr);
    this.command.addFileSource(this.editor, this.fileName);
    this.trigger = new ButtonTrigger(this.command, this.display, this.buttonID, this);
    document.getElementById(this.buttonID).disabled = false;
  }
}
</script>
<style></style>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>OpenMP topic: Loop parallelism</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


19.1 : <a href="omp-loop.html#Loopparallelism">Loop parallelism</a><br>
19.2 : <a href="omp-loop.html#Anexample">An example</a><br>
19.3 : <a href="omp-loop.html#Loopschedules">Loop schedules</a><br>
19.4 : <a href="omp-loop.html#Reductions">Reductions</a><br>
19.5 : <a href="omp-loop.html#Collapsingnestedloops">Collapsing nested loops</a><br>
19.6 : <a href="omp-loop.html#Orderediterations">Ordered iterations</a><br>
19.7 : <a href="omp-loop.html#<tt>nowait<tt>"><tt>nowait</tt></a><br>
19.8 : <a href="omp-loop.html#Whileloops">While loops</a><br>
19.9 : <a href="omp-loop.html#Scalingtests">Scaling tests</a><br>
19.9.1 : <a href="omp-loop.html#Lonestar6">Lonestar 6</a><br>
19.9.2 : <a href="omp-loop.html#Frontera">Frontera</a><br>
19.9.3 : <a href="omp-loop.html#Stampede2skylake">Stampede2 skylake</a><br>
19.9.4 : <a href="omp-loop.html#Stampede2KnightsLanding">Stampede2 Knights Landing</a><br>
19.9.5 : <a href="omp-loop.html#Longhorn">Longhorn</a><br>
19.10 : <a href="omp-loop.html#Reviewquestions">Review questions</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>19 OpenMP topic: Loop parallelism</h1>
<!-- TranslatingLineGenerator file ['file'] -->
<p name="switchToTextMode">

<h2><a id="Loopparallelism">19.1</a> Loop parallelism</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Loopparallelism">Loop parallelism</a>
</p>

</p>

<p name="switchToTextMode">
Loop parallelism is a very common type of parallelism in scientific
codes, so OpenMP has an easy mechanism for it.
OpenMP parallel loops are a first example of OpenMP `worksharing'
constructs (see section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-reduction.html#Reductionsandfloating-pointmath">20.5</a>
 for the full list):
constructs that take an amount of work and distribute it over the
available threads in a parallel region,
created with the 
<tt>parallel</tt>
 pragma.
</p>

<p name="switchToTextMode">
The parallel execution of a loop can be handled a number of different ways.
For instance, you can create a parallel region around the loop, and
adjust the loop bounds:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel
{
  int threadnum = omp_get_thread_num(),
    numthreads = omp_get_num_threads();
  int low = N*threadnum/numthreads,
    high = N*(threadnum+1)/numthreads;
  for (i=low; i&lt;high; i++)
    // do something with i
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
In effect, this is how you would parallelize a loop in MPI.
<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  What is an important difference between the resulting OpenMP and MPI code?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

A more natural option is to use the
<tt>for</tt>
 pragma:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel
#pragma omp for
for (i=0; i&lt;N; i++) {
  // do something with i
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
This has several advantages. For one, you don't have to calculate the loop bounds
for the threads yourself, but you can also tell OpenMP to assign the loop
iterations according to different schedules (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-loop.html#Loopschedules">19.3</a>
).
</p>

<!-- environment: fortrannote start embedded generator -->
<!-- environment block purpose: [[ environment=fortrannote ]] -->
<remark>
<b>Fortran note</b>
<p name="remark">
<!-- TranslatingLineGenerator fortrannote ['fortrannote'] -->
  The 
<tt>for</tt>
 pragma only exists in&nbsp;C;
  there is a correspondingly named 
<tt>do</tt>
 pragma in Fortran.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
$!omp parallel
$!omp do
  do i=1,N
    ! something with i
  end do
$!omp end do
$!omp end parallel
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</remark>
<!-- environment: fortrannote end embedded generator -->
<p name="switchToTextMode">
{OMP do pragma}
</p>

<p name="switchToTextMode">
Figure&nbsp;
19.1
 shows the execution on four threads of
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel
{
  code1();
#pragma omp for
  for (i=1; i&lt;=4*N; i++) {
    code2();
  }
  code3();
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
The code before and after the loop is executed identically
in each thread; the loop iterations are spread over the four threads.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/parallel-do.jpeg" width=800></img>
<p name="caption">
FIGURE 19.1: Execution of parallel code inside and outside a loop
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Note that the 
<tt>do</tt>
 and 
<tt>for</tt>

pragmas do not create a team of threads: they
take the team of threads that is active,
and divide the loop iterations over them.
This means that the 
<tt>omp for</tt>
 or 
<tt>omp do</tt>
 directive needs to be
inside a parallel region.
</p>

<p name="switchToTextMode">
As an illustration:
\csnippetwithoutput{ompparforthread}{examples/omp/c}{parfor}
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  What would happen in the above example if you increase the number of threads
  to be larger than the number of cores?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

It is also possible to have a combined

<tt>omp parallel for</tt>
 or 
<tt>omp parallel do</tt>
 directive.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel for
  for (i=0; .....
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  There are certain restrictions on the type of loop
  that can be executed in parallel.
  These arise from the fact that iterations need to be scheduled
  before the loop starts.
  Thus
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
the loop body is not allowed to change the loop variable:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
      for (int i=0; i&lt;N; ) {
        // something
        if (something)
        i++;
        else
        i += 2;
      }
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  Furthermore, you are not allowed to 
<tt>break</tt>

  out of a loop.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  The loop index needs to be an integer value
  for the loop to be parallelizable.
  Unsigned values are allowed as of OpenMP-.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  Compute $\pi$ by 
<i>numerical integration</i>
. We use the fact that $\pi$
  is the area of the unit circle, and we approximate this by computing
  the area of a quarter circle using 
<i>Riemann sums</i>
.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Let $f(x)=\sqrt{1-x^2}$ be the function that describes the
    quarter circle for $x=0&hellip; 1$;
<li>
Then we compute 
\[
 \pi/4\approx\sum_{i=0}^{N-1} \Delta x
    f(x_i) \qquad \hbox{where $x_i=i\Delta x$ and $\Delta x=1/N$} 
\]
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">
  Write a program for this, and parallelize it using OpenMP parallel
  for directives.
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Put a 
<tt>parallel</tt>
 directive around your loop. Does it still
    compute the right result? Does the time go down with the number of
    threads? (The answers should be no and no.)
<li>
Change the 
<tt>parallel</tt>
 to 
<tt>parallel for</tt>
 (or \n{parallel
    do}). Now is the result correct? Does execution speed up? (The
    answers should now be no and yes.)
<li>
Put a 
<tt>critical</tt>
 directive in front of the update. (Yes and
    very much no.)
<li>
Remove the 
<tt>critical</tt>
 and add a clause
<tt>reduction</tt>

<tt>(+:quarterpi)</tt>
 to the 
<tt>for</tt>
 directive.
    Now it should be correct and efficient.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
  Use different numbers of cores and compute the
  speedup you attain over the sequential computation. Is there a
  performance difference between the OpenMP code with 1&nbsp;thread and the
  sequential code?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: remark start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=remark ]] -->
<remark>
<b>Remark</b>
<p name="remark">
<!-- TranslatingLineGenerator remark ['remark'] -->
  In this exercise you may have seen the runtime go up a couple of times
  where you weren't expecting it. The issue here is 
    sharing}; see&nbsp;
<i>Eijkhout:IntroHPC</i>
 for more explanation.
</p name="remark">
</remark>
<!-- environment: remark end embedded generator -->
<p name="switchToTextMode">

There are some restrictions on the loop: basically, OpenMP needs to be
able to determine in advance how many iterations there will be.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
The loop can not contains 
<tt>break</tt>
, 
<tt>return</tt>
, 
<tt>exit</tt>
 statements, or
  
<tt>goto</tt>
 to a label outside the loop.
<li>
The 
<tt>continue</tt>
 (C) or 
<tt>cycle</tt>
 (F) statement is allowed.
<li>
The index update has to be an increment (or decrement) by a fixed amount.
<li>
The loop index variable is automatically private, and not changes to it
  inside the loop are allowed.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Anexample">19.2</a> An example</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Anexample">An example</a>
</p>
</p>

<p name="switchToTextMode">
To illustrate the speedup of perfectly parallel calculations,
we consider a simple code that applies the same calculation
to each element of an array.
</p>

<p name="switchToTextMode">
All tests are done on the 
<i>TACC Frontera</i>
 cluster,
which has dual-socket 
<i>Intel Cascade Lake</i>
 nodes,
with a total of 56 cores.
We control affinity by setting

<tt>OMP_PROC_BIND=true</tt>
.
</p>

<p name="switchToTextMode">
Here is the essential code fragment:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ompsineloop" aria-expanded="false" aria-controls="ompsineloop">
        C Code: ompsineloop
      </button>
    </h5>
  </div>
  <div id="ompsineloop" class="collapse">
  <pre>
// speedup.c
#pragma omp parallel for
      for (int ip=0; ip&lt;N; ip++) {
        for (int jp=0; jp&lt;M; jp++) {
          double f = sin( values[ip] );
          values[ip] = f;
        }
      }
</pre>
</div>
</div>
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Verify that the outer loop is parallel, but the inner one is not.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Compare the time for the sequential code and the single-threaded OpenMP code.
  Try different optimization levels, and different compilers if you have them.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
    Do you sometimes get a significant difference? What would be an explanation?
<li>
Does your compiler have a facility for generating optimization reports?
    For instance 
<tt>-qoptreport=5</tt>
 for the
    
<i>Intel compiler</i>

<!-- index -->
.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Now we investigate the influence of two parameters:
<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
the OpenMP thread count: while we have 56 cores, values larger than that are allowed; and
<li>
the size of the problem: the smaller the problem, the larger the relative overhead
  of creating and synchronizing the team of threads.
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">
We execute the above computation several times to even out effects of cache loading.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.2: Speedup as function of problem size
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The results are in figure&nbsp;
19.2
:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
While the problem size is always larger than the number of threads,
  only for the largest problem,
  which has at least 400 points per thread,
  is the speedup essentially linear.
<li>
OpenMP allows for the number of threads to be larger than the core count,
  but there is no performance improvement in doing so.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.3: Speedup on a hyper-threaded architecture
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The above tests did not use 
<i>hyperthread</i>
s,
since that is disabled on Frontera.
However, the 
<i>Intel Knights Landing</i>
 nodes
of the 
<i>TACC Stampede2</i>
 cluster
have four 
<i>hyperthread</i>
s per core.
Table&nbsp;
19.3
 shows that this will indeed give a modest speedup.
</p>

<p name="switchToTextMode">
For reference, the commandlines executed were:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
# frontera
  make localclean run_speedup EXTRA_OPTIONS=-DN=200 NDIV=8 NP=112
  make localclean run_speedup EXTRA_OPTIONS=-DN=2000 NDIV=8 NP=112
  make localclean run_speedup EXTRA_OPTIONS=-DN=20000 NDIV=8 NP=112
# stampede2
  make localclean run_speedup NDIV=8 EXTRA_OPTIONS="-DN=200000 -DM=1000" NP=272
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

\begin{cppnote}
  Parallel loops in C++ can use range-based syntax:
<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ompcxxloop" aria-expanded="false" aria-controls="ompcxxloop">
        C++ Code: ompcxxloop
      </button>
    </h5>
  </div>
  <div id="ompcxxloop" class="collapse">
  <pre>
// speedup.cxx
#pragma omp parallel for
for ( auto& v : values ) {
  for (int jp=0; jp&lt;M; jp++) {
    double f = sin( v );
    v = f;
  }
}
</pre>
</div>
</div>
  Tests not reported here show exactly the same speedup as the C&nbsp;code.
\end{cppnote}
</p>

<h2><a id="Loopschedules">19.3</a> Loop schedules</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Loopschedules">Loop schedules</a>
</p>

<p name="switchToTextMode">

Usually you will have many more iterations in a loop than there are threads.
Thus, there are several ways you can assign your loop iterations to the threads.
OpenMP lets you specify this with the 
<tt>schedule</tt>
 clause.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp for schedule(....)
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

The first distinction we now have to make is between static and dynamic schedules.
With static schedules, the iterations are assigned purely based on the number
of iterations and the number of threads (and the 
<tt>chunk</tt>
 parameter; see later).
In dynamic schedules, on the other hand, iterations are assigned to threads that
are unoccupied. Dynamic schedules are a good idea if iterations take an unpredictable
amount of time, so that 
<i>load balancing</i>
 is needed.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/scheduling.jpeg" width=800></img>
<p name="caption">
FIGURE 19.4: Illustration static round-robin scheduling versus dynamic
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
19.4
 illustrates this: assume that each core
gets assigned two (blocks of) iterations and these blocks take
gradually less and less time. You see from the left picture that
thread&nbsp;1 gets two fairly long blocks, where as thread&nbsp;4 gets two short
blocks, thus finishing much earlier. (This phenomenon of threads
having unequal amounts of work is known as
<i>load imbalance</i>
.)
On the other hand, in the right figure thread&nbsp;4 gets
block&nbsp;5, since it finishes the first set of blocks early. The effect
is a perfect load balancing.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/schedules.jpeg" width=800></img>
<p name="caption">
FIGURE 19.5: Illustration of the scheduling strategies of loop iterations
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

The default static schedule is to assign one consecutive block of
iterations to each thread. If you want different sized blocks you can
define a 
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp for schedule(static[,chunk])
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
(where the square brackets indicate an optional argument).
With static scheduling,
the compiler will determine the assignment of loop iterations to the threads
at compile time,
so,
provided the iterations take roughly the same amount of time,
this is the most efficient at runtime.
</p>

<p name="switchToTextMode">
The choice of a chunk size is often a balance between the low overhead of having
only a few chunks, versus the load balancing effect of having smaller chunks.
<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Why is a chunk size of&nbsp;1 typically a bad idea? (Hint: think about
  cache lines, and read 
<i>Eijkhout:IntroHPC</i>
.)
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

In dynamic scheduling OpenMP will put blocks of iterations
(the default chunk size is&nbsp;1) in a task queue, and the threads take one of these
tasks whenever they are finished with the previous.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp for schedule(static[,chunk])
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
While this schedule may give good load balancing if the iterations
take very differing amounts of time to execute, it does carry runtime
overhead for managing the queue of iteration tasks.
</p>

<p name="switchToTextMode">
Finally, there is the 
The thinking here is that large chunks carry the least overhead, but smaller chunks are better
for load balancing.
The various schedules are illustrated in figure&nbsp;
19.5
.
</p>

<p name="switchToTextMode">
If you don't want to decide on a schedule in your code, you can
specify the 
schedule will then at runtime be read from the
<tt>OMP_SCHEDULE</tt>
 environment variable. You can even just
leave it to the runtime library by specifying
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->

  We continue with exercise&nbsp;
19.1
.
  We add `adaptive integration'%
<!-- index -->
<!-- index -->
:
  where needed, the program refines the step
  size\footnote{It doesn't actually do this in a mathematically
    sophisticated way, so this code is more for the sake of the
    example.}.  This means that the iterations no longer take a
  predictable amount of time.
</p>

<!-- environment: multicols start embedded generator -->
<!-- environment block purpose: [[ environment=multicols ]] -->
<multicols>

<p name="multicols">
<!-- TranslatingLineGenerator multicols ['multicols'] -->
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for (i=0; i&lt;nsteps; i++) {
    double
    x = i*h,x2 = (i+1)*h,
    y = sqrt(1-x*x),y2 = sqrt(1-x2*x2),
    slope = (y-y2)/h;
    if (slope&gt;15) slope = 15;
    int
    samples = 1+(int)slope, is;
    for (is=0; is&lt;samples; is++) {
        double
        hs = h/samples,
        xs = x+ is*hs,
        ys = sqrt(1-xs*xs);
        quarterpi += hs*ys;
        nsamples++;
    }
}
  pi = 4*quarterpi;
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</multicols>
<!-- environment: multicols end embedded generator -->
<p name="switchToTextMode">

<!-- environment: enumerate start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Use the 
<tt>omp parallel for</tt>
 construct to parallelize the loop.
  As in the previous lab, you may at first see an incorrect result.
  Use the 
<tt>reduction</tt>
 clause to fix this.
<li>
Your code should now see a decent speedup, but possible not for all cores.
  It is possible to get completely linear speedup by adjusting the schedule.
</p>

<p name="switchToTextMode">
  Start by using 
<tt>schedule</tt>

<tt>(static,$n$)</tt>
. Experiment with values
  for&nbsp;$n$.  When can you get a better speedup? Explain this.
<li>
Since this code is somewhat dynamic, try 
<tt>schedule</tt>

<tt>(dynamic)</tt>
.
  This will actually give a fairly bad result. Why?  Use
<tt>schedule</tt>

<tt>(dynamic,$n$)</tt>
 instead, and experiment with values
  for&nbsp;$n$.
<li>
Finally, use 
<tt>schedule</tt>

<tt>(guided)</tt>
, where OpenMP uses a
  heuristic.  What results does that give?
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
<p name="switchToTextMode">

</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Program the 
<i>LU factorization</i>
 algorithm without pivoting.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for k=1,n:
  A[k,k] = 1./A[k,k]
  for i=k+1,n:
    A[i,k] = A[i,k]/A[k,k]
    for j=k+1,n:
      A[i,j] = A[i,j] - A[i,k]*A[k,j]
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<!-- environment: enumerate start embedded generator -->
<!-- environment block purpose: [[ environment=enumerate ]] -->
<enumerate>
<ol>
<!-- TranslatingLineGenerator enumerate ['enumerate'] -->
<li>
Argue that it is not possible to parallelize the outer loop.
<li>
Argue that it is possible to parallelize both the $i$ and $j$ loops.
<li>
Parallelize the algorithm by focusing on the $i$ loop. Why is the algorithm as given here best
  for a matrix on row-storage? What would you do if the matrix was on column storage?
<li>
Argue that with the default schedule, if a row is updated by one thread in one iteration,
  it may very well be updated by another thread in another. Can you find a way to schedule
  loop iterations so that this does not happen? What practical reason is there for doing so?
</ol>
</enumerate>
<!-- environment: enumerate end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

The schedule can be declared explicitly, set at runtime
through the 
<tt>OMP_SCHEDULE</tt>
 environment variable, or left up to the runtime system
by specifying 
<tt>auto</tt>
. Especially in the last two cases  you may want to enquire
what schedule is currently being used with
<tt>omp_get_schedule</tt>
.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
int omp_get_schedule(omp_sched_t * kind, int * modifier );
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

Its mirror call is 
<tt>omp_set_schedule</tt>
, which sets the
value that is used when schedule value 
<tt>runtime</tt>
 is used. It is in
effect equivalent to setting the environment variable
<tt>OMP_SCHEDULE</tt>
.
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
void omp_set_schedule (omp_sched_t kind, int modifier);
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<!-- environment: tabular start embedded generator -->
</p>
<table>
<tr>
<td>
<!-- TranslatingLineGenerator tabular ['tabular'] -->
  \toprule
  Type</td><td>environment variable</td><td>clause</td><td>
<tt>omp_sched_t</tt>
</td><td>
<tt>omp_sched_t</tt>
</td><td>modifier default</td></tr>
<tr><td>
        </td><td>{\tt OMP\_SCHEDULE\char`\=}</td><td>{\tt schedule( ... )}</td><td>name</td><td>value </td></tr>
<tr><td>
  \midrule
  static</td><td>
<tt>static[,n]</tt>
</td><td>{static[,n]}</td><td>
<tt>omp_sched_static</tt>
</td><td>1</td><td>$N/\mathit{nthreads}$</td></tr>
<tr><td>
  dynamic</td><td>
<tt>dynamic[,n]</tt>
</td><td>{dynamic[,n]}</td><td>
<tt>omp_sched_dynamic</tt>
</td><td>2</td><td>$1$</td></tr>
<tr><td>
  guided</td><td>
<tt>guided[,n]</tt>
</td><td>{guided[,n]}</td><td>
<tt>omp_sched_guided</tt>
</td><td>3</td></tr>
<tr><td>
  auto</td><td>
<tt>auto</tt>
</td><td>auto</td><td>
<tt>omp_sched_auto</tt>
</td><td>4</td></tr>
<tr><td>
  \bottomrule
</td>
</tr>
</table>
<!-- environment: tabular end embedded generator -->
<p name="switchToTextMode">

Here are the various schedules you can set with the
<tt>schedule</tt>
 clause:
<!-- environment: description start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=description ]] -->
<description>
<ul>
<!-- TranslatingLineGenerator description ['description'] -->
<li>
[affinity] Set by using value  
<tt>omp_sched_affinity</tt>
<li>
[auto] The schedule is left up to the implementation. Set by
    using value 
<tt>omp_sched_auto</tt>
<li>
[static] value:&nbsp;1. The modifier parameter is the 
<i>chunk</i>
 size.
    Can also be set by using value  
<tt>omp_sched_static</tt>
<li>
[dynamic] value:&nbsp;2. The modifier parameter is the
<i>chunk</i>
 size; default&nbsp;1.
    Can also be set by using value
<tt>omp_sched_dynamic</tt>
<li>
[guided] Value:&nbsp;3. The modifier parameter is the
<tt>chunk</tt>
 size. Set by using value
<tt>omp_sched_guided</tt>
<li>
[runtime] Use the value of the 
<tt>OMP_SCHEDULE</tt>
    environment variable. Set by using value
<tt>omp_sched_runtime</tt>
</ul>
</description>
<!-- environment: description end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Reductions">19.4</a> Reductions</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Reductions">Reductions</a>
</p>
</p>

<p name="switchToTextMode">
So far we have focused on loops with independent iterations.
Reductions are a common type of loop with dependencies.
There is an extended discussion of reductions in section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-loop.html#Reviewquestions">19.10</a>
.
</p>

<h2><a id="Collapsingnestedloops">19.5</a> Collapsing nested loops</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Collapsingnestedloops">Collapsing nested loops</a>
</p>
<p name="switchToTextMode">

In general, the more work there is to divide over a number of threads,
the more efficient the parallelization will be. In the context of
parallel loops, it is possible to increase the amount of work by
parallelizing all levels of loops instead of just the outer one.
</p>

<p name="switchToTextMode">
Example: in
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for ( i=0; i&lt;N; i++ )
  for ( j=0; j&lt;N; j++ )
    A[i][j] = B[i][j] + C[i][j]
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
all $N^2$ iterations are independent, but a regular 
<tt>omp for</tt>

directive will only parallelize one level. The 
clause will parallelize more than one level:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp for collapse(2)
for ( i=0; i&lt;N; i++ )
  for ( j=0; j&lt;N; j++ )
    A[i][j] = B[i][j] + C[i][j]
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
It is only possible to collapse perfectly nested loops, that is, the
loop body of the outer loop can consist only of the inner loop; there
can be no statements before or after the inner loop in the loop body
of the outer loop. That is, the two loops in
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for (i=0; i&lt;N; i++) {
  y[i] = 0.;
  for (j=0; j&lt;N; j++)
    y[i] += A[i][j] * x[j]
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
can not be collapsed.
</p>

<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  You could rewrite the above code as
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for (i=0; i&lt;N; i++)
  y[i] = 0.;
for (i=0; i&lt;N; i++) {
  for (j=0; j&lt;N; j++)
    y[i] += A[i][j] * x[j]
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Is it now correct to have the 
<tt>collapse</tt>
 directive
on the nested loop?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Consider this code for matrix transposition:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
void transposer(int n, int m, double *dst, const double *src) {
   int blocksize;
   for (int i = 0; i &lt; n; i += blocksize) {
      for (int j = 0; j &lt; m; j += blocksize) {
          // transpose the block beginning at [i,j]
          for (int k = i; k &lt; i + blocksize; ++k) {
              for (int l = j; l &lt; j + blocksize; ++l) {
                  dst[k + l*n] = src[l + k*m];
               }
          }
      }
   }
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Assuming that the 
<tt>src</tt>
 and 
<tt>dst</tt>

array are disjoint, which loops are parallel, and how many
levels can you collapse?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Orderediterations">19.6</a> Ordered iterations</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Orderediterations">Ordered iterations</a>
</p>

</p>

<p name="switchToTextMode">
Iterations in a parallel loop that are executed in parallel do not
execute in lockstep. That means that in
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel for
for ( ... i ... ) {
  ... f(i) ...
  printf("something with %d\n",i);
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
it is not true that all function evaluations happen more or less at
the same time, followed by all print statements. The print statements
can really happen in any order. The 
coupled with the 
<tt>ordered</tt>
 directive can
force execution in the right order:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel for ordered
for ( ... i ... ) {
  ... f(i) ...
#pragma omp ordered
  printf("something with %d\n",i);
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
Example code structure:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel for shared(y) ordered
for ( ... i ... ) {
  int x = f(i)
#pragma omp ordered
  y[i] += f(x)
  z[i] = g(y[i])
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
There is a limitation:
each iteration can encounter only one 
<tt>ordered</tt>
 directive.
</p>

<h2><a id="<tt>nowait<tt>">19.7</a> <tt>nowait</tt></h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#<tt>nowait<tt>"><tt>nowait</tt></a>
</p>

<p name="switchToTextMode">

The implicit barrier at the end of a work sharing construct
can be cancelled with a
This has the effect that threads that are finished can continue
with the next code in the parallel region:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel
{
#pragma omp for nowait
  for (i=0; i&lt;N; i++) { ... }
  // more parallel code
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

In the following example, threads that are finished with the first loop
can start on the second. Note that this requires both loops to have
the same schedule. We specify the static schedule here to have an
identical scheduling of iterations over threads:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
#pragma omp parallel
{
  x = local_computation()
#pragma omp for schedule(static) nowait
  for (i=0; i&lt;N; i++) {
    x[i] = ...
  }
#pragma omp for schedule(static)
  for (i=0; i&lt;N; i++) {
    y[i] = ... x[i] ...
  }
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Whileloops">19.8</a> While loops</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Whileloops">While loops</a>
</p>
</p>

<p name="switchToTextMode">
OpenMP can only handle `for' loops: 
<i>while loops</i>
 can not
be parallelized. So you have to find a way around that. While loops
are for instance used to search through data:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
while ( a[i]!=0 && i&lt;imax ) {
 i++; }
// now i is the first index for which \n{a[i]} is zero.
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
We replace the while loop by a for loop that examines all locations:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
result = -1;
#pragma omp parallel for
for (i=0; i&lt;imax; i++) {
  if (a[i]!=0 && result&lt;0) result = i;
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<!-- environment: exercise start embedded generator -->
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Show that this code has a race condition.
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">
You can fix the race condition by making the condition into a critical section;
section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-sync.html#<tt>critical<tt>and<tt>atomic<tt>">23.2.2</a>
. In this particular example, with a very small amount
of work per iteration, that is likely to be inefficient
in this case (why?).
A&nbsp;more efficient solution uses the 
<tt>lastprivate</tt>
 pragma:
<!-- environment: lstlisting start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
result = -1;
#pragma omp parallel for lastprivate(result)
for (i=0; i&lt;imax; i++) {
  if (a[i]!=0) result = i;
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
<p name="switchToTextMode">
You have now solved a slightly different problem: the result variable
contains the 
<i>last</i>
 location where 
<tt>a[i]</tt>
 is zero.
</p>

<h2><a id="Scalingtests">19.9</a> Scaling tests</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a>
</p>
<p name="switchToTextMode">

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#ompsineloop" aria-expanded="false" aria-controls="ompsineloop">
        Text: ompsineloop
      </button>
    </h5>
  </div>
  <div id="ompsineloop" class="collapse">
  <pre>
// speedup.c
#pragma omp parallel for
      for (int ip=0; ip&lt;N; ip++) {
        for (int jp=0; jp&lt;M; jp++) {
          double f = sin( values[ip] );
          values[ip] = f;
        }
      }
</pre>
</div>
</div>
</p>

<h3><a id="Lonestar6">19.9.1</a> Lonestar 6</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a> > <a href="omp-loop.html#Lonestar6">Lonestar 6</a>
</p>
<p name="switchToTextMode">

Lonestar 6, dual socket 
<i>AMD Milan</i>
, total 112 cores:
figure&nbsp;
19.6
.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.6: Speedup as function of thread count, Lonestar 6 cluster, different binding parameters
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Frontera">19.9.2</a> Frontera</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a> > <a href="omp-loop.html#Frontera">Frontera</a>
</p>
</p>

<i>Intel Cascade Lake</i>
<p name="switchToTextMode">
, dual socket, 56 cores total;
figure&nbsp;
19.7
.
</p>

<p name="switchToTextMode">
For all core counts to half the total,
performance for all binding strategies seems equal.
After that , 
<tt>close</tt>
 and 
<tt>spread</tt>
 perform equally,
but the speedup for the 
<tt>false</tt>
 value gives erratic numbers.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.7: Speedup as function of thread count, Frontera cluster, different binding parameters
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Stampede2skylake">19.9.3</a> Stampede2 skylake</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a> > <a href="omp-loop.html#Stampede2skylake">Stampede2 skylake</a>
</p>
</p>

<p name="switchToTextMode">
Dual 24-core 
<i>Intel Skylake</i>
;
figure&nbsp;
19.8
.
</p>

<p name="switchToTextMode">
We see that 
<tt>close</tt>
 binding gives worse performance than 
<tt>spread</tt>
.
Setting binding to 
<tt>false</tt>
 only gives bad performance for large core counts.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.8: Speedup as function of thread count, Stampede2 skylake cluster, different binding parameters
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Stampede2KnightsLanding">19.9.4</a> Stampede2 Knights Landing</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a> > <a href="omp-loop.html#Stampede2KnightsLanding">Stampede2 Knights Landing</a>
</p>
</p>

<p name="switchToTextMode">
Single socket 68-core 
<i>Intel Knights Landing</i>
;
19.9
.
</p>

<p name="switchToTextMode">
Since this is a single socket design,
we don't distinguish between the 
<tt>close</tt>
 and 
<tt>spread</tt>
 binding.
However, the binding value of 
<tt>true</tt>
 shows good speedup
--&nbsp;in fact beyond the core count&nbsp;--
while 
<tt>false</tt>
 gives worse performance than in other architectures.
</p>

<p name="switchToTextMode">
Single 68 core
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.9: Speedup as function of thread count, Stampede2 Knights Landing cluster, different binding parameters
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<h3><a id="Longhorn">19.9.5</a> Longhorn</h3>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Scalingtests">Scaling tests</a> > <a href="omp-loop.html#Longhorn">Longhorn</a>
</p>
</p>

<p name="switchToTextMode">
Dual 20-core 
<i>IBM Power9</i>
, 4&nbsp;hyperthreads;
19.10
</p>

<p name="switchToTextMode">
Unlike the Intel processors, here we use the hyperthreads.
Figure&nbsp;
19.10
 shows dip in the speedup
at 40 threads.
For higher thread counts the speedup increases to well beyond the
physical core count of&nbsp;$40$.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<!-- environment: tikzpicture start embedded generator -->
<!-- environment block purpose: [[ environment=tikzpicture ]] -->
<tikzpicture>


</tikzpicture>
<!-- environment: tikzpicture end embedded generator -->
<p name="caption">
FIGURE 19.10: Speedup as function of thread count, Longhorn cluster, different binding parameters
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

</p>

<h2><a id="Reviewquestions">19.10</a> Review questions</h2>
<p name=crumbs>
crumb trail:  > <a href="omp-loop.html">omp-loop</a> > <a href="omp-loop.html#Reviewquestions">Review questions</a>
</p>
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  The following loop can be parallelized with a  <tt>parallel for</tt> .
  Is it correct to add the directive  <tt>collapse(2)</tt> ?
</p>

<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for (i=0; i&lt;N; i++) {
  y[i] = 0.;
  for (j=0; j&lt;N; j++)
  y[i] += A[i][j] * x[j]
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Same question for the nested loop here:
</p>

<!-- environment: lstlisting start embedded generator -->
<!-- environment block purpose: [[ environment=lstlisting ]] -->
<lstlisting>
<pre>
for (i=0; i&lt;N; i++)
y[i] = 0.;
for (i=0; i&lt;N; i++) {
  for (j=0; j&lt;N; j++)
  y[i] += A[i][j] * x[j]
}
</pre>
</lstlisting>
<!-- environment: lstlisting end embedded generator -->
</exercise>
<!-- environment: exercise end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
