<html>
<head>
<link href="ihpsc.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.4.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$']]}
  });
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src=https://ccrs.cac.cornell.edu:8443/client.0.2.js></script>
<script id="script">
class Example {
  constructor(buttonID, editorID, outputID, sourceFile, fileName, commandStr) {
    this.buttonID = buttonID;
    this.editorID = editorID;
    this.outputID = outputID;
    this.sourceFile = sourceFile;
    this.fileName = fileName;
    this.commandStr = commandStr;
  }
    
  async display(results, object) {
    if (results.stdout.length > 0)
      document.getElementById(object.outputID).textContent = results.stdout;
    else
      document.getElementById(object.outputID).textContent = results.stderr;
  }

  async initialize() {
    this.editor = await MonacoEditorFileSource.create(this.editorID);
    this.editor.setTextFromFile(this.sourceFile);
    this.job = await Job.create(JobType.MPI);
    this.command = new CommandWithFiles(this.job, this.commandStr);
    this.command.addFileSource(this.editor, this.fileName);
    this.trigger = new ButtonTrigger(this.command, this.display, this.buttonID, this);
    document.getElementById(this.buttonID).disabled = false;
  }
}
</script>
<style></style>

<div class="container">
  <div class="row">
    <div class="col-12">
      <div class="pagehead">
        <h1>Hybrid computing</h1>
        <h5>Experimental html version of downloadable textbook, see http://www.tacc.utexas.edu/~eijkhout/istc/istc.html</h5>
      </div>
    </div>
  </div>
  <div>


\[
\newcommand\inv{^{-1}}\newcommand\invt{^{-t}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbR{\mathbb{R}}
\newcommand\defined{
  \mathrel{\lower 5pt \hbox{${\equiv\atop\mathrm{\scriptstyle D}}$}}}
\]


45.1 : <a href="hybrid.html#Affinity">Affinity</a><br>
45.2 : <a href="hybrid.html#Whatdoesthehardwarelooklike?">What does the hardware look like?</a><br>
45.3 : <a href="hybrid.html#Affinitycontrol">Affinity control</a><br>
45.4 : <a href="hybrid.html#Discussion">Discussion</a><br>
45.5 : <a href="hybrid.html#Processesandcoresandaffinity">Processes and cores and affinity</a><br>
45.6 : <a href="hybrid.html#Practicalspecification">Practical specification</a><br>
<a href="index.html">Back to Table of Contents</a>
<h1>45 Hybrid computing</h1>
<!-- TranslatingLineGenerator file ['file'] -->
</p>

<p name="switchToTextMode">
So far, you have learned to use MPI for distributed memory and OpenMP
for shared memory parallel programming. However, distribute memory
architectures actually have a shared memory component, since each
cluster node is typically of a multicore design. Accordingly, you
could program your cluster using MPI for inter-node and OpenMP for
intra-node parallelism.
</p>

<p name="switchToTextMode">
You now have to find the right balance between processes and threads,
since each can keep a core fully busy.
Complicating this story, a node can have more than one 
<i>socket</i>
,
and corresponding 
<i>NUMA</i>
 domain.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/mpi-omp-hybrid.png" width=800></img>
<p name="caption">
FIGURE 45.1: Three modes of MPI/OpenMP usage on a multi-core cluster
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure~
45.1
 illustrates three modes: pure MPI
with no threads used; one MPI process per node and full
multi-threading; two MPI processes per node, one per socket, and
multiple threads on each socket.
</p>

<h2><a id="Affinity">45.1</a> Affinity</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Affinity">Affinity</a>
</p>
<p name="switchToTextMode">

<!-- index -->
</p>

<p name="switchToTextMode">
In the preceeding chapters we mostly considered all MPI nodes or
OpenMP thread as being in one flat pool.
However, for high performance you need to worry about 
<i>affinity</i>
:
the question of which process or thread is placed where, and how
efficiently they can interact.
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/ranger-numa.jpg" width=800></img>
<p name="caption">
FIGURE 45.2: The NUMA structure of a Ranger node
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Here are some situations where you affinity becomes a concern.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
In pure MPI mode processes that are on the same node can
  typically communicate faster than processes on different
  nodes. Since processes are typically placed sequentially, this means
  that a scheme where process~$p$ interacts mostly with $p+1$ will be
  efficient, while communication with large jumps will be less so.
<li>
If the cluster network has a structure
  (
<i>processor grid</i>
 as opposed to 
<i>fat-tree</i>
),
  placement of processes has an effect on program efficiency.  MPI
  tries to address this with 
<i>graph topology</i>
;
  section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html#Distributedgraphtopology">11.2</a>
.
<li>
Even on a single node there can be
  asymmetries. Figure~
45.2
 illustrates the structure
  of the four sockets of the 
<i>Ranger</i>
 supercomputer (no
  longer in production). Two cores have no direct connection.
</p>

<p name="switchToTextMode">
  This asymmetry affects both MPI processes and threads on that node.
<li>
Another problem with multi-socket designs is that each socket
  has memory attached to it. While every socket can address all the
  memory on the node, its local memory is faster to access. This
  asymmetry becomes quite visible in the 
<i>first-touch</i>
  phenomemon; section~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html#First-touch">25.2</a>
.
<li>
If a node has fewer MPI processes than there are cores, you want
  to be in control of their placement. Also, the operating system can
  migrate processes, which is detrimental to performance since it
  negates data locality. For this reason, utilities such as
<tt>numactl</tt>
<!-- environment: tacc start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
(and at TACC 
<tt>tacc_affinity</tt>
)
</p name="tacc">

</tacc>
<!-- environment: tacc end embedded generator -->
<p name="switchToTextMode">
  can be used to 
<i>pin a thread</i>
 or process to a specific core.
<li>
Processors with 
<i>hyperthreading</i>
 or
<i>hardware threads</i>
 introduce another level or worry
  about where threads go.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Whatdoesthehardwarelooklike?">45.2</a> What does the hardware look like?</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Whatdoesthehardwarelooklike?">What does the hardware look like?</a>
</p>
</p>

<p name="switchToTextMode">
If you want to optimize affinity, you should first know what the
hardware looks like. The 
 utility is valuable
here~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/bibliography.html#goglin:hwloc">[goglin:hwloc]</a>
 (
<a href=https://www.open-mpi.org/projects/hwloc/>https://www.open-mpi.org/projects/hwloc/</a>
).
</p>

<!-- environment: figure start embedded generator -->
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<iframe src="graphics/stampede-compute.pdf" width=800></iframe>
<p name="caption">
FIGURE 45.3: Structure of a Stampede compute node
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<iframe src="graphics/stampede-largemem.pdf" width=800></iframe>
<p name="caption">
FIGURE 45.4: Structure of a Stampede largemem four-socket compute node
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<iframe src="graphics/ls5.pdf" width=800></iframe>
<p name="caption">
FIGURE 45.5: Structure of a Lonestar5 compute node
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">

Figure~
45.3
 depicts a
<i>Stampede compute node</i>
, which is a two-socket
<i>Intel Sandybridge</i>
 design;
figure~
45.4
 shows a
<i>Stampede largemem node</i>
, which is a four-socket design.
Finally, figure~
45.5
 shows a
<i>Lonestar5</i>
 compute node, a~two-socket design with 12-core
<i>Intel Haswell</i>
 processors with two hardware threads
each.
</p>

<h2><a id="Affinitycontrol">45.3</a> Affinity control</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Affinitycontrol">Affinity control</a>
</p>
<p name="switchToTextMode">

See chapter~
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html">OpenMP topic: Affinity</a>
 for OpenMP affinity control.
</p>

<!-- index -->
<p name="switchToTextMode">

<h2><a id="Discussion">45.4</a> Discussion</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Discussion">Discussion</a>
</p>
</p>

<p name="switchToTextMode">
The performance implications of the pure MPI strategy versus hybrid
are subtle.
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
First of all, we note that there is no obvious speedup: in a
  well balanced MPI application all cores are busy all the time, so
  using threading can give no immediate improvement.
<li>
Both MPI and OpenMP are subject to Amdahl's law that quantifies
  the influence of sequential code; in hybrid computing there is a new
  version of this law regarding the amount of code that is
  MPI-parallel, but not OpenMP-parallel.
<li>
MPI processes run unsynchronized, so small variations in load or
  in processor behavior can be tolerated. The frequent barriers in
  OpenMP constructs make a hybrid code more tightly synchronized, so
  load balancing becomes more critical.
<li>
On the other hand, in OpenMP codes it is easier to divide the
  work into more tasks than there are threads, so statistically a
  certain amount of load balancing happens automatically.
<li>
Each MPI process has its own buffers, so hybrid takes less
  buffer overhead.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<!-- environment: exercise start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=exercise ]] -->
<exercise>
<b>Exercise</b>
<p name="exercise">
<!-- TranslatingLineGenerator exercise ['exercise'] -->
  Review the scalability argument for 1D versus 2D matrix
  decomposition in 
<i>Eijkhout:IntroHPC</i>
. Would you get
  scalable performance from doing a 1D decomposition (for instance, of
  the rows) over MPI processes, and decomposing the other directions
  (the columns) over OpenMP threads?
</p name="exercise">
</exercise>
<!-- environment: exercise end embedded generator -->
<!-- environment: answer start embedded generator -->
<!-- environment block purpose: [[ environment=answer ]] -->
<answer>


</answer>
<!-- environment: answer end embedded generator -->
<p name="switchToTextMode">

Another performance argument we need to consider concerns message
traffic.  If let all threads make MPI calls (see
section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-hybrid.html#MPIsupportforthreading">13.1</a>
) there is going to be little
difference. However, in one popular hybrid computing strategy we would
keep MPI calls out of the OpenMP regions and have them in effect done
by the master thread.
In that case there are only MPI messages
between nodes, instead of between cores. This leads to a decrease in
message traffic, though this is hard to quantify. The number of
messages goes down approximately by the number of cores per node, so
this is an advantage if the average message size is small. On the
other hand, the amount of data sent is only reduced if there is
overlap in content between the messages.
</p>

<p name="switchToTextMode">
Limiting MPI traffic to the master thread also means that no buffer
space is needed for the on-node communication.
</p>

<h2><a id="Processesandcoresandaffinity">45.5</a> Processes and cores and affinity</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Processesandcoresandaffinity">Processes and cores and affinity</a>
</p>
<p name="switchToTextMode">

In OpenMP, threads are purely a software construct and you can create
however many you want.
The hardware limit of the available cores can be queried with
<tt>omp_get_num_procs</tt>
 (section&nbsp;
<a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-basics.html#Creatingparallelism">17.5</a>
).
How does that work in a hybrid context?
Does the `proc' count return the total number of cores,
or does the MPI scheduler limit it to a number exclusive to each MPI process?
</p>

<p name="switchToTextMode">
The following code fragment explore this:
</p>

<div class="card">
  <div class="card-header" id="headingOne">
    <h5 class="mb-0">
      <button class="btn btn-link" data-toggle="collapse" data-target="#corecountttotal" aria-expanded="false" aria-controls="corecountttotal">
        C Code: corecountttotal
      </button>
    </h5>
  </div>
  <div id="corecountttotal" class="collapse">
  <pre>
// procthread.c
int ncores;
#pragma omp parallel
#pragma omp master
ncores = omp_get_num_procs();

int totalcores;
MPI_Reduce(&ncores,&totalcores,1,MPI_INT,MPI_SUM,0,comm);
if (procid==0) {
  printf("Omp procs on this process: %d\n",ncores);
  printf("Omp procs total: %d\n",totalcores);
}
</pre>
</div>
</div>
<p name="switchToTextMode">

Running this with 
<i>Intel MPI</i>
 (version&nbsp;19)
gives the following:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
---- nprocs: 14
Omp procs on this process: 4
Omp procs total: 56
---- nprocs: 15
Omp procs on this process: 3
Omp procs total: 45
---- nprocs: 16
Omp procs on this process: 3
Omp procs total: 48
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
We see that
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Each process get an equal number of cores, and
<li>
Some cores will go unused.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

While the OpenMP `proc' count is such that the MPI processes will not
oversubscribe cores, the actual placement of processes and threads
is not expressed here.
This assignment is known as 
<i>affinity</i>
 and it is
determined by the MPI/OpenMP runtime system.
Typically it can be controlled through environment variables,
but one hopes the default assignment makes sense.
<!-- environment: figure start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=figure ]] -->
<figure>
<float mode=figure>
<!-- TranslatingLineGenerator figure ['figure'] -->
<img src="graphics/knl-affinity.png" width=800></img>
<p name="caption">
FIGURE 45.6: Process and thread placement on an Intel Knights Landing
</p>

</float>
</figure>
<!-- environment: figure end embedded generator -->
<p name="switchToTextMode">
Figure&nbsp;
45.6
 illustrates this for the
<i>Intel Knights Landing</i>
:
<!-- environment: itemize start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=itemize ]] -->
<itemize>
<ul>
<!-- TranslatingLineGenerator itemize ['itemize'] -->
<li>
Placing four MPI processes on 68 cores gives 17 cores per process.
<li>
Each process receives a contiguous set of cores.
<li>
However, cores are grouped in `tiles' of two, so processes 1&nbsp;and&nbsp;3 start
  halfway a tile.
<li>
Therefore, thread zero of that process is bound to the second core.
</ul>
</itemize>
<!-- environment: itemize end embedded generator -->
<p name="switchToTextMode">

<h2><a id="Practicalspecification">45.6</a> Practical specification</h2>
<p name=crumbs>
crumb trail:  > <a href="hybrid.html">hybrid</a> > <a href="hybrid.html#Practicalspecification">Practical specification</a>
</p>
</p>

<p name="switchToTextMode">
Say you use 100 cluster nodes, each with 16 cores. You could then
start 1600 MPI processes, one for each core, but you could also start
100 processes, and give each access to 16 OpenMP threads.
</p>

<!-- environment: tacc start embedded generator -->
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
In your slurm scripts, the first scenario would be specified \n{-N 100
-n 1600}, and the second as
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
#$ SBATCH -N 100
#$ SBATCH -n 100


export OMP_NUM_THREADS=16
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->

</tacc>
<!-- environment: tacc end embedded generator -->
<p name="switchToTextMode">

There is a third choice, in between these extremes, that makes
sense. A&nbsp;cluster node often has more than one socket, so you could put
one MPI process on each 
<i>socket</i>
, and use a number of
threads equal to the number of cores per socket.
</p>

<!-- environment: tacc start embedded generator -->
<!-- environment block purpose: [[ environment=tacc ]] -->
<tacc>

<p name="tacc">
<!-- TranslatingLineGenerator tacc ['tacc'] -->
The script for this would be:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
#$ SBATCH -N 100
#$ SBATCH -n 200


export OMP_NUM_THREADS=8
ibrun tacc_affinity yourprogram
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">

The 
<tt>tacc_affinity</tt>
 script unsets the following variables:
<!-- environment: verbatim start embedded generator -->
</p>
<!-- environment block purpose: [[ environment=verbatim ]] -->
<verbatim>
<pre>
export MV2_USE_AFFINITY=0
export MV2_ENABLE_AFFINITY=0
export VIADEV_USE_AFFINITY=0
export VIADEV_ENABLE_AFFINITY=0
</pre>
</verbatim>
<!-- environment: verbatim end embedded generator -->
<p name="switchToTextMode">
If you don't use 
<tt>tacc_affinity</tt>
 you may want to do this by hand,
otherwise 
<i>mvapich2</i>
 will use its own affinity rules.
</p name="tacc">

</tacc>
<!-- environment: tacc end embedded generator -->
</div>
<a href="index.html">Back to Table of Contents</a>
